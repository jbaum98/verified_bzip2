\documentclass[sigplan,10pt,anonymous,review]{thesis}

\acmConference
    [CPP'20]
    {ACM SIGPLAN International Conference on Certified Programs and Proofs}
    {January 20--21, 2020}{New Orleans, LA, USA}
\acmYear{2020}
\acmISBN{} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM} \acmDOI{} %
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
\citestyle{acmauthoryear}  %% For author/year citations
%\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].

\begin{document}

\title[Coq Formalization of Stable Sorts, Radix Sort, and BWT]{A Coq
  Formalization of Stable Sorts, Radix Sort, and the Burrows-Wheeler
  Transform}

\author{Jake Waksbaum}
\affiliation{
  \department{Department of Computer Science}
  \institution{Princeton University}
  \city{Princeton}
  \state{New Jersey}
  \postcode{08540}
  \country{USA}
}
\email{waksbaum@princeton.edu}

\author{Andrew Appel}
\affiliation{
  \department{Department of Computer Science}
  \institution{Princeton University}
  \city{Princeton}
  \state{New Jersey}
  \postcode{08540}
  \country{USA}
}
\email{appel@princeton.edu}

\begin{abstract}
Stable sorts are an important and fundamental type of sorting
algorithm. Some algorithms, such as radix sorts, rely upon the
stability of a sorting subroutine for their correctness. In other
cases, stable sorts are useful for reasoning about algorithms, often
because any two stable sorts are interchangeable. One such algorithm
is the Burrows--Wheeler transform (BWT), which has applications in
compression, full-text indexing, and pattern searching.

There has not been much work on formally verifying stable sorts, or
rather, the stability of stable sorts. While Bird et~al.
\cite{birdmu,pearls} provide an elegant derivation and proof for the
BWT, formalizing it in Coq reveals that they rely on a few lemmas that
are not trivial to prove, including the correctness of
least-significant digit (LSD) radix sort. We develop the theory of
stable sorts, and use this theory to prove the correctness of LSD
radix sort and thereby the BWT.
\end{abstract}

%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML} <ccs2012>
<concept>
<concept_id>10003752.10003809.10010031.10002975</concept_id>
<concept_desc>Theory of computation~Data compression</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003809.10010031.10010033</concept_id>
<concept_desc>Theory of computation~Sorting and searching</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10010940.10010992.10010998</concept_id>
<concept_desc>Software and its engineering~Formal methods</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Data compression}
\ccsdesc[500]{Theory of computation~Sorting and searching}
\ccsdesc[500]{Software and its engineering~Formal methods}
%% End of generated code

\keywords{Formal methods, Sorting and searching, Data compression}

\maketitle

\section{Introduction}
\label{sec:intro}

Stable sorts are an important and fundamental type of sorting
algorithm. A sort is stable if it preserves the relative order of
equal elements. For example, if we are stably sorting the list
$[\var{banana},\ \allowbreak\var{armpit},\ \allowbreak\var{carrot},\ \allowbreak\var{apple}]$,
we have to produce
$[\var{armpit},\ \allowbreak\var{apple},\ \allowbreak\var{banana},\ \allowbreak\var{carrot}]$
and not
$[\var{apple},\ \allowbreak\var{armpit},\ \allowbreak\var{banana},\ \allowbreak\var{carrot}]$.
Even though both are sorted on the first character, because
\var{armpit} preceded \var{apple} in the original lists, it must also
precede \var{apple} in the stably sorted list.

Stability can be directly useful in certain contexts, such as when
sorting a table by a single column. However, stability is also crucial
to the correctness of certain general-purpose algorithms. One such
algorithm, or set of algorithms, is radix sort. Radix sorts are used
to sort data lexicographically, and can be more efficient than
comparison-based sorting methods, both in theory and in practice
\cite{McIlroy93,Bentley:1997:FAS:314161.314321,10.1007/978-3-540-89097-3_3}.
Many radix sorts rely on a some stable sorting subroutine.

Stable sorts are also important for reasoning about sorting algorithms
more broadly. This is because any two stable sorting algorithms are
equivalent, in the sense that they will produce the same outputs for a
given input. Crucially, this allows us to reason using simple
algorithms and replace them with more efficient algorithms later on.
We make use of this technique in order to reason about LSD radix sort,
as well as the Burrows-Wheeler transform (BWT).

The Burrows--Wheeler transform is an invertible transformation that
makes a string more amenable to compression by other methods
\cite{bw}. Notably, the BWT is the core pass in the popular,
open-source bzip2 compression software \cite{tsai_2016}. In addition,
the BWT has been applied to full-text indexing and pattern searching,
for example in genomics \cite{ferragina_index, dna}.

There has not been much work on formally verifying stable sorts, or
rather, the stability of stable sorts. Some work has been done on
verifying the stability of mergesort
\cite{leroy_2018,Sternagel2013,Leino2015,deGouw2014}, but we do not
know of any formally verified implementation of a radix sort that
includes stability.

In addition, to our knowledge no one has produced a formally verified
implementation of the BWT, by which we mean an implementation with a
proof that the inverse-transform inverts the forwards-transform. Bird
et~al.\cite{alg-of-prog,birdmu} provide an excellent derivation of the
BWT and its inverse based on the technique program calculation, in
which programs are calculated from their specifications using
equational reasoning. However, their derivation leaves a few crucial
lemmas unproven, including the correctness of LSD radix sort.

In this paper, we develop more fully the theory of stable sorts: we
give multiple definitions for permutations and stable permutations and
prove them equivalent. This allows us to choose whichever definition
makes the proof easiest when we are reasoning about stability. We also
proved that any two sorts are interchangeable.
\begin{restatable*}[StableSort\_unique]{corollary}{stablesortunique}
  \label{thm:stablesort_unique}
  For all stable sorting functions $f, g: \var{list}~A \to
  \var{list}~A$,
  \begin{equation*}
    (\forall l,f~l = g~l)
  \end{equation*}
\end{restatable*}

Using \cref{thm:stablesort_unique}, we prove the correctness of an LSD
radix sort that relies on insertion sort to sort each column. This
choice was crucial to the proof: parametrizing radix sort over any
stable sort made it difficult to reason about, but reasoning about
insertion sort is very elegant. \cref{thm:stablesort_unique} allows us
to recover the generality of parametrizing over all stable sorts,
because it proves that insertion sort is equivalent to any other
stable sort.

Finally, we use the correctness of radix sort to reason about the BWT.
Specifically, we implement the functions $\var{bwl} : \var{list}~A \to
\var{list}~A$ and $\var{bwn} : \var{list}~A \to \nat$ that produce
the string and index part of the BWT respectively, and the function
$\var{unbwt} : \nat \to \var{list}~A \to \var{list}~A$ that reverses
the transform. We then prove the following theorem:
\begin{restatable*}[unbwt\_correct]{theorem}{unbwtcorrect}
  \label{thm:unbwt_correct}
  For all lists $l$,
  \begin{equation*}
    \var{unbwt}~(\var{bwn}~l)~(\var{bwl}~l) = l
  \end{equation*}
\end{restatable*}

The implementation is useful in its own right as a functional
specification for a potential verified implementation of a program
that relies on Burrows--Wheeler, such as bzip2. But more generally,
this provides evidence that Bird's approach of program calculation, in
which programs are derived from their specifications via algebraic
manipulations, is useful for proving the correctness of algorithms in
a machine-checked setting. Bird has applied this approach to greedy
algorithms, the Boyer-Moore algorithm, the Knuth-Morris-Pratt
algorithm, Sudoku solvers, arithmetic coding, the Schorr-Waite
algorithm, the Johnson-Trotter algorithm, and many more problems
\cite{pearls}.

Every definition and theorem in this paper corresponds directly to a
definition or theorem in our Coq sources, which we provide as
supplementary materials. Definitions are copied verbatim, modulo
whitespace. Theorems are rephrased for clarity, with the name of the
corresponding Coq theorem given in parentheses. The only exceptions
are \cref{sec:opt} and places where we give hypothetical definitions
to drive the explanation.

\section{Sorting}
\label{sec:sorting}

Whenever we sort a list, we do so with respect to some ordering on its
elements. For example, we could sort a list of students by first name,
last name, birthday, etc. More precisely, for sorting we need a total,
decideable preorder: a \textit{preorder} is a relation $\le$ that is
reflexive and transitive, it is \textit{total} if we can compare any
two elements, and it is \textit{decidable} if we have a procedure that
takes any two elements $x$ and $y$ and tells us if $x \le y$ or $x \ge y$.

Given a total decideable preorder, we can define when a list is in
ascending order with respect to that preorder. In this case, as will
be the case with other properties we will discuss, there are multiple
reasonable definitions. Our approach will be to state all the
definitions we can think of and to prove equivalence between them.
This increases our confidence that we have defined things correctly,
and allows us to choose whichever definition we find most convenient
for a given proof.

We can define \var{Sorted} in two main ways: an index-based definition
(\var{SortedIx}), and an inductive definition (\var{Sorted}).
\begin{definition}{SortedIx}
  A list $l$ is sorted if for all indices $i$ and $j$,
  \begin{equation*}
    i \le j \implies l_i \le l_j
  \end{equation*}
\end{definition}

\begin{definition}{Sorted}
  \begin{align*}
    \infer[\var{Sorted\_nil}]{\var{Sorted}~[]}{}
    &&
    \infer[\var{Sorted\_cons}]{\var{Sorted}~(a :: l)}{%
      (\forall~x \in l,~a < x) & \var{Sorted}~l
    }
  \end{align*}
\end{definition}

\begin{restatable}[SortedIx\_iff]{theorem}{sortedixiff}
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~l \iff \var{SortedIx}~l
  \end{equation*}
\end{restatable}
\begin{proof}
  See \cref{appendix:proofs}.
\end{proof}

However, a sorting function cannot produce just any sorted list as its
output; the sorted list must also contain all the same elements as the
original list. To capture this, we could use the \var{Permutation}
relation defined in the Coq standard library. This relation holds on
two lists when they contain all the same elements, and is defined
inductively:
\begin{definition}[Permutation]
  \label{def:permutation}
  \begin{gather*}
    \infer[\var{perm\_nil}]{\var{Permutation}~[]~[]}{}
    \\
    \allowbreak
    \\
    \infer[\var{perm\_skip}]{\var{Permutation}~(x :: l)~(x :: l')}{%
      \var{Permutation}~l~l'
    }
    \\
    \allowbreak
    \\
    \infer[\var{perm\_swap}]{\var{Permutation}~(x :: y :: l)~(y :: x :: l)}{}
    \\
    \allowbreak
    \\
    \infer[\var{perm\_trans}]{\var{Permutation}~l~l''}{
      \var{Permutation}~l~l' & \var{Permutation}~l'~l''
    }
  \end{gather*}
\end{definition}

Before we go further, we should clarify what we mean by the word
\textit{permutation}, as it has two related meanings that are often
used interchangeably. We sometimes say that two lists are permutations
of one other if they contain all of the same elements, perhaps in a
different order, like the lists $l =
[\var{zebra},\ \allowbreak\var{appliance},\ \allowbreak\var{kite}]$
and $l' =
[\var{appliance},\ \allowbreak\var{kite},\ \allowbreak\var{zebra}]$
However, in mathematics we sometimes say that a permutation is
actually the bijection on the indices that defines the correspondence
between the two lists. In this case, we could map indices in $l$ to
indices in $l'$ by mapping $0 \mapsto 2$, mapping $1 \mapsto 0$, and mapping $2 \mapsto
0$. Formally, we say that $p$ is the permutation that relates $l$ to
$l'$ if for all indices $i$ and $j$,
\begin{equation*}
  l_i = l'_{p(i)}
\end{equation*}
For consistency with Coq's built-in \var{Permuation} relation, in a
situation like this we will say that $l$ and $l'$ are permutations of
one other, and that $p$ is the \textit{permutation function} that
relates $l$ to $l'$.

This suggests an alternative definition for \var{Permutation}:
\begin{definition}{PermutationEx}
  \label{def:permutation_ex}
  Two lists $l$ and $l'$ are permutations of one another when, letting
  $n = \var{length}~l$, there exists a permutation function $p$ on $n$
  elements such that $p(l) = l'$.
\end{definition}
Here we are overloading the notation $p(l)$ to denote the result of
shuffling a list~$l$ according to a permutation function~$p$. To
define this precisely, we have to first define how we will represent
permutation functions in Coq.

A permutation function on $n$ elements maps indices to indices, and so
has type $\nat_{<n} \to \nat_{<n}$. In Coq, we can represent
a finite function as a list of its outputs: $f : \nat_{<n} \to A$
is represented by the list $[f(0),~f(1),\allowbreak\ldots,~f(n-1)]$.
Applied to permutations, this means that the permutation~$p$ mentioned
above would be represented by $[2,~1,~0]$. This is consistent with
``one-line'' notation for permutations, in which a permutation on $n$
elements is represented by its result when applied to the
list~$[0,~2,\ldots,n-1]$. Now we can make precise the notation
$p(l)$\footnote{In Coq, this corresponds the the \var{apply} function,
  whose definition is slightly more complex due to the extra default
  parameter for \var{nth}.}
\begin{equation*}
  p(l) \coloneq \var{map}~(\lambda i. l_i)~p
\end{equation*}

However, not any list of natural numbers represents a valid
permutation function, because a permutation must be a bijection. We
can define the property $\var{PermFun}~n~p$, which holds when $p :
\var{list}~\nat$ represents a permutation function on lists of
length $n$:
\begin{definition}[PermFun]
  A list~$p$ of natural numbers is a permutation function on $n$
  elements if for all natural numbers $i$, $i \in p \iff i < n$, and if
  $p$ contains no duplicate elements\footnote{For this we can use the
    \var{NoDup} predicate from Coq's standard library.}.
\end{definition}

We should also mention a third definition for permutations that can be
used on types with decideable equality:
\begin{definition}[PermutationCount]
  \label{def:permutation_count}
  Two lists are permutations of one another when for any $x$,
  \begin{equation*}
    \var{count\_occ}~l~x = \var{count\_occ}~l'~x
  \end{equation*}
  where $\var{count\_occ}~l~x$ counts the number of elements in $l$
  that are equal to $x$.
\end{definition}

With this we can define the specification for sorting functions:
\begin{definition}[Sort]
  A function $f : \var{list}~A \to \var{list}~A$ is a sorting function
  when for all inputs $l$,
  \begin{equation*}
    \var{Sorted}~(f~l) \land \var{Permutation}~(f~l)~l
  \end{equation*}
\end{definition}

\section{Stable sorting}
\label{sec:stable_sorting}

Now that we've defined sorts, we can start to consider stable sorts.
Stable sorts must preserve the relative orders of ``equivalent''
elements. This equivalence comes from the ordering we are using: any
total decidable preorder also give us a decideable equivalence
relation $\equiv$, where $x \equiv y$ when $x \le y$ and $x \ge y$.

Stable sorts still must produce sorted outputs, but the output cannot
be just any permutation of the input. We need a stricter notion of
a permutation that respects equivalence, which we will call a stable
permutation.
\begin{definition}[StableSort]
  A function $f : \var{list}~A \to \var{list}~A$ is a stable sorting
  function when for all inputs $l$,
  \begin{equation*}
    \var{Sorted}~(f~l) \land \var{StablePerm}~(f~l)~l
  \end{equation*}
\end{definition}

To define stable permutations, we can tweak each of the definitions of
permutation mentioned above. For example, we can modify our
function-based definition.

\begin{definition}[StablePermEx]
  Two lists $l$ and $l'$ are stable permutations of one another when
  there exists a \textit{stable} permutation function~$p$ on $l$ such
  that $p(l) = l'$.
\end{definition}

\begin{definition}[StablePermFun]
  A permutation function $p$ is a \textit{stable} permutation function
  on some list $l$ when for all indices $i < j$ where $l_i \equiv l_j$, we
  also have $p(i) < p(j)$.
\end{definition}

We can also modify the inductive definition of permutations to respect
equivalence. The problematic rule is \var{perm\_swap} because we could
porentially swap two equivalent elements. We will restrict it, so that
you can only swap $x$ and $y$ when $x \nequiv y$.

\begin{definition}[StablePermInd]
  \begin{gather*}
    \infer[\var{StablePermInd\_nil}]{\var{StablePermInd}~[]~[]}{}
    \\ \\
    \infer[\var{StablePermInd\_skip}]{\var{StablePermInd}~(x :: l)~(x :: l')}{%
      \var{StablePermInd}~l~l'
    }
    \\ \\
    \infer[\var{StablePermInd\_swap}]
          {\var{StablePermInd}~(x :: y :: l)~(y :: x :: l)}
          {x \not\equiv y}
    \\ \\
    \infer[\var{StablePermInd\_trans}]{\var{StablePermInd}~l~l''}{
      \var{StablePermInd}~l~l' & \var{StablePermInd}~l'~l''
    }
  \end{gather*}
\end{definition}

Finally, we can say that if two lists are in a stable permutation, and
we examine only the the elements equivalent to $x$ for some $x$, then
those elements should appear in the same order in both lists.
\begin{definition}[StablePerm]
\item Two lists $l$ and $l'$ are stable permutations of one another if
  for every $x$,
  \begin{equation*}
    \var{filter}~(\lambda y. y \equiv x)~l = \var{filter}~(\lambda y. y \equiv x)~l'
  \end{equation*}
\end{definition}
In a sense, this is a stricter version of
\cref{def:permutation_count}, because that can be rephrased as
$\var{filter}~(\lambda y. y = x)~l = \var{filter}~(\lambda y. y = x)~l'$.

From these definitions, we can see that stable sorts are restricted in
which permutation they produce. In fact, there is only one stable
permutation that is also sorted.
\begin{theorem}[StablePerm\_Sorted\_eq]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~l \implies \var{Sorted}~l' \implies
    \var{StablePerm}~l~l' \implies l = l'.
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $l$. In the inductive case, we have to
  show that
  \begin{equation*}
    h :: t = h' :: t'
  \end{equation*}
  given $\var{Sorted}~(h :: t)$, $\var{Sorted}~(h' :: t')$, and
  $\var{StablePerm}~(h :: t)~(h' :: t')$, as well as $t = t'$ from the
  induction hypothesis. So it suffices to show $h = h'$.

  First, we can prove that $h \equiv h'$. We know that $h \in h' :: t'$ and
  $h' \in h :: t$ because they are permutations of one another. If $h =
  h'$, then we are done, so assuming $h \neq h'$ we know $h \in t'$ and $h'
  \in t$. Because both $h::t$ and $h'::t'$ are sorted, we know that $h \le
  h'$ and $h' \le h$, implying $h \equiv h'$.

  From this we can prove that $h = h'$. From $\var{StablePerm}~(h ::
  t)~(h' :: t')$ we have
  \begin{align*}
    \var{filter}~(\equiv h)~(h :: t) &= \var{filter}~(\equiv h)~(h' :: t') \\
    h :: \var{filter}~(\equiv h)~t &= h' :: \var{filter}~(\equiv h)~t'
  \end{align*}
  We also have $t = t'$ by the induction hypothesis, so we can
  conclude that $h = h'$.
\end{proof}

This means that any two stable sorts must produce the same results for
every input, and are thus interchangeable.
\stablesortunique*

This can be generalized a little further: even if a sorting algorithm
itself is not a stable sorting algorithm, if the ordering we are using
doesn't allow us to distinguish between equivalent elements, then we
will not be able to tell if equivalent elements are being swapped.
More precisely, notice that it isn't always true that $x \equiv y$ implies
$x = y$. For example, if we are sorting strings by their first
character, then $\text{\texttt{apple}} \equiv \text{\texttt{armpit}}$ under
this ordering, but clearly $\text{\texttt{apple}} \neq
\text{\texttt{armpit}}$. When we do have that the equivalence ($\equiv$)
implies Leibniz equality ($=$), we say that we have an \textit{order},
as we have when we are sorting strings using all of their characters.
Under an order, as opposed to a preorder, we can prove that every
permutation is a stable permutation and every sort is a stable sort,
which implies that every sort is equivalent to every other sort.

\begin{theorem}[all\_perm\_stable]
  If we have an equivalence that implies Leibniz equality, then for
  all lists $l$ and $l'$
  \begin{equation*}
    \var{Permutation}~l~l' \implies \var{StablePerm}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the evidence of $\var{Permutation}~l~l'$.
  Every case other than the \var{perm\_swap} case follows easily.
  In the \var{perm\_swap} case, we must show that
  \begin{equation*}
    \var{StablePerm}~(y :: x :: l)~(x :: y :: l)
  \end{equation*}
  for all $x$ and $y$. If $x \not\equiv y$, we can just apply
  \var{StablePerm\_swap}. If $x \equiv y$, then we know that $x = y$, and
  we can show
  \begin{equation*}
    \var{StablePerm}~(x :: x :: l)~(x :: x :: l)
  \end{equation*}
  by applying \var{perm\_skip} twice.
\end{proof}

\begin{corollary}[Sort\_StableSort\_Ord]
  \label{thm:stablesort_ord}
  When sorting using an order,
  \begin{equation*}
    \var{Sort}~f \iff \var{StableSort}~f
  \end{equation*}
  for any $f$.
\end{corollary}

\begin{corollary}[Sort\_Ord\_unique]
  \label{thm:sort_ord_unique}
  When sorting using an order, for any two sorting function $f, g:
  \var{list}~A \to \var{list}~A$,
  \begin{equation*}
    (\forall l, f~l = g~l)
  \end{equation*}
\end{corollary}

\section{LSD radix sort}
\label{sec:radix_sort}

LSD radix sort is a simple radix sort that is used to
lexicographically sort strings of equal length. We can define what it
means to sort lexicographically:

\begin{definition}[lex\_le]
  Given a preorder~$\le$ on a type $A$, we can define another preorder,
  $\lexle$ on $\var{list}~A$.
  \begin{gather*}
    \infer[\var{lex\_le\_nil}]{[] \lexle ys}{}
    \\
    \\
    \infer[\var{lex\_le\_cons\_lt}]{(x :: xs) \lexle (y :: ys)}{%
      x < y
    }
    \\
    \\
    \infer[\var{lex\_le\_cons\_eq}]{(x :: xs) \lexle (y :: ys)}{%
      x \equiv y & xs \lexle ys
    }
  \end{gather*}
\end{definition}

LSD radix sort operates by conceptually placing all the strings in a
matrix, and by stably sorting on each column, from the end of the
string to the front (\cref{fig:lsd_radixsort}). This works because if
two strings match on a column, the stable sort ``defers'' to the
preexisting ordering, and that preexisting ordering was determined by
sorting by the tail of the strings. This mirrors the way a
lexicographic ordering ``defers'' to the rest of a string when two
strings match on earlier characters.

\begin{figure}
  \centering
  \begin{tt}
    \setlength{\tabcolsep}{0pt}
    \begin{tabular}{c}
    fit \\
    rib \\
    fly \\
    vat \\
    age \\
    mix \\
    row \\
    fog
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    ri&b& \\
    ag&e& \\
    fo&g& \\
    fi&t& \\
    va&t& \\
    mi&x& \\
    fl&y& \\
    ro&w&
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    v&a&t \\
    a&g&e \\
    r&i&b \\
    f&i&t \\
    m&i&x \\
    f&l&y \\
    f&o&g \\
    r&o&w
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &a&ge \\
    &f&it \\
    &f&ly \\
    &f&og \\
    &m&ix \\
    &r&ib \\
    &r&ow \\
    &v&at
    \end{tabular}
  \end{tt}
  \caption{LSD radix in-place}
  \label{fig:lsd_radixsort}
\end{figure}

In a functional setting, we can define an LSD radixsort elegantly by
repeatedly moving the last column to the front of the matrix and
sorting on the first column (\cref{fig:lsd_radixsort_func}). We will
take as a parameter the number of columns $n$.
\begin{equation*}
  \var{radixsort}~m~n = (\var{hdsort} \comp \var{map}~\var{rrot})^n~m
\end{equation*}

\begin{figure}
  \centering
  \begin{tt}
    \setlength{\tabcolsep}{0pt}
    \begin{tabular}{c}
    fit \\
    rib \\
    fly \\
    vat \\
    age \\
    mix \\
    row \\
    fog
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c}
    tfi \\
    bri \\
    yfl \\
    tva \\
    eag \\
    xmi \\
    wro \\
    gfo
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &b&ri \\
    &e&ag \\
    &g&fo \\
    &t&fi \\
    &t&va \\
    &x&mi \\
    &y&fl \\
    &w&ro
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    ibr \\
    gea \\
    ogf \\
    itf \\
    atv \\
    ixm \\
    lyf \\
    owr
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &a&tv \\
    &g&ea \\
    &i&br \\
    &i&tf \\
    &i&xm \\
    &l&yf \\
    &o&gf \\
    &o&wr
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    vat \\
    age \\
    rib \\
    fit \\
    mix \\
    fly \\
    fog \\
    row
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &a&ge \\
    &f&it \\
    &f&ly \\
    &f&og \\
    &m&ix \\
    &r&ib \\
    &r&ow \\
    &v&at
    \end{tabular}
  \end{tt}
  \caption{LSD radix using rotations}
  \label{fig:lsd_radixsort_func}
\end{figure}

Here \var{hdsort} is an insertion sort that only examines the first
character of each string. We can implement this by defining orderings
that compare elements using some key function:

\begin{definition}[keyOrd]
  Given a preorder on a type $K$ and a key function $\var{key} : A \to
  K$, we can define a preorder on $A$ that compares all elements using
  the key function. That is, we can define the relation
  $\le_{\var{key}}$ on $A$ such that for all $x, y : A$
  \begin{equation*}
    x \le_{\var{key}} y \coloneqq (\var{key}~x) \le (\var{key}~y)
  \end{equation*}
\end{definition}

Now we can define prefix lexicographic orderings on lists, where the
lexicographic ordering is only allowed to compare up to the first $n$
elements of two lists. We define the relation $\le_n$ on $\var{list}~A$
such that for all $xs, ys : \var{list}~A$,
\begin{equation*}
  xs \le_n ys \coloneqq (\var{firstn}~n~xs) \lexle (\var{firstn}~n~ys)
\end{equation*}
We also define $\var{PrefixSorted}~n$ as \var{Sorted} specialized to
the $\le_n$ ordering. \var{hdsort} is insertion sort specialized to the
$\le_1$ ordering.

We would like to prove that radix sort is a stable sort.

\begin{corollary}[radixsort\_correct]
  \label{thm:radixsort_correct}%
  For all matrices $m$ with $n$ columns,
  \begin{equation*}
    \var{Sorted}~(\var{radixsort}~m~n) \land \var{StablePerm}~(\var{radixsort}~m~n)~m
  \end{equation*}
\end{corollary}

For both properties, our approach will be to prove some invariant that
holds after $j$ iterations. For \var{StablePerm}, we can say that
after $j$ iterations of rotating and sorting, the matrix should be a
stable permutation of the original matrix had we rotated $j$ times.

\begin{restatable}[radixsort\_stable\_inv]{theorem}{radixsortstableinv}
  \label{thm:radixsort_stable_inv}
  For all matrices $m$ and $j : \nat$,
  \begin{equation*}
    \var{StablePerm}~(\var{radixsort}~m~j)~((\var{map}~\var{rrot})^j~m)
  \end{equation*}
\end{restatable}

For \var{Sorted}, the invariant will be that after $j$ iterations, the
matrix is sorted on the first $j$ columns.

\begin{restatable}[radixsort\_sorted\_inv]{theorem}{radixsortsortedinv}
  \label{thm:radixsort_sorted_inv}
  For all matrices $m$ with rows of length $n$, and $j \le n$,
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{radixsort}~m~j)
  \end{equation*}
\end{restatable}

To prove both of these invariants, we will have to be able to reason
about the relationship between the various orderings we are using: we
use $\le_1$ in \var{hdsort}, $\le_j$ in $\var{PrefixSorted}~j$, and
$\lexle \approx \le_n$ everywhere else.

Let's generalize, and consider two orders $\le_j$ and $\le_k$ such that $j
\le k$. If in the first $j$ elements we can conclude that $x <_j y$,
then examining more elements can't change that.

\begin{theorem}[key\_lt\_firstn\_ge]
  \label{thm:key_lt_firstn_ge}
  For all $j$ and $k$ such that $j \le k$,
  \begin{equation*}
    x <_j y \implies x <_k y
  \end{equation*}
\end{theorem}

We can rephrase this in terms of $\le$ and $\equiv$.

\begin{corollary}[key\_le\_firstn\_ge]
  For all $j$ and $k$ such that $j \le k$,
  \begin{equation*}
    x \le_k y \implies x \le_j y
  \end{equation*}
\end{corollary}
\begin{proof}
  This is the contrapositive of \cref{thm:key_lt_firstn_ge}.
\end{proof}

\begin{corollary}
  For all $j$ and $k$ such that $j \le k$,
  \begin{equation*}
    x \equiv_k y \implies x \equiv_j y
  \end{equation*}
\end{corollary}

These theorems show that $\equiv_j$ is weaker than $\equiv_k$ in that it can
distinguish between fewer elements. If we consider the implications
this has for stability, stability with respect to a weaker order is
actually a stronger property, because the weaker order, considering
more elements equivalent, is more restrictive about which swaps it
allows.

\begin{theorem}[StablePerm\_weaken]
  \label{thm:stableperm_weaken}
  Suppose we have a two preorders $\le$ and $\le'$ on $A$, and two lists
  $l_1,l_2 : \var{list}~A$, such that for all $x$ and $y$ in $l_1$,
  \begin{equation*}
    x \le' y \implies x \le y.
  \end{equation*}
  That is, $\le'$ is stronger than $\le$.

  Then, denoting by \var{StablePerm} stability relative to $\le$ and and
  by $\var{StablePerm}'$ stability relative to $\le'$,
  \begin{equation*}
    \var{StablePerm}~l_1~l_2 \implies \var{StablePerm}'~l_1~l_2
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason inductively on the proof of $\var{StablePerm}~l_1~l_2$.
  The important case is the swap case, where we have to show
  \begin{equation*}
    \var{StablePerm}'~(y :: x :: l)~(x :: y :: l)
  \end{equation*}
  given that $x \not\equiv y$.

  We have that $x \equiv' y \implies x \equiv y$, so $x \not\equiv y \implies x
  \not\equiv' y$.
\end{proof}

Now we can prove the stability invariant:
\radixsortstableinv*
\begin{proof}
  We reason by induction on $j$. In the inductive case, we have to prove
  \begin{equation*}
    \var{StablePerm}~(\var{radixsort}~m~(j+1))~((\var{map}~\var{rrot})^{j+1}~m)
  \end{equation*}
  Expanding $\var{radixsort}~m~(j+1)$:
  \begin{align*}
     \var{radixsort}~m~(j+1)
    ={}& (\var{hdsort} \comp \var{map}~\var{rrot})^{j+1}~m \\
    ={}& \var{hdsort}~(\var{map}~\var{rrot}~(\var{hdsort} \comp \var{map}~\var{rrot})^j~m)
  \end{align*}
  By \cref{thm:stableperm_weaken}, the stability relative to $\le_1$
  that is guaranteed by \var{hdsort} implies stability relative to
  $\lexle$, so we have only to show.
  \begin{align*}
    \var{StablePerm}~&(\var{map}~\var{rrot}~(\var{hdsort} \comp \var{map}~\var{rrot})^j~m) \\
                     &((\var{map}~\var{rrot})^{j+1}~m)
  \end{align*}
  By \cref{thm:stableperm_map_rrot} (proved below), we can take off
  the outer $\var{map}~\var{rrot}$ from both sides.
  \begin{equation*}
    \var{StablePerm}~
    ((\var{hdsort} \comp \var{map}~\var{rrot})^j~m)~((\var{map}~\var{rrot})^j~m)
  \end{equation*}
  This is exactly the induction hypothesis.
\end{proof}

\begin{lemma}[StablePerm\_map\_rrot]
  \label{thm:stableperm_map_rrot}
  For all matrices $m$ and $m'$,
  \begin{equation*}
    \var{StablePerm}~m~m' \implies
    \var{StablePerm}~(\var{map}~\var{rrot}~m)~(\var{map}~\var{rrot}~m').
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $\var{StablePerm}~m~m'$. The important
  case is the swap case, where we have to show
  \begin{equation*}
    \var{StablePerm}~(\var{map}~\var{rrot}~(y :: x :: l))~(\var{map}~\var{rrot}~(x :: y :: l))
  \end{equation*}
  given that $x \not\equivle y$.

  Here we can apply the swap ruel if we can show that $\var{rrot}~x
  \not\equivle \var{rrot}~y$. This follows from the fact that
  \begin{equation*}
    x \equivle y \iff \var{rrot}~x \equivle \var{rrot}~y \qedhere
  \end{equation*}
\end{proof}

We would also like to prove \cref{thm:radixsort_sorted_inv}. To do so,
we need to be able to reason about the process of adding a column to
the front of a sorted matrix and sorting on that column. Intuitively,
we need to know that if we have a matrix that is sorted on the first
$j$ columns, and we add a new column to the front and sort on it, the
result list will be sorted on the first $j+1$ columns.
\begin{restatable}[hdsort\_sorted\_S]{theorem}{hdsortsortedS}
  \label{thm:hdsortsortedS}
  For all matrices $m$ and naturals $j$,
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{map}~\var{tl}~m) \implies
    \var{PrefixSorted}~(j+1)~(\var{hdsort}~m)
  \end{equation*}
\end{restatable}
\begin{proof}
  See \cref{appendix:hdsort_sorted_S}.
\end{proof}

\radixsortsortedinv*
\begin{proof}
  We reason by induction on $j$. In the inductive case, we must prove.
  \begin{equation*}
    \var{PrefixSorted}~(j+1)~(\var{radixsort}~m~(j+1))
  \end{equation*}
  We can reason backwards from our goal. Unfolding \var{radixsort},
  \begin{equation*}
    \var{PrefixSorted}~(j+1)~(\var{hdsort}~(\var{map}~\var{rrot}~(\var{radixsort}~m~j)))
  \end{equation*}
  By \cref{thm:hdsortsortedS},
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{map}~\var{tl}~(\var{map}~\var{rrot}~(\var{radixsort}~m~j)))
  \end{equation*}
  Because $\var{tl} \comp \var{rrot} = \var{init}$,
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{map}~\var{init}~(\var{radixsort}~m~j))
  \end{equation*}
  Unfolding $\var{PrefixSorted}~j$,
  \begin{equation*}
    \var{Sorted}~(\var{map}~(\var{firstn}~j)~(\var{map}~\var{init}~(\var{radixsort}~m~j)))
  \end{equation*}
  Here, because $j$ is less than the length of all the rows,
  $\var{firstn}~j \comp \var{init} = \var{firstn}~j$.
  \begin{equation*}
    \var{Sorted}~(\var{map}~(\var{firstn}~j)~(\var{radixsort}~m~j))
  \end{equation*}
  Folding back up $\var{PrefixSorted}~j$, we have the induction hypothesis
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{radixsort}~m~j) \qedhere
  \end{equation*}
\end{proof}

\section{The Burrows-Wheeler transform}
\label{sec:bwt}

The Burrows--Wheeler transform was originally introduced as a pass in a
compression algorithm \cite{bw}, and continues to be used as a part of
the popular, open-source bzip2 compression software \cite{tsai_2016}.
Here we describe a standard compression algorithm built around BWT.
\begin{enumerate}
\item Apply the BWT, which tends to create runs of identical
  characters.
\item Apply Move-to-Front (MTF) encoding, in which a character is
  represented by the number of different characters encountered since
  the last occurrence of this character. This creates many runs of
  zeroes from the runs in the output of BWT. On English texts, often
  50-60\% of the values produced by the MTF pass are zeroes
  \cite{fenwick2007}.
\item Apply Run-Length Encoding (RLE) to encode only the runs of
  zeroes with their lengths, using a bijective base-2 encoding
  \cite{bw-analysis, tsai_2016}.
\item Compress with some zeroth-order coder like a Huffman coder.
\end{enumerate}

The BWT is also at the heart of algorithms that exploit the
relationship between the rotation matrix in the BWT and the suffix
array data structure in order to search for patterns in a compressed
text \cite{ferragina_index}. These techniques have been applied to
enable fast matching of DNA subsequences \cite{dna}.

To apply the BWT to a string of length $n$:
\begin{enumerate}
\item Form the \(n \times n\) matrix of all cyclic shifts of the
  original string (referred to as the rotation matrix). Starting
  with the original string, each row is formed from the previous one
  by removing the first character and sticking it on the end.
\item Sort the rows lexicographically
\item Take the last column of the matrix. That string is the result of
  the transformation.
\item Note the index where the original string appears in the sorted
  rotation matrix. This piece of information is necessary to reverse
  the transformation.
\end{enumerate}
As shown in \cref{fig:bw_ex}, applying the algorithm to the string
\var{abracadabra!} produces the new string \var{ard!rcaaabb} and the
index \var{3}.

\begin{figure}
  \centering
  \begin{tt}
  \begin{tabular}{rc}
    0  & abracadabra! \\
    1  & bracadabra!a \\
    2  & racadabra!ab \\
    3  & acadabra!abr \\
    4  & cadabra!abra \\
    5  & adabra!abrac \\
    6  & dabra!abraca \\
    7  & abra!abracad \\
    8  & bra!abracada \\
    9  & ra!abracadab \\
    10 & a!abracadabr \\
    11 & !abracadabra
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{rc}
    0  & !abracadabr\textbf{a} \\
    1  & a!abracadab\textbf{r} \\
    2  & abra!abraca\textbf{d} \\
    \textit{3} & \textit{abracadabra\textbf{!}} \\
    4  & acadabra!ab\textbf{r} \\
    5  & adabra!abra\textbf{c} \\
    6  & bra!abracad\textbf{a} \\
    7  & bracadabra!\textbf{a} \\
    8  & cadabra!abr\textbf{a} \\
    9  & dabra!abrac\textbf{a} \\
    10 & ra!abracada\textbf{b} \\
    11 & racadabra!a\textbf{b}
  \end{tabular}
  \end{tt}
  \caption{BWT acting on \var{abracadabra!}}
  \label{fig:bw_ex}
\end{figure}

The BWT tends to produce runs like the three \var{a}s or the two
\var{b}s in \var{ard!rcaaabb}, and it is this feature that makes the
resulting string easier to compress. It creates those runs by
exploiting the correlation between adjacent characters. In many
real-world inputs, adjacent characters are correlated, which means
that by knowing one character, one can predict the character that
follows it with some success. Sorting the rotation matrix
lexicographically brings together rows that start with the same
characters. But, the beginning and end of each row were adjacent in
the original string, so this will also tend to bring together rows
that end with the same characters.

\section{Inverting the BWT}
\label{sec:invert_bwt}

Although it is not obvious, the transformed string and the index
provide enough information to recover the the original string.
Conceptually, we can find the original string by using the transformed
string to reconstruct the entire sorted rotation matrix, and then
using the index to find the row containing the original string. This,
though, relies on our ability to reconstruct the sorted rotation
matrix from just its last column.

A key insight is that because of \cref{thm:sort_ord_unique}, we can
pretend that during the forwards transform, the rotation matrix is
being sorted using any sorting algorithm we find convenient.
Specifically, if we examine how an LSD radixsort sorts the rotation
matrix (\cref{fig:radixsort_rots}) we notice an interesting pattern.
Each column of the rotation matrix is a permutation of the original
string, so when we sort a on particular column, we always produce the
same result. As we move right-to-left, the column containing the
sorted input string moves with us until it reaches the left edge of
the matrix.

\begin{figure*}
  \centering
  \begin{tt}
  \setlength{\tabcolsep}{0pt}
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    abracadabra&!& \\
    bracadabra!&a& \\
    cadabra!abr&a& \\
    dabra!abrac&a& \\
    bra!abracad&a& \\
    !abracadabr&a& \\
    racadabra!a&b& \\
    ra!abracada&b& \\
    adabra!abra&c& \\
    abra!abraca&d& \\
    acadabra!ab&r& \\
    a!abracadab&r&
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    racadabra&!&ab \\
    bracadabr&a&!a \\
    acadabra!&a&br \\
    a!abracad&a&br \\
    dabra!abr&a&ca \\
    bra!abrac&a&da \\
    cadabra!a&b&ra \\
    !abracada&b&ra \\
    abra!abra&c&ad \\
    ra!abraca&d&ab \\
    abracadab&r&a! \\
    adabra!ab&r&ac
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    a&!&abracadabr \\
    r&a&!abracadab \\
    d&a&bra!abraca \\
    !&a&bracadabra \\
    r&a&cadabra!ab \\
    c&a&dabra!abra \\
    a&b&ra!abracad \\
    a&b&racadabra! \\
    a&c&adabra!abr \\
    a&d&abra!abrac \\
    b&r&a!abracada \\
    b&r&acadabra!a
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &!&abracadabra \\
    &a&!abracadabr \\
    &a&bra!abracad \\
    &a&bracadabra! \\
    &a&cadabra!abr \\
    &a&dabra!abrac \\
    &b&ra!abracada \\
    &b&racadabra!a \\
    &c&adabra!abra \\
    &d&abra!abraca \\
    &r&a!abracadab \\
    &r&acadabra!ab
  \end{tabular}
  \end{tt}
  \caption{Sorting the rotation matrix with LSD radixsort. The
    highlighted column contains the sorted original string.}
  \label{fig:radixsort_rots}
\end{figure*}

We also can notice that the columns to the right of the sorted column
move with it for similar reasons. The column directly to its right is
the result of sorting the input string, and then stably sorting it
again by the character that precedes it in the original string. The
next column is the result of sorting by each character, sorting by the
preceding character, and then sorting by the character 2 spaces in
front.

This suggests that we can recreate the matrix from the last column by
sorting it to produce the first column, then repeatedly prepending the
last column to the growing matrix and stably sorting by it
(\cref{fig:recreate}. As the matrix grows from right to left, we are
mimicing the process of LSD radix sort in \cref{fig:radixsort_rots},
in that the sorted portions to the right of the shaded column are
identical.

\begin{figure*}
  \centering
  \begin{tt}
  \setlength{\tabcolsep}{0pt}
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{aaaaaaaaaaa}&!& \\
    \textcolor{lightgray}{rrrrrrrrrrr}&a& \\
    \textcolor{lightgray}{ddddddddddd}&a& \\
    \textcolor{lightgray}{!!!!!!!!!!!}&a& \\
    \textcolor{lightgray}{rrrrrrrrrrr}&a& \\
    \textcolor{lightgray}{ccccccccccc}&a& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&b& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&b& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&c& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&d& \\
    \textcolor{lightgray}{bbbbbbbbbbb}&r& \\
    \textcolor{lightgray}{bbbbbbbbbbb}&r&
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{aaaaaaaaa}&!&ab \\
    \textcolor{lightgray}{rrrrrrrrr}&a&!a \\
    \textcolor{lightgray}{ddddddddd}&a&br \\
    \textcolor{lightgray}{!!!!!!!!!}&a&br \\
    \textcolor{lightgray}{rrrrrrrrr}&a&ca \\
    \textcolor{lightgray}{ccccccccc}&a&da \\
    \textcolor{lightgray}{aaaaaaaaa}&b&ra \\
    \textcolor{lightgray}{aaaaaaaaa}&b&ra \\
    \textcolor{lightgray}{aaaaaaaaa}&c&ad \\
    \textcolor{lightgray}{aaaaaaaaa}&d&ab \\
    \textcolor{lightgray}{bbbbbbbbb}&r&a! \\
    \textcolor{lightgray}{bbbbbbbbb}&r&ac
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{a}&!&abracadabr \\
    \textcolor{lightgray}{r}&a&!abracadab \\
    \textcolor{lightgray}{d}&a&bra!abraca \\
    \textcolor{lightgray}{!}&a&bracadabra \\
    \textcolor{lightgray}{r}&a&cadabra!ab \\
    \textcolor{lightgray}{c}&a&dabra!abra \\
    \textcolor{lightgray}{a}&b&ra!abracad \\
    \textcolor{lightgray}{a}&b&racadabra! \\
    \textcolor{lightgray}{a}&c&adabra!abr \\
    \textcolor{lightgray}{a}&d&abra!abrac \\
    \textcolor{lightgray}{b}&r&a!abracada \\
    \textcolor{lightgray}{b}&r&acadabra!a
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &!&abracadabra \\
    &a&!abracadabr \\
    &a&bra!abracad \\
    &a&bracadabra! \\
    &a&cadabra!abr \\
    &a&dabra!abrac \\
    &b&ra!abracada \\
    &b&racadabra!a \\
    &c&adabra!abra \\
    &d&abra!abraca \\
    &r&a!abracadab \\
    &r&acadabra!ab
  \end{tabular}
  \end{tt}
  \caption{Recreating the rotation matrix from the last column.}
  \label{fig:recreate}
\end{figure*}

The choice of the last column is crucial, because as the ``original
string'' in the stable column sort, it acts as the tie-breaker. The
last column is the solution to the fixpoint equation
\begin{equation*}
  \var{hdsort}~(\var{prefix\_shift}~c~m) = m
\end{equation*}
where \var{hdsort} sorts on the first column of a matrix,
$\var{prefix\_shift}~c$ adds $c$ as the first column of a matrix,
dropping the last column, and $m$ is the rotation matrix. This means
as we repeatedly prefix $l$ and sort, we preserve the ``rest of the
matrix'', even the parts that we have yet to reconstruct.

We can think of the process of sorting the matrix as computing the
solution to the fixpoint equation, subject to the constraints of the
lexicographic ordering. For example, we know that the collection of
all characters preceding an \texttt{a} in the sorted column must be
some permutation of
$[\texttt{c},~\texttt{d},~\texttt{r},~\texttt{r},~\texttt{!}]$,
because those are the characters that precede \texttt{a}s in the
original string. However, the order of these characters in the final
string
($[\texttt{r},~\texttt{d},~\texttt{!},~\texttt{r},~\texttt{c}]$) is
important. An \texttt{r} must be first, because in the original string
there is an \texttt{a} following an \texttt{r} which is itself
followed by a \texttt{!}, the smallest character. Similarly, although
the \texttt{d} and \texttt{!} both precede \texttt{a}s that are
followed by \texttt{bra}s, the next character for the \texttt{d} is a
\texttt{!}, and the next character for the \texttt{!} is a \texttt{c}.
This requires that the \texttt{d} precede the \texttt{!} in the last
column.

Generally, there are constraints on the relative order of the letters
in the column before the sorted column, and in each step we are
ensuring that these constraints are not violated one more character
deep in the string. By the time we have reached the last column, we
have satisfied all the constraints.

\section{Implementing the forwards BWT}
\label{sec:forwards_BWT}

First, we define the left and right rotations of a list. \var{lrot}
moves the first element of a list to the end.
\begin{lstlisting}
Definition lrot (l: list A) : list A :=
  match l with
  | [] => []
  | hd :: tl => tl ++ [hd]
  end.
\end{lstlisting}
\var{rrot} moves the last element of a list to the front, and
relies on \var{init}, which drops the last element of a list.
\begin{lstlisting}
Definition rrot (l: list A) : list A :=
  match l with
  | [] => []
  | hd :: tl => last l hd :: init l
  end.
\end{lstlisting}
Note that in order to ensure that $\var{last}$ is total, it takes
an extra argument to return in the case of an empty list.

Now we produce the rotation matrix by repeatedly applying \var{lrot}.
We will use \var{iter}, which takes a function $f : A \to A$, a number
$n$, and an initial value $z$, and returns a list of length $n$ formed
by repeatedly applying $f$ to $z$:
\begin{equation*}
  \var{iter}~f~n~z = [z,~f~z,~f~(f~z),~\ldots,~f^{n-1} z]
  \label{eq:iter}
\end{equation*}
We use the notation $f^i z$ for $\var{rep}~f~i~z$, which implements
repeated function application. We will also use the notation $l_i$ for
the $i$th element of a list, though in Coq, that is given by
$\var{nth}~i~l~d$, where $d:A$ is a default value in case $i \ge
\var{length}~l$.

Now we can define \var{rots} to produce the rotation matrix.
\begin{lstlisting}
Definition rots (l : list A) : list (list A) :=
  iter lrot (length l) l.
\end{lstlisting}

\var{bwl} produces the transformed string by taking the last column of
the sorted rotation matrix. \var{lexsort} sorts the rotation matrix
using insertion sort (\cref{appendix:insertion_sort}) according to the
induced lexicographic ordering(\cref{sec:lex_ord}).
\begin{lstlisting}
Definition bwl (l : list A) : list A :=
  match l with
  | [] => []
  | hd :: _ =>
    List.map (fun x => last x hd) (lexsort (rots l))
  end.
\end{lstlisting}

\var{bwn} returns the index in the sorted rotation matrix where the
original string appears. It relies on \var{findIndex}, which searches
a list for an element $y$ that is equal to the given element $x$, and
returns the index of $y$.
\begin{lstlisting}
Definition bwn (l : list A) : nat :=
  findIndex l (lexsort (rots l)).
\end{lstlisting}

Given these definitions, we can quickly prove that \var{bwn}
satisfies its specification using a few lemmas.
\begin{lemma}[findIndex\_correct]
  For all elements $x$ and lists $l$,
  \begin{equation*}
    (\exists y \in l,~x \equiv y) \implies l_{(\var{findIndex}~x~xs)} \equiv x
  \end{equation*}
\end{lemma}
\begin{lemma}[orig\_in\_sorted\_rots]
  For every non-empty list $l$,
  \begin{equation*}
    \exists y \in \var{lexsort}~(\var{rots}~l),~x = y
  \end{equation*}
\end{lemma}
\begin{restatable}[bwn\_correct]{theorem}{bwncorrect}
  \label{thm:bwn_correct}
  For every non-empty list $xs$,
  \begin{equation*}
    (\var{lexsort}~(\var{rots}~xs))_{(\var{bwn}~xs)} = xs
  \end{equation*}
\end{restatable}

\section{Impelementing the inverse BWT}
\label{sec:inverse_BWT}

Now we want to construct a function \var{unbwt} such that for all $l$,
\begin{equation}
  \var{unbwt}~(\var{bwn}~l)~(\var{bwl}~l) = l
  \label{eq:unbwt}
\end{equation}
If we could recreate the entire sorted rotation matrix from
$\var{bwl}~l$, it would be simple to recover $l$ by indexing into
the matrix at $\var{bwn}~l$.

More precisely, suppose we had a function function \var{recreate}
such that
\begin{equation}
  \label{eq:recreate}
  \var{recreate}~(\var{bwl}~l) = \var{lexsort}~(\var{rots}~l)
\end{equation}
Then, we could define \var{unbwt} as
\begin{lstlisting}
Definition unbwt (i : nat) (l : list A) : list A :=
  nth i (recreate l) l.
\end{lstlisting}
and \cref{eq:unbwt} would follow directly from \cref{thm:bwn_correct}.

We implement \var{recreate} as follows, introducing a parameter $j$ so
that we can recursively recreate the first $j$ columns of the matrix,
one column at a time. Here $l$ is meant to be the output of the BWT,
the last column of the sorted rotation matrix.
\begin{lstlisting}
Fixpoint recreate (j : nat) (l : list A)
  : list (list A) :=
  match j with
  | O    => map (const []) l
  | S j' => hdsort (prepend_col l (recreate j' l))
  end.
\end{lstlisting}
This definition relies on \var{prepend\_col}, which prepends a column
to a matrix.

Now we can redefine \var{unbwt} as
\begin{lstlisting}
Definition unbwt (i : nat) (l : list A) : list A :=
  nth i (recreate (length l) l) l.
\end{lstlisting}

Since we recreate the matrix column by column, we can define an
invariant based on \cref{eq:recreate} that uses $\var{cols}~j$, which
takes the first $j$ columns of a matrix.
\begin{restatable*}[recreate\_correct\_inv]{theorem}{recreatecorrectinv}
  For all lists $l$ and $j \le \var{length}~l$,
  $\var{recreate}~j~(\var{bwl}~l) =
  \var{cols}~j~(\var{lexsort}~(\var{rots}~l))$
\end{restatable*}

If we can prove this theorem, we will be able to prove that our
implementation of \var{unbwt} is correct along the same lines as
before. To do so, we need to prove a few other theorems that
have already come up when explaining the algorithm conceptually.

First, we note that moving the last column of the rotation matrix to the front
is the same as moving the last row to the top, because the rotation
matrix is symmetric.
\begin{theorem}[map\_rrot\_rots]
  For all lists $l$,
  \begin{equation*}
    \var{map}~\var{rrot}~(\var{rots}~l) = \var{rrot}~(\var{rots}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  Letting $n = \var{length}~l$
  \begin{align*}
       &\var{map}~\var{rrot}~(\var{rots}~l) \\
    ={}&\var{map}~\var{rrot}~[l,~\var{lrot}~l,~\var{lrot}^2~l,~\ldots,~\var{lrot}^{n-1}~l] \\
    ={}&[\var{rrot}~l,~l,~\var{lrot}~l,~\ldots,~\var{lrot}^{n-2}~l] \\
    ={}&[\var{lrot}^{n-1}~l,~l,~\var{lrot}~l,~\ldots,~\var{lrot}^{n-2}~l] \\
    ={}&\var{rrot}~[l,~\var{lrot}~l,~\ldots,~\var{lrot}^{n-1}~l] \\
    ={}&\var{rrot}~(\var{rots}~l) \qedhere
  \end{align*}
\end{proof}

Now we can prove that the last column is indeed a fixpoint.
\begin{theorem}[lexsort\_rots\_hdsort]
  For all lists $l$,
  \begin{equation*}
    (\var{hdsort} \comp
    \var{map}~\var{rrot})~(\var{lexsort}~(\var{rots}~l)) =
    \var{lexsort}~(\var{rots}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  By \cref{thm:sort_ord_unique}, any two sorts are equivalent, and we
  can replace \var{lexsort} with an LSD radixsort implemented as
  \begin{equation*}
    \var{radixsort}~m \coloneqq (\var{hdsort} \comp \var{map}~\var{rrot})^n~m
  \end{equation*}
  where $m$ is a matrix with $n$ columns. In this case, we are sorting
  the rotation matrix and we let $n = \var{length}~l$. Then we have
  \begin{align*}
       &(\var{hdsort} \comp \var{map}~\var{rrot})~(\var{lexsort}~(\var{rots}~l)) \\
    ={}&(\var{hdsort} \comp \var{map}~\var{rrot})~((\var{hdsort} \comp \var{map}~\var{rrot})^n~(\var{rots}~l)) \\
    ={}&(\var{hdsort} \comp \var{map}~\var{rrot})^{n+1}~(\var{rots}~l) \\
    ={}&(\var{hdsort} \comp \var{map}~\var{rrot})^n~((\var{hdsort} \comp \var{map}~\var{rrot})~(\var{rots}~l)) \\
    ={}&\var{lexsort}~((\var{hdsort} \comp \var{map}~\var{rrot})~(\var{rots}~l)) \\
    ={}&\var{lexsort}~((\var{hdsort} \comp \var{rrot})~(\var{rots}~l)) \\
    ={}&\var{lexsort}~(\var{rots}~l)
  \end{align*}
  The last step follows because the function $\var{hdsort} \comp
  \var{rrot}$ permutes the rotation matrix.
\end{proof}

We need two more lemmas that describe the interaction between
\var{cols} and $\var{map}~\var{rrot}$.

\begin{theorem}[cols\_hdsort\_comm]
  For all matrices $m$ and naturals $j$,
  \begin{equation*}
    \var{cols}~(j+1)~(\var{hdsort}~m) = \var{hdsort}~(\var{cols}~(j+1)~m)
  \end{equation*}
\end{theorem}
\begin{proof}
  \var{hdsort} only examines the first elements of the rows of $m$,
  and $\var{cols}~(j+1)$ doesn't alter the first elements of the rows
  of $m$.\footnote{This argument is made precise by
    \var{key\_sort\_inv} in \cref{sec:lex_ord}.}
\end{proof}
\begin{theorem}[cols\_map\_rrot]
  For all $j$ and matrices $m$ that has rows all of length less than $j$,
  \begin{gather*}
    \var{cols}~(j+1)~(\var{map}~\var{rrot}~m) = \\
    \var{prepend\_col}~(\var{map}~\var{last}~m)~(\var{cols}~j~m)
  \end{gather*}
\end{theorem}
\begin{proof}
  We reason by induction on $m$. In the inductive case we need to show that
  \begin{gather*}
    \var{cols}~(j+1)~(\var{map}~\var{rrot}~(r::m)) = \\
    \var{prepend\_col}~(\var{map}~\var{last}~(r::m))~(\var{cols}~j~(r::m))
  \end{gather*}
  Simplifying, we have that the tails are equal by the induction
  hypothesis, and we must show that the heads are equal.
  \begin{align*}
    \var{firstn}~(j+1)~(\var{rrot}~r) &= \var{last}~r :: \var{firstn}~j~r \\
    \var{firstn}~(j+1)~(\var{last}~r :: \var{init}~r) &= \var{last}~r
    :: \var{firstn}~j~r \\
    \var{last}~r :: \var{firstn}~j~(\var{init}~r) &= \var{last}~r :: \var{firstn}~j~r \\
    \var{firstn}~j~(\var{init}~r) &= \var{firstn}~j~r
  \end{align*}
  The bottom equation is true because $j > \var{length}~r$, so if $j=
  0$, $r = []$.
\end{proof}

Now we are ready to prove the invariant:
\recreatecorrectinv
\begin{proof}
  Letting $m = \var{lexsort}~(\var{rots}~l)$,
  \begin{align*}
    &\var{cols}~(j+1)~m \\
    ={}& \var{cols}~(j+1)~(\var{hdsort}~(\var{map}~\var{rrot}~m)) \\
    ={}&
    \var{hdsort}~(\var{cols}~(j+1)~(\var{map}~\var{rrot}~m)) \\
    ={}&
    \var{hdsort}~(\var{prepend\_col}~(\var{map}~\var{last}~m)~(\var{cols}~j~m)) \\
    ={}&
    \var{hdsort}~(\var{prepend\_col}~(\var{bwl}~l)~(\var{recreate}~j~(\var{bwl}~l))) \\
    ={}& \var{recreate}~j~(\var{bwl}~l) \qedhere
  \end{align*}
\end{proof}

\begin{theorem}[cols\_id]
  For all matrices $m$ such that all the rows of $m$ have length less
  than or equal to $n$,
  \begin{equation*}
    \var{cols}~n~m = m
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows by induction on $m$.
\end{proof}

\unbwtcorrect
\begin{proof}
\begin{align*}
     \var{unbwt}~(\var{bwn}~l)~(\var{bwl}~l)
  &= (\var{recreate}~(\var{length}~l))(\var{bwl}~l))_{(\var{bwn})~l}\\
  &= (\var{cols}~(\var{length}~l)~(\var{lexsort}~(\var{rots}~l)))_{(\var{bwn})~l} \\
  &= (\var{lexsort}~(\var{rots}~l))_{(\var{bwn})~l} \\
  &= l \qedhere
\end{align*}
\end{proof}

\section{Relationship to the standard BWT}
\label{sec:opt}

Standard implementations of the BWT do not actually reconstruct the
entire rotation matrix, as this would be inefficient. Instead, they
avoid this using the optimization described in this section. Although,
we did not prove or implement this optimization in Coq, it would be
straightforward to do so.

In \var{recreate}, we repeatedly prepend the last column to the front
of the matrix and sort on it. However, each time that column is the
same, so the permutation that takes us from the unsorted column to the
sorted column is the same. Therefore, we can calculate that
permutation function once and store it in a variable $p$. The $i$th
column of the recreated matrix is simply $(\var{apply}~p)^i~f$ where
$f$ is the first column of the sorted rotation matrix, so we can
define
\begin{lstlisting}
  Definition recreate' (p : list nat) (l : list A)
  : list (list A) :=
  transpose (iter (apply p) (length l) (apply p l))
\end{lstlisting}
In the definition above, we are repeatedly applying $p$ to the
first column of the rotation matrix, $\var{apply}~p~l$. Then we
take the transpose to create the matrix from the columns.

Since in $\var{unbwt}~i~l$, we want the $i$th row of the matrix, we
can eliminate the transpose by replacing the $\var{nth}~i$ with a
$\var{map}~\var{nth}~i$ Assuming we already have calculated $p$,
\begin{align*}
  \var{unbwt}~i~l &= (\var{recreate}~p~l)_i \\
  &= (\var{transpose}~(\var{iter}~(\var{apply}~p)~(\var{length}~l)~(\var{apply}~p~l)))_i \\
  &= \var{map}~(\lambda
  r.~r_i)~(\var{iter}~(\var{apply}~p)~(\var{length}~l)~(\var{apply}~p~l)) \\
  &= \var{map}~(\lambda r.~r_i)~(\var{tl}~(\var{iter}~(\var{apply}~p)~(1 +
  \var{length}~l)~l)) \\
  &= \var{tl}~(\var{map}~(\lambda r.~r_i)~(\var{iter}~(\var{apply}~p)~(1 + \var{length}~l)~l))
\end{align*}

Now, instead of indexing into each column at $i$, where the columns
are all the result of successively applying $p$ to $l$, we will
successively take the image of $i$ under $p$ to generate the indices
into $l$ where those characters appear. This is justified via the
following theorem:
\begin{theorem}
  For all $n$,
  \begin{equation*}
    \var{map}~(\lambda r.~r_i)~(\var{iter}~(\var{apply}~p)~n~l)
    =
    \var{map}~(\lambda j.~l_j)~(\var{iter}~(\var{image}~p)~n~i)
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{align*}
    & \var{map}~(\lambda r.~r_i)~(\var{iter}~(\var{apply}~p)~n~l) \\
    ={}& \var{map}~(\lambda r.~r_i)~[l,~(\var{apply}~p)~l,~(\var{apply}~p)^2~l,~\ldots,~(\var{apply}~p)^{n-1}~l] \\
    ={}& [l_i,~((\var{apply}~p)~l)_i,~((\var{apply}~p)^2~l)_i,~\ldots,~((\var{apply}~p)^{n-1}~l)_i] \\
    ={}& [l_i,~l_{p(i)},~l_{p^2(i)},~\ldots,~l_{p^{n-1}~i}] \\
    ={}& \var{map}~(\lambda j.~l_j)~[i,~p(i),~p^2(i),~\ldots,~p^{n-1}~i] \\
    ={}& \var{map}~(\lambda j.~l_j)~(\var{iter}~(\var{image}~p)~n~i) \qedhere
  \end{align*}
\end{proof}

So assuming we have some function $\var{calc\_sort\_perm} :
\var{list}~A \to \var{list}~\nat$ to calculate the permutation
function that sorts $l$, we redefine \var{unbwt} as
\begin{lstlisting}
  Definition unbwt' (i : nat) (l : list A) : list A :=
  match l with
  | [] => []
  | d :: _ =>
  let p := calc_sort_perm l in
  let indices :=
  tl (iter (image p) (S (length l)) i)
  in map (fun j => nth l j d) indices
\end{lstlisting}
\var{calc\_sort\_perm} can be implemented in linear time using a
counting sort, so if \var{nth} took constant time, then \var{unbwt}
would take linear time as well. As a functional specification, this
closely matches the standard imperative algorithm for the inverse BWT.

\section{Conclusions}
We have implemented and proved that the the inverse BWT inverts the
forwards BWT, basing our work on Bird et~al.'s program calculation
derivation \cite{birdmu,pearls}. In the process, we defined the
properties of stable sorts, and implemented and proved correct an LSD
radixsort.

This paper shows that Bird's style of program calculation is very
useful for building machine-checked proofs. It leads to proofs that
are composed of many smaller lemmas, each justifying a single rewrite
step. Most of the time and effort went into proving the correctness of
radixsort. If we exclude the parts related to sorting or to other
general-purpose functions not specific to the BWT, the entire verified
implementation of the BWT consists of 42 lines of Gallina definitions,
7 lines of Gallina that are used only in proofs, and 823 lines of
theorem statements and Ltac code.\footnote{This includes
  \var{BurrowsWheeler.v}, \var{Columns.v}, \var{Rotations/Rotation.v},
  \var{Rotations/Rots.v}, \var{Lib/Iterate.v}, \var{Lib/Repeat.v} and
  \var{Lib/FindIndex.v}, which implement \var{bwl}, \var{bwn},
  \var{unbwt}, \var{cols}, \var{prepend\_col}, \var{lrot}, \var{rrot},
  \var{rots}, \var{iter}, and \var{rep}, \var{findIndex}.}

This implementation, including the optimizations mentioned in
\cref{sec:opt}, can be used as a functional specification to for a
verified implementation of bzip2. Of course, it would also be
necessary to have functional specifications for the other phases of
bzip2, such as Run-Length Encoding, Move-to-Front Encoding, and
arithmetic coding. As luck would have it, Bird has already analyzed
arithmetic coding \cite{pearls}, and the other two phases are
relatively simple.

\begin{acks}
  Thanks to Professor Andrew Appel for being extremely generous with his
  time and his guidance. He was always available for questions, and
  allowed me to focus my work on the areas I found interesting.

  I'd also like to thank Matthew Weaver, Lennart Berringer, and Qinshi
  Wang for answering my questions throughout the year. Zachary Kincaid
  provided useful comments on a draft of this paper.

  I'd like to thank Benjamin Huang, Yael Marans, and Eli Waksbaum for
  their help and support.
\end{acks}

\bibliography{draft_paper}

\appendix

\section{Additional Proofs}
\label{appendix:proofs}

\sortedixiff*
\begin{proof}
  We reason by induction on $l$. The base cases are easy, so we
  address the inductive case, $l = a :: l'$

  ($\implies$) We have to prove that $(a :: l')_i \le (a :: l')_j$ given
  $i \le j$ and $\var{Sorted}~l$. We reason by cases on $i$ and $j$:
  \begin{itemize}
  \item $i = j = 0$. By reflexivity, $a \le a$.
  \item $i = 0,~j=j' + 1$. We need to show $a \le l'_{j'}$. We know that
    $a \le x$ for all $x \in l'$ by $\var{Sorted}~(a::l')$, so we are
    done.
  \item $i = i' +1,~j = j' + 1$. We need to show $l'_{i'} \le l'_{j'}$,
    which follows from the induction hypothesis.
  \end{itemize}

  ($\impliedby$) We have $\var{SortedIx}\ \allowbreak(a :: l')$, and
  we need to show $\var{Sorted}\ \allowbreak(a :: l')$ given the
  induction hypothesis $\var{SortedIx}~l' \implies \var{Sorted}~l'$.
  By \var{SortedLocal\_iff}, we can just prove
  $\var{SortedLocal}\ \allowbreak(a :: l')$. If $l' = []$, we can
  apply $\var{SortedLocal\_1}$, so assume $l' = b :: l''$. To apply
  \var{SortedLocal\_cons} we need to show
  $\var{SortedLocal}\ \allowbreak(b :: l'')$, which we have by the
  induction hypothesis and $\var{SortedIx}\ \allowbreak(a :: b ::
  l'')$, and that $a \le b$. That follows because $a = (a :: b ::
  l'')_0$ and $b = (a :: b :: l'')_1$, so by
  $\var{SortedIx}\ \allowbreak(a :: b :: l'')$, we are done.
\end{proof}


\section{Insertion sort}
\label{appendix:insertion_sort}

Insertion sort is a simple and elegant stable sort that is easy to
reason about. The theorems proved in \cref{sec:unique} allow us to
prove things using insertion sort that apply to any stable sort. For
example, our proof of correctness for radixsort relies on the fact
that it uses insertion sort as the inner sort, but
\cref{thm:stablesort_unique} allows us to extend that proof to a
radixsort that uses any stable sort. We also use it as the
implementation of \var{lexsort} because it is convenient, but
similarly \cref{thm:sort_ord_unique} ensures that it can be replaced with
any other sort.

\begin{lstlisting}
  Fixpoint insert (x : A) (l : list A) :=
  match l with
  | [] => [x]
  | h :: t =>
  if le_dec x h then x :: h :: t else h :: insert x t
  end.

  Fixpoint sort (l : list A) : list A :=
  match l with
  | [] => []
  | h :: t => insert h (sort t)
  end.
\end{lstlisting}

\begin{lemma}[insert\_perm]
  For all lists $l$ and elements $x$,
  \begin{equation*}
    \var{Permutation}~(x::l)~(\var{insert}~x~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $l$. In the inductive case, we need to show
  \begin{equation*}
    \var{Permutation}~(x::h::t)~(\var{insert}~x~(h::t))
  \end{equation*}
  given the induction hypothesis that
  \begin{equation*}
    \var{Permutation}~(x::t)~(\var{insert}~x~t)
  \end{equation*}
  If $x \le h$, then $\var{insert}~x~(h::t) = x :: h :: t$ and we are
  done. Otherwise, $\var{insert}~x~(h::t) = h :: \var{insert}~x~t$ and
  we can apply the induction hypothesis.
\end{proof}

\begin{theorem}[sort\_perm]
  For all lists $l$,
  \begin{equation*}
    \var{Permutation~l}~(\var{sort}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof follows by induction on $l$. The inductive case is
  \begin{equation*}
    h :: t \eqperm h :: \var{sort}~t \eqperm
    \var{insert}~h~(\var{sort}~t) = \var{sort}~(h :: t)
  \end{equation*}
  where the first step follows from the induction hypothesis, and the
  second from \var{insert\_perm}.
\end{proof}

\begin{lemma}[insert\_sorted]
  For all lists $l$ and elements $x$,
  \begin{equation*}
    \var{Sorted}~l \implies \var{Sorted}~(\var{insert}~x~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on the proof that $\var{Sorted}~l$. In the
  inductive case we need prove that
  \begin{equation*}
    \var{Sorted}~(\var{insert}~x~(h::t))
  \end{equation*}
  If $x \le h$, then $\var{insert}~x~(h::t) = x :: h :: t$, and by
  \var{SortedLocal\_cons} we are done.

  If $x > h$, $\var{insert}~x~(h::t) = h :: \var{insert}~x~t$, and it
  suffices to show that $\forall y \in \var{insert}~x~t,~h \le y$. By
  \var{insert\_perm}, this is equivalent to $\forall y \in x::t,~h \le y$ From
  the the fact that $\var{Sorted}~(h::t)$, we know that $\forall y \in t,~h \le
  y$, and we have that $h < x$, so we are done.
\end{proof}

\begin{theorem}[sort\_sorted]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~(\var{sort}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof follows by induction on $l$ and \var{insert\_sorted}.
\end{proof}

\begin{lemma}[insert\_stable]
  For all lists $l$ and elements $x$,
  \begin{equation*}
    \var{StablePerm}~(x::l)~(\var{insert}~x~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $l$. In the inductive case, we need to show
  \begin{equation*}
    \var{StablePerm}~(x::h::t)~(\var{insert}~x~(h::t))
  \end{equation*}
  given the induction hypothesis that
  \begin{equation*}
    \var{StablePerm}~(x::t)~(\var{insert}~x~t)
  \end{equation*}
  If $x \le h$, then $\var{insert}~x~(h::t) = x :: h :: t$ and we are
  done. Otherwise, $x > h$ and $\var{insert}~x~(h::t) = h ::
  \var{insert}~x~t$.
  \begin{equation*}
    x :: h :: t \eqstable h :: x :: t \eqstable h :: \var{insert}~x~t
  \end{equation*}
  where the first step follows from the fact that $x \not\equiv h$ and the
  second from the induction hypothesis.
\end{proof}

\begin{theorem}[sort\_stable]
  For all lists $l$,
  \begin{equation*}
    \var{StablePerm}~l~(\var{sort}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof is almost identical to the proof for \var{sort\_perm}.
\end{proof}

\begin{theorem}[sort\_StableSort]
  \begin{equation*}
    \var{StableSort}~\var{sort}
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows from \var{sort\_sorted} and \var{sort\_stable}.
\end{proof}

\section{\var{hdsorted\_sorted\_S}}
\label{appendix:hdsort_sorted_S}

\begin{lemma}[insert\_Sorted\_S]
  For all matrices $m$, rows $r$, and naturals $j$,
  \begin{gather*}
    \var{PrefixSorted}~(j+1)~(\var{hdsort}~m) \implies \\
    \var{PrefixSorted}~j~(\var{tl}~r :: \var{map}~{tl}~m) \implies \\
    \var{PrefixSorted}~(j+1)~(\var{insert}_1~r~(\var{hdsort}~m))
  \end{gather*}
  where $\var{insert}_1$ is \var{insert} that uses the 1-prefix
  lexicographic ordering used by \var{hdsort}.
\end{lemma}
\begin{proof}
  We can destruct $\var{insert}_1~r~(\var{hdsort}~m)$ as $m_1 \dplus r
  :: m_2$, where $m_1 \dplus m_2 = \var{hdsort}~m$. Then by
  \var{Sorted\_app\_cons}, it suffices to show
  \begin{gather*}
    \var{PrefixSorted}~(j+1)~m_1 \land \\
    \var{PrefixSorted}~(j+1)~(r :: m_2) \land \\
    (\forall x \in m_1,~ r >_{j+1} x)
  \end{gather*}
  \begin{itemize}
  \item $\var{PrefixSorted}~(j+1)~m_1$ follows from the fact that
    $\var{hdsort}~m = m_1 \dplus m_2$ and
    $\var{PrefixSorted}~(j+1)~(\var{hdsort}~m)$.
  \item By similar reasoning, we know that
    $\var{PrefixSorted}~(j+1)~m_2$, so in order to show
    $\var{PrefixSorted}~(j+1)~(r :: m_2)$ all we need to prove is that
    for every row $x$ in $m_2$,
    \begin{equation*}
      r \le_{j+1} x
    \end{equation*}
    We know from the fact that $\var{insert}_1~r~(\var{hdsort}~m) =
    m_1 \dplus r :: m_2$ that $r \le_1 x$. Then by \var{key\_firstn\_S},
    we only need to show that
    \begin{equation*}
      \var{tl}~r \le_j \var{tl}~x
    \end{equation*}
    This follows from $\var{PrefixSorted}~j~(\var{tl}~r :: \var{map}~{tl}~m)$.
  \item We have that for any $x \in m_1$, $r >_1 x$ by the fact that
    $\var{insert}_1~r~(\var{hdsort}~m) = m_1 \dplus r :: m_2$. This
    gives us $r >_{j+1} x$ by \var{key\_lt\_firstn\_ge}. \qedhere
  \end{itemize}
\end{proof}

\hdsortsortedS*
\begin{proof}
  This follows by induction from \var{insert\_Sorted\_S}
\end{proof}

\section{\var{Permutation} definitions equivalence}
\label{appendix:perm_def_eq}

We know we will eventually need to reason inductively about
permutation functions, so we would like to define the relationship
between applying a permutation function $i :: p$ and applying $p$.
Conceptually, the first element of $\var{apply}~(i::p)~l$ will be
$l_i$. The rest will be the result of applying some new permutation
$p'$, where everything greater than $i$ has been shifted down by one,
to some new list $l'$, which has its $i$th element removed. So, we
define \var{rem\_PermFun} and \var{rem\_nth} respectively, hoping to
prove that
\begin{equation*}
  \var{apply}~(i::p)~l =
  l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l)
\end{equation*}
\begin{lstlisting}
  Definition rem_PermFun i :=
  map (fun j => if lt_dec i j then pred j else j).

  Fixpoint rem_nth i l :=
  match l with
  | [] => []
  | h :: t =>
  match i with
  | 0 => t
  | S i' => h :: rem_nth i' t
  end
  end.
\end{lstlisting}

These two theorems define how \var{rem\_nth} affects indexing.
\begin{theorem}[nth\_lt\_rem\_nth, nth\_ge\_rem\_nth]
  For all lists $l$ and indices $i$ and $j$,
  \begin{equation*}
    j < i \implies (\var{rem\_nth}~i~l)_j = l_j
  \end{equation*}
  and
  \begin{equation*}
    j \ge i \implies (\var{rem\_nth}~i~l)_j = l_{j+1}
  \end{equation*}
\end{theorem}
\begin{proof}
  Both theorems follow by induction on $i$.
\end{proof}

As we expect, removing the $n$th element and sticking it on the front
preserves all elements.
\begin{theorem}[rem\_nth\_Perm]
  For all list $l$ and indices $i$
  \begin{equation*}
    \var{Permutation}~l~(l_i :: \var{rem\_nth}~i~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows from induction on $l$.
\end{proof}

\begin{theorem}[rem\_PermFun\_correct]
  For all permutation functions $p$, lists $l$, and indices $i$.
  \begin{equation*}
    \var{apply}~(i::p)~l =
    l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  Working backwards from our goal,
  \begin{align*}
    \var{apply}~(i::p)~l &=
    l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l) \\
    l_i :: \var{map}~(\lambda j.~l_j)~p &=
    l_i :: \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_j)~(\var{rem\_PermFun}~i~p) \\
    \var{map}~(\lambda j.~l_j)~p &=
    \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_j)\\
    &\phantom{=} \quad (\var{map}~(\lambda j. \bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j)~p) \\
    \var{map}~(\lambda j.~l_j)~p &=
    \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_{\bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j})~p \\
  \end{align*}
  From here, we need to prove that the mapping functions agree on all
  the elements of $p$, namely that for all $j \in p$
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_{\bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j}
  \end{equation*}
  We know that $i \neq j$, because $j \in p$ and $\var{NoDup}~(i :: p)$.
  Therefore there are two cases, $i < j$ and $i > j$. When $i < j$, we have
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_{j-1}
  \end{equation*}
  by \var{nth\_ge\_rem\_nth}, and when $i > j$, we have
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_j
  \end{equation*}
  by \var{nth\_lt\_rem\_nth}.
\end{proof}

\begin{theorem}[apply\_correct]
  For all permutation functions $p$ and lists $l$,
  \begin{equation*}
    \var{Permutation}~(\var{apply}~p~l)~l
  \end{equation*}
\end{theorem}
\begin{proof}
  First we let $n = \var{length}~l$, so that we can reason by
  induction on $n$. The base case is uninteresting. In the inductive
  case, we have the induction hypothesis that for $p :
  \var{list}~\nat$ and $l : \var{list}~A$
  \begin{equation*}
    \var{lenth}~l = n \implies \var{PermFun}~n~p \implies
    \var{Permutation}~(\var{apply}~p~l)~l
  \end{equation*}
  Given $\var{PermFun}~(n+1)~(i::p')$, we must show that
  \begin{equation*}
    \var{Permutation}~(\var{apply}~(i :: p')~(a :: l))~(a :: l)
  \end{equation*}

  We reason as follows, using the notation $\eqperm$ to
  chain together \var{Permutation}s:
  \begin{align*}
    &\var{apply}~(i :: p')~(a :: l) \\
    \eqperm{}& \text{\{ by \var{rem\_PermFun\_correct} \}} \\
    &(a :: l)_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~(a::l))
    \\
    \eqperm{}& \text{\{ by IH \}} \\
    &(a :: l)_i :: \var{rem\_nth}~i~(a::l)
    \\
    \eqperm{}& \text{\{ \var{rem\_nth\_Perm} \}} \\
    & a :: l \qedhere
  \end{align*}
\end{proof}

\begin{theorem}[PermutationEx\_iff]
  \label{thm:permutationex_iff}
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{Permutation}~l~l' \iff \var{PermutationEx}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  The $\impliedby$ direction follows from \var{apply\_correct}.

  ($\implies$) We reason by induction on the evidence of
  $\var{Permutation}~l~l'$. The empty case is simple.
  \begin{itemize}
  \item We have a permutation function $p$ from the induction
    hypothesis such that $\var{apply}~p~l = l'$, and we need to
    construct a permutation function $p'$ such that
    $\var{apply}~p'~(x::l) = x :: l'$. We let $p' = 0 ::
    \var{map}~(+1)~p$, shifting every index up by one and mapping
    the first element of the list to itself.
  \item We have to construct a permutation function $p$ such that
    $\var{apply}~p~(y :: x :: l) = x :: y :: l$. We let $p = 1 :: 0 ::
    \var{map}~(+2)~(\var{seq}~0~(\var{length}~l))$, leaving every
    element in place except for the first 2.
  \item We have permutation functions $p_1$ and $p_2$ from our two
    induction hypotheses, such that
    \begin{align*}
      \var{apply}~p_1~l = l' && \var{apply}~p_2~l' = l''
    \end{align*}
    We need to construct a $p'$ such that $\var{apply}~p'~l=l''$, so
    we let $p' = p_2 \comp p_1$. For the definition of composition of
    permutations, see \cref{appendix:perm_comp}. Then,
    \begin{equation*}
      \var{apply}~(p_2 \comp p_1)~l =
      \var{apply}~p_2~(\var{apply}~p_1~l) = l'' \qedhere
    \end{equation*}
  \end{itemize}
\end{proof}

\section{\var{StablePerm} definitions equivalence}
\label{appendix:stableperm_def_eq}

\begin{lemma}[StablePerm\_app]
  For all lists $l$, $l'$, $m$ and $m'$,
  \begin{gather*}
    \var{StablePerm}~l~l' \implies \var{StablePerm}~m~m' \implies \\
    \var{StablePerm}~(l \dplus m)~(l' \dplus m')
  \end{gather*}
\end{lemma}
\begin{proof}
  This follows from the distributivity of \var{filter} over \var{app}.
\end{proof}

\begin{lemma}[StablePerm\_skip]
  For all lists $l$ and $l'$, and elements $a$,
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{StablePerm}~(a::l)~(a::l')
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show $\var{filter}~(\equiv x)~(a::l) =
  \var{filter}~(\equiv x)~(a::l')$. If $x \equiv a$, $a$ is added to both lists,
  and if $x \not\equiv a$ it is added to neither.
\end{proof}

\begin{lemma}[StablePerm\_swap]
  For all lists $l$ and $l'$, and elements $a$ and $b$ where $a \not\equiv b$,
  \begin{equation*}
    \var{StablePerm}~(a::b::l)~(b::a::l)
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show $\var{filter}~(\equiv x)~(a::b::l) =
  \var{filter}~(\equiv x)~(b::a::l)$. The proof follows from case analysis
  on $x \equiv a$ and $x \equiv b$.
\end{proof}

\begin{lemma}[StablePerm\_cons\_app]
  For all lists $l$, $l_1$ and $l_2$, and elements $a$ such that $\forall b
  \in l_1,~a \not\equiv b$,
  \begin{equation*}
    \var{StablePerm}~l~(l_1 \dplus l_2) \implies \var{StablePerm}~(a
    :: l)~(l_1 \dplus a :: l_2)
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show
  \begin{equation*}
    \var{filter}~(\equiv x)~(a::l) = \var{filter}~(\equiv x)~(l_1 \dplus a :: l_2)
  \end{equation*}
  If $x \not\equiv a$ the proof is easy, so assume $x \equiv a$. Then we have
  \begin{align*}
    \var{filter}~(\equiv x)~(l_1 \dplus a :: l_2)
    &= \var{filter}~(\equiv x)~l_1 \dplus \var{filter}~(a :: l_2) \\
    &= [] \dplus a::\var{filter}~l_2 \\
    &= a::\var{filter}~l
  \end{align*}
  where we know that $\var{filter}~(\equiv x)~l_1 = []$ because $\forall b \in
  l_1,~a \not\equiv b$, and the last step follows from
  $\var{StablePerm}~l~(l_1 \dplus l_2)$.
\end{proof}

\begin{theorem}[StablePerm\_destr]
  For all lists $l$ and $l'$, and elements $h$ and $h'$ such that $h
  \not\equiv h'$,
  \begin{gather*}
    \var{StablePerm}~(h :: l)~(h' :: l') \implies \\
    \exists~l_1~l_2, ~l = l_1 \dplus h' :: l_2 \land (\forall x \in l_1, h' \not\equiv x)
  \end{gather*}
\end{theorem}
\begin{proof}
  Let $l_1 = \var{take\_while}~(\not\equiv h')~l$ and $l_2 =
  \var{drop\_while}~(\not\equiv h')~l$. Clearly $l = l_1 \dplus l_2$, and
  because $\var{StablePerm}~(h :: l)~(h' :: l')$, we know the first
  element of $l_2$ must be $h'$. Let $l_2'$ be the rest of $l_2$, and
  we have two lists $l_1$ and $l_2'$ that satisfy the theorem.
\end{proof}

\begin{theorem}[StablePerm\_length]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{length}~l = \var{length}~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $n = \var{length}~l$. In the inductive
  case, we need to show that $\var{length}~(h::t) =
  \var{length}~(h'::t')$ given that $\var{StablePerm}~(h::t)~(h'::t')$
  and the induction hypothesis that for all lists of length $n$,
  \var{StablePerm} implies their lengths are equal.

  If $h \equiv h'$, then from $\var{StablePerm}~(h::t)~(h'::t')$ we know
  that $h = h'$. We can then remove $h$ from the front of $h::t$ and
  $h::t'$ and apply the induction hypothesis.

  If $h \not\equiv h'$, we can destructure $t$ and $t'$ as $l_1 \dplus h'
  :: l_2$ and $l_1' \dplus h :: l_2'$ respectively, using
  \var{StablePerm\_destr}. Then,
  \begin{equation*}
    \var{length}~t
    = \var{length}~(h' :: l_1  \dplus l_2)
    = \var{length}~(h  :: l_1' \dplus l_2')
    =\var{length}~t'
  \end{equation*}
  where the first and last steps follow from the induction hypothesis,
  using the fact that $\var{StablePerm}~t~(h'::l_1 \dplus l_2)$ and
  $\var{StablePerm}~t'~(h::l_1' \dplus l_2')$.
\end{proof}

\begin{theorem}[StablePerm\_Perm]
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{Permutation}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by strong induction on the length of both lists (we know
  the lengths are equal by \var{StablePerm\_length}). In the inductive
  case, we need to show $\var{Permutation}~(h :: t)~(h' :: t')$. If $h
  \equiv h$, we know that $h = h'$, and the proof follows by
  \var{perm\_skip}.

  If $h \not\equiv$, we again destructure $t$ and $t'$ as $l_1 \dplus h' ::
  l_2$ and $l_1' \dplus h :: l_2'$ respectively, using
  \var{StablePerm\_destr}.
  \begin{align*}
    h::t ={}& h :: l_1 \dplus h' :: l_2 \\
    \eqperm{}& h :: h' :: l_1 \dplus l_2 \\
    \eqperm{}& h' :: h :: l_1 \dplus l_2 \\
    \eqperm{}& h' :: h :: l_1' \dplus l_2' \\
    \eqperm{}& h' :: l_1' \dplus h :: l_2' \\
    ={}& h'::t
  \end{align*}
  The crucial step is $l_1 \dplus l_2 \eqperm l_1' \dplus l_2'$, which
  follows from the induction hypothesis.
\end{proof}

\begin{theorem}[StablePermInd\_iff]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \iff \var{StablePermInd}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  The ($\impliedby$) direction follows easily by induction on
  $\var{StablePermInd}~l~l'$, using \var{StablePerm\_skip},
  \var{StablePerm\_swap}, and \var{StablePerm\_trans}.

  The ($\implies$) direction is proved very similarly to
  \var{StablePerm\_Perm}, using \var{StablePerm\_destr} to split up
  the lists.
\end{proof}

Now we prove equivalence with \var{StablePermEx}.

\begin{theorem}[apply\_correct\_stable]
  \label{thm:apply_correct_stable}
  For all lists $l$ and permutation functions $p$,
  \begin{equation*}
    \var{StablePermFun}~l~p \implies \var{StablePerm}~(\var{apply}~p~l)~l
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the length of $l$. In the inductive case,
  we need to show
  \begin{equation*}
    \var{StablePerm}~(\var{apply}~(i :: p)~(a :: l))~(a :: l)
  \end{equation*}

  Using $\eqstable$ to chain \var{StablePerm}s transitively,
  \begin{align*}
    {}& \var{apply}~(i :: p)~(a :: l) \\
    ={}& (a :: l)_i  ::
    \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~(a :: l)) \\
    \eqstable{}& (a :: l)_i :: \var{rem\_nth}~i~(a :: l) \\
    \eqstable{}& (a :: l)_i
  \end{align*}
  where we rely on the induction hypothesis between the first and
  second lines.
\end{proof}

\begin{theorem}[StablePermEx\_iff]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \iff \var{StablePermEx}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We prove $\var{StablePermInd}~l~l' \iff \var{StablePermEx}~l~l'$.

  The $\implies$ direction is very similar to the $\implies$ direction
  of the \var{PermutationEx\_iff} proof: we reason by induction on
  $\var{StablePermInd}~l~l'$, constructing the same permutation
  functions in each case, but this time we have to prove that it is
  stable as well. The $\impliedby$ direction follows from
  \cref{thm:apply_correct_stable}.
\end{proof}


\section{Permutation composition}
\label{appendix:perm_comp}

In this section, we define \var{compose} to compose permutations,
which we will denote using $\comp$ in proofs. We prove the following
property:

\begin{theorem}[compose\_apply]
  For all lists $l$ and permutation functions $p_1$ and $p_2$
  \begin{equation*}
    \var{apply}~(p_2 \comp p_1)~l = \var{apply}~p_2~(\var{apply}~p_1~l)
  \end{equation*}
\end{theorem}

Because we represent the permutations as lists of outputs, we can
simply apply one permutation to the other permutation to compose them.
\begin{lstlisting}
  Definition compose p2 p1 := apply p2 p1.
\end{lstlisting}
This definition, while simple, does not make it obvious that
\var{compose\_apply} holds.
\begin{equation}
  \label{eq:compose_apply}
  \var{apply}~(\var{apply}~p_2~p_1)~l = \var{apply}~p_2~(\var{apply}~p_1~l)
\end{equation}

Conceptually, the theorem works because when we permute the list $l$,
we can think of it as permuting the original indices $\iota =
[0,~1,~\ldots,~n-1]$. If we rephrase \cref{eq:compose_apply} the effect on
on $I$, we get
\begin{equation*}
  \var{apply}~(\var{apply}~p_2~p_1)~\iota = \var{apply}~p_2~(\var{apply}~p_1~\iota)
\end{equation*}
This equation clearly holds if we can prove that $\iota$ is the identity
of \var{apply}, giving as an outline of a proof.

We will use the function \var{combine} defined in the Coq standard
library to annotate every element of $l$ with its index.
\begin{lstlisting}
  Fixpoint combine (l : list A) (l' : list B)
  : list (A*B) :=
  match l,l' with
  | x :: tl, y :: tl' => (x,y) :: (combine tl tl')
  | _, _ => nil
  end.
\end{lstlisting}

To formalize the idea that we can reason about the indices of the
original list $l$ in place of $l$, we need the following theorem.
Intuitively, it says that if we have two lists of pairs, if we know
that the $x$-coordinates are equal and we know that the list of pairs
are permutations of one another, then we know the $y$-coordinates must
be equal as well, because the permutation condition guarantees that
each $x$-coordinate is paired with the same $y$-coordinate in both
lists.
\begin{theorem}[Permutation\_combine\_eq]
  \label{thm:Permutation_combine_eq}
  For all lists $xs$, $ys_1$, and $ys_2$ with all the same length,
  if $xs$ has no duplicates then
  \begin{equation*}
    \var{Permutation}~(\var{combine}~xs~ys_1)~(\var{combine}~xs~ys_2) \implies ys_1 = ys_2
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the list $\var{combine}~xs~ys_1$. In the
  inductive case, we have to show that
  \begin{equation*}
    y_1 :: ys_1 = y_2 :: ys_2
  \end{equation*}
  given that
  \begin{equation*}
    (x,~y_1) :: \var{combine}~xs~ys_1 \eqperm (x, y_2) :: \var{combine}~xs~ys_2
  \end{equation*}
  This means that $(x, y_1) \in (x, y_2) :: \var{combine}~xs~ys_2$, so
  either $(x, y_1) = (x, y_2)$, in which case we are done, or $(x,
  y_1) \in \var{combine}~xs~ys_2$. In the latter case, we have a
  contradiction, because we know that $x :: xs$ contains no
  duplicates, so $x$ cannot be in $xs$.
\end{proof}

We also need to show that \var{apply} distributes over \var{combine}:
\begin{theorem}[apply\_combine]
  For all lists $xs$ and $ys$ with equal length,
  \begin{equation*}
    \var{apply}~p~(\var{combine}~xs~ys) =
    \var{combine}~(\var{apply}~p~xs)~(\var{apply}~p~ys)
  \end{equation*}
\end{theorem}
\begin{proof}
  Let $n = \var{length}~xs = \var{length}~ys$. We reason by induction
  on n. In the inductive case, we have to show
  \begin{align*}
    & \var{apply}~(i::p)~(\var{combine}~(x::xs)~(y::ys)) \\
    ={}& \var{combine}~(\var{apply}~(i::p)~(x::xs))~(\var{apply}~(i::p)~(y::ys))
  \end{align*}
  We can use \var{rem\_PermFun\_correct} to simplify both sides of the
  equation. We have that the heads of both lists must are equal:
  \begin{equation*}
    (\var{combine}~(x::xs)~(y::ys))_i = ((x::xs)_i, (y::ys)_i)
  \end{equation*}
  That the tails are equal follows from the induction hypothesis, so
  we are done.
\end{proof}

Now we show that $\iota$ is the identity of $\var{compose}$/$\var{apply}$.
\begin{theorem}[compose\_id\_l]
  For all permutation functions $p$ on lists of length $n$,
  \begin{equation*}
    \var{compose}~(\var{seq}~0~n)~p = p
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof follows from induction on $p$. In the inductive case,
  \begin{align*}
    {}& \var{compose}~(\var{seq}~0~(n+1))~(i::p) \\
    ={}& \var{map}~(\lambda j.~(i::p)_j)~(\var{seq}~0~(n+1)) \\
    ={}& \var{map}~(\lambda j.~(i::p)_j)~(0 :: \var{seq}~1~(n+1)) \\
    ={}& i :: \var{map}~(\lambda j.~(i::p)_j)~(\var{seq}~1~(n+1)) \\
    ={}& i :: \var{map}~(\lambda j.~(i::p)_j)~(\var{map}~(\lambda j.~j+1)~(\var{seq}~0~n)) \\
    ={}& i :: \var{map}~(\lambda j.~(i::p)_{j+1})~(\var{seq}~0~n) \\
    ={}& i :: \var{map}~(\lambda j.~p_j)~(\var{seq}~0~n) \\
    ={}& i :: \var{compose}~(\var{seq}~0~n)~p \\
    ={}& i :: p \qedhere
  \end{align*}
\end{proof}

\begin{theorem}[compose\_id\_r]
  For all permutation functions $p$ on lists of length $n$,
  \begin{equation*}
    \var{compose}~p~(\var{seq}~0~n) = p
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{equation*}
    \var{compose}~p~(\var{seq}~0~n)
    = \var{map}~(\lambda j.~(\var{seq}~0~n)_j)~p
    = \var{map}~(\lambda j.~j)~p
    = p \qedhere
  \end{equation*}
\end{proof}

Now we can prove the main theorem:
\begin{theorem}[compose\_apply]
  For all lists $l$ and permutation functions $p_1$ and $p_2$
  \begin{equation*}
    \var{apply}~(p_2 \comp p_1)~l = \var{apply}~p_2~(\var{apply}~p_1~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  Letting $\iota = \var{seq}~0~(\var{length}~l)$, we annotate $l$ with
  indices. By \var{apply\_correct}, we have
  \begin{equation*}
    \var{combine}~\iota~l \eqperm \var{apply}~(p_2 \comp p_1)~(\var{combine}~\iota~l)
  \end{equation*}
  and
  \begin{gather*}
    \var{combine}~\iota~l
    \eqperm
    \var{apply}~p_1~(\var{combine}~\iota~l) \\
    \eqperm
    \var{apply}~p_2~(\var{apply}~p_1~(\var{combine}~\iota~l))
  \end{gather*}
  We combine these by transitivity, and then simplify. For space, we
  denote \var{apply} by only juxtaposition, and \var{combine} by $\oplus$.
  \begin{align*}
    (p_2 \comp p_1)~(\iota \oplus l)
    &\eqperm
    p_2~(p_1~(\iota \oplus l)) \\
    ((p_2 \comp p_1)~\iota) \oplus ((p_2 \comp p_1)~l)
    &\eqperm
    p_2~((p_1~\iota) \oplus (p_1~l)) \\
    (p_2 \comp p_1) \oplus ((p_2 \comp p_1)~l)
    &\eqperm
    p_2~(p_1 \oplus (p_1~l)) \\
    (p_2 \comp p_1) \oplus ((p_2 \comp p_1)~l)
    &\eqperm
    (p_2~p_1)\oplus(p_2~(p_1~l))
  \end{align*}
  Because $p_2 \comp p_1 = p_2~p_1$, we can apply
  \cref{thm:Permutation_combine_eq} to show $(p_2 \comp p_1)~l \eqperm
  p_2~(p_1~l)$
\end{proof}

\end{document}
