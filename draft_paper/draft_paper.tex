\documentclass[11pt]{thesis}

\title{Towards a Verified Bzip2: A Functional Specification for Burrows-Wheeler}
\author{Jake Waksbaum}
\date{May 6, 2019}

\begin{document}

\maketitle

\section{Introduction}
\label{sec:intro}

The Burrows-Wheeler transform (BWT) is an invertible transformation
that makes a string more amenable to compression by other methods
\cite{bw}. The forwards transform is straightforward to describe: for
a string of length \(n\), form the \(n \times n\) rotaion matrix of all
cyclic shifts of the original string, sort it lexicographically, and
take the last column of the matrix. In addition, we note the index
where the original string appears in the sorted rotation matrix. As
shown in \Cref{fig:bw_ex}, applying the algorithm to the string
\var{abracadabra!} produces the new string \var{ard!rcaaabb} and
the index \var{3}. The transformation tends to produce runs like
the three \var{a}s or the two \var{b}s in \var{ard!rcaaabb},
and it is this feature that makes the resulting string easier to
compress. Although it is not obvious, the transformed string, together
with the index, provide enough information to recover the the original
string. The BWT is discussed more in \cref{sec:bwt}.

\begin{figure}
  \centering
  \begin{tt}
  \begin{tabular}{rc}
    0  & abracadabra! \\
    1  & bracadabra!a \\
    2  & racadabra!ab \\
    3  & acadabra!abr \\
    4  & cadabra!abra \\
    5  & adabra!abrac \\
    6  & dabra!abraca \\
    7  & abra!abracad \\
    8  & bra!abracada \\
    9  & ra!abracadab \\
    10 & a!abracadabr \\
    11 & !abracadabra
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{rc}
    0  & !abracadabr\textbf{a} \\
    1  & a!abracadab\textbf{r} \\
    2  & abra!abraca\textbf{d} \\
    \textit{3} & \textit{abracadabra\textbf{!}} \\
    4  & acadabra!ab\textbf{r} \\
    5  & adabra!abra\textbf{c} \\
    6  & bra!abracad\textbf{a} \\
    7  & bracadabra!\textbf{a} \\
    8  & cadabra!abr\textbf{a} \\
    9  & dabra!abrac\textbf{a} \\
    10 & ra!abracada\textbf{b} \\
    11 & racadabra!a\textbf{b}
  \end{tabular}
  \end{tt}
  \caption{BWT acting on \var{abracadabra!}}
  \label{fig:bw_ex}
\end{figure}

Bird and Mu \cite{birdmu,pearls} analyze the Burrows-Wheeler algorithm
in a functional setting. They express the algorithm as the composition
of many smaller functions, and then derive an expression for the
inverse transform from the specification that it inverts the forwards
transform. In addition to providing an intuitive derivation of the
inverse transform, this provides the outline of a proof of the
correctness of the Burrows-Wheeler algorithm. We give our version of
their derivation in \cref{sec:forwards_BWT,sec:inverse_BWT}.

Our main contribution is a formalization in the Coq proof assistant of
the BWT, together with a machine-checked proof that our implementation
is correct. Specifically, we implement the functions $\var{bwp} :
\var{list}~A \to \var{list}~A$ and $\var{bwn} : \var{list}~A
\to \mathbb{N}$ that produce the string and index part of the BWT
respectively, and the function $\var{unbwt} : \mathbb{N} \to
\var{list}~A \to \var{list}~A$ that reverses the transform. We
prove the following theorem:
\begin{theorem}[unbwt\_correct]
  \label{thm:unbwt_correct}
  For all lists $l$,
  \begin{equation*}
    \var{unbwt}~(\var{bwn}~l)~(\var{bwp}~l) = l
  \end{equation*}
\end{theorem}
The implementation of the functions and the outline of the proof are
based on Bird et~al.'s derivations.

The implementation is useful in its own right as a functional
specification for a potential verified implementation of a program
that relies on Burrows-Wheeler \cite{appel-func-spec}. But more
generally, this provides evidence that Bird's approach of program
calculation, in which programs are derived from their specifications
via algebraic manipulations, is useful for proving the correctness of
algorithms in a machine-checked setting. Bird has applied this
approach to greedy algorithms, the Boyer-Moore algorithm, the
Knuth-Morris-Pratt algoirthm, Sudoku solvers, arithmetic coding, and
the Schorr-Waite algorithm, the Johnson-Trotter algorithm, and many
more problems \cite{pearls}.

As a part of the proof of \Cref{thm:unbwt_correct}, we implemented
radix-sort and proved its correctness,
\begin{theorem}[radixsort\_correct]
  \label{thm:radixsort_correct}%
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~(\var{radixsort}~l) \land \var{Permutation}~(\var{radixsort}~l)~l
  \end{equation*}
\end{theorem}
and proved that any two stable sorts are interchangable,
\begin{theorem}[stable\_sort\_unique]
  \label{thm:radixsort_correct}%
  For all functions $f, g : \var{list}~A \to \var{list}~A$,
  \begin{gather*}
    (\forall l,~\var{Sorted}~(f~l) \land \var{StablePermutation}~(f~l)~l) \implies \\
    (\forall l,~\var{Sorted}~(g~l) \land \var{StablePermutation}~(g~l)~l) \implies \\
    \forall l,~f~l = g~l
  \end{gather*}
\end{theorem}
This in turn required developing suitable definitions for
\var{Sorted}, \var{Permutation}, and \var{StablePermutation}.
We explored multiple definitions for each of these properties, and
proved equivalence between them.

The rest of this paper is organize as follows. \Cref{sec:bwt} details
the history of the BWT and its applications. \Cref{sec:ord} defines
what we need to know about a type $A$ to sort a $\var{list}~A$ as a
part of BWT. Following Bird et~al., \cref{sec:forwards_BWT} defines
the implementation of the forwards BWT in Coq, and
\cref{sec:inverse_BWT} derives an implementation of the inverse BWT
via program calculation. \Cref{sec:sorted_perm_stable} provides
multiple definitions of \var{Sorted}, \var{Permutation}, and
\var{StablePermutation}, and proves equivalence between them.
\Cref{sec:sort} implements insertion sort and radixsort, and proves
their correctness. \Cref{appendix:BWT_proof} more formally proves the
theorems about the BWT mentioned in previous sections, including
\Cref{thm:unbwt_correct}.

The full source code can be found at
\url{https://github.com/jbaum98/verifed_bzip2/tree/2019-05-06}.

\section{Burrows-Wheeler}
\label{sec:bwt}

The Burrows-Wheeler transform was originally introduced as a pass in a
compression algorithm. First the input is transformed with BWT, which
tends to group together characters. The intuition for why this happens
is that all adjacent characters in the output string are followed in
the input string by characters that are adjacent in the sorted first
column. If there is any correlation between a character and the
character preceding it, bringing together the characters in the first
column will tend to bring together similar characters in the last
column.

The next pass is a Move-to-Front (MTF) encoding, in which a character
is represented by the number of different characters encountered since
the last occurence of this character. This creates many runs of zeroes
from the runs in the output of BWT. On English texts, often 50-60\% of
the values produced by the MTF pass are zeroes
\cite{fenwick2007,bw-analysis}. Often a Run-Length Encoding (RLE) pass
is then used to encode only the runs of zeroes with their lengths,
using a bijective base-2 encoding \cite{bw-analysis, tsai_2016}.
Finally, some sort of zeroth-order coder like a Huffman coder is used.

BWT is also at the heart of algorithms that exploit the relationship
between the rotation matrix in BWT and the suffix array data structure
in order to search for patterns in a compressed text
\cite{ferragina_index}.

\section{Orders and Preorders}
\label{sec:ord}

Although it is commonly demonstrated on strings, the BWT can be
applied to any list whose elements are comparable. To make that
precise, if we have a type $A$ and a relation on that type $\le$, we say
that this forms a preorder on $A$ when $\le$ is reflexive and transitive, that is
$x \le x$ for all $xA$, and
\begin{equation*}
  x \le y \implies y \le z \implies x \le z
\end{equation*}
for all $x,y,z$.

We say that we have a \textit{total} preorder if for any two elements
$x$ and $y$, $x \le y \lor y \le x$.

We say that we have a \textit{decidable} preorder if we have a
decision procedure for determining whether $x \le y$ or $x \not\le y$.

We represent a total, decidable preorder on type with the
\var{Preord} typeclass in Coq:
\lstinputlisting[firstline=9, lastline=14]{../theories/BWT/Sorting/Ord.v}
We do not explicitly require reflexivity because it follows from
totality. Given a \var{Preord} instance for a type $A$, we will be
able to sort $\var{list}~A$ as shown in \cref{sec:sort}.

From $\le$, we can define $x \ge y \coloneqq y \le x$, $x < y \coloneqq
\lnot (y \le x)$, and $x > y \coloneqq \lnot (x \le y)$, and prove
transitivity and reflexivity where appropriate.

We can also define $x \equiv y \coloneqq x \le y \land y \le x$. $\equiv$ is a
decidable equivalence relation, which means it is reflexive,
transitive, and symmetric, and we have a decision procedure for it.
\lstinputlisting[
  firstline=116, lastline=120, widthgobble=2
]{../theories/BWT/Sorting/Ord.v}

The builtin class \var{EqDec} is used to describe types with
decidable equivalence relations, so for any \var{Preord} instance
we have the \var{EqDec} instance \var{Preord\_EqDec} using
\var{eqv\_dec}. In Coq, the notation \var{===} is used for the
equivalence relation of an \var{EqDec} instance and \var{==} for
its decision procedure.

We say that we have a total, decidable \textit{order} when we have
that for all $x$ and $y$,
\begin{equation*}
  x \equiv y \implies x = y
\end{equation*}
that is, the induced equivalence relation of the preorder ($\equiv$)
implies Leibniz equality ($=$). We use the \var{Ord} class in Coq
for this:
\lstinputlisting[firstline=175, lastline=176, widthgobble=2]{../theories/BWT/Sorting/Ord.v}
For the BWT, we require an \var{Ord} instance for $A$, but
elsewhere, such as in sorting, \var{Preord} will suffice.

\section{The Forwards BWT}
\label{sec:forwards_BWT}

First, we define the left and right rotations of a list. \var{lrot}
moves the first element of a list to the end.
\lstinputlisting[widthgobble=2, firstline=24, lastline=28]{../theories/BWT/Rotation/Rotation.v}
\var{rrot} moves the last element of a list to the front, and
relies on \var{init}, which drops the last element of a list.
\lstinputlisting[widthgobble=2, firstline=18, lastline=22]{../theories/BWT/Lib/List.v}
\lstinputlisting[widthgobble=2, firstline=17, lastline=21]{../theories/BWT/Rotation/Rotation.v}
Note that in order to ensure that $\var{last}$ is total, it takes
an extra argument to return in the case of an empty list.

Now we would likel to produce the rotation matrix by repeatedly
applying \var{lrot}. We will use \var{iter}, which takes a function
$f : A \to A$, a number $n$, and an initial value $z$, and returns the
list of intermediate values obtained by applying $f$ to $z$ $n$ times.
Or,
\begin{equation}
  \var{iter}~f~n~z = [z,~f~z,~f~(f~z),~\ldots,~f^n z]
  \label{eq:iter}
\end{equation}
\lstinputlisting[
  widthgobble=2,
  firstline=14,
  lastline=18
]{../theories/BWT/Lib/Iterate.v}

We can prove that this definition satisfies \Cref{eq:iter} by defining
the function \var{rep} for repeated function application.
\lstinputlisting[
  widthgobble=2,
  firstline=8,
  lastline=12
]{../theories/BWT/Lib/Repeat.v}
In proofs, we will still use the notation $f^i z$ for $\var{rep}~f~i~z$.

\begin{theorem}[rep\_r]
  For all $f$, $n$, and $z$,
  \begin{equation}
     f^i~(f~z) = f^{i+1}~z
  \end{equation}
\end{theorem}
\begin{proof}
  The theorem follows by induction on $n$.
\end{proof}

\begin{theorem}[iter\_length]
  For all $f$, $n$, and $z$,
  \begin{equation}
    \var{length}~(\var{iter}~f~n~z) = n
  \end{equation}
\end{theorem}
\begin{proof}
  The theorem follows by induction on $n$.
\end{proof}

\begin{theorem}[iter\_nth]
  For all $f$, $n$, $z$, and $i < n$,
  \begin{equation}
    (\var{iter}~f~n~z)_i = f^i~z
  \end{equation}
\end{theorem}
\begin{proof}
  The theorem follows by induction on $i$. The inductive case is
  \begin{align*}
       (\var{iter}~f~(n+1)~z)_{i+1}
    &= (z :: \var{iter}~f~n~(f~z))_{i+1} &&\{\text{\var{iter} def.}\} \\
    &= (\var{iter}~f~n~(f~z))_i          &&\{\text{by \var{nth} def.}\} \\
    &= f^i~(f~z)                            &&\{\text{by \var{rep\_r}}\} \\
    &= f^{i+1}~z && \qedhere
  \end{align*}
\end{proof}

Now we can define \var{rots} to produce the rotation matrix.
\lstinputlisting[
  widthgobble=2,
  firstline=17,
  lastline=18,
]{../theories/BWT/Rotation/Rots.v}

\var{bwp} produces the transformed string by taking the last column
of the sorted rotation matrix. \var{lexsort} sorts the rotation
matrix lexicographically using the \var{Ord} instance; we will
discuss it's implementation in \cref{sec:sort}.
\lstinputlisting[
  widthgobble=2,
  firstline=31,
  lastline=35
]{../theories/BWT/BurrowsWheeler.v}

\var{bwn} returns the index in the sorted rotation matrix where the
original string appears. It relies on \var{findIndex}, which is
defined with respect to some decidable equivalence relation.
\lstinputlisting[
  widthgobble=2,
  firstline=12,
  lastline=19,
]{../theories/BWT/Lib/FindIndex.v}

When we use \var{findIndex} in \var{bwn}, we are using the
induced equivalence relation from the \var{Ord} instance, so it
finds the row that is Leibniz equal to $l$.
\lstinputlisting[
  widthgobble=2,
  firstline=73,
  lastline=74,
]{../theories/BWT/BurrowsWheeler.v}

Given these definitions, we can quickly prove that \var{bwn}
satisfies its specification, namely that for all non-empty $l$
\begin{equation*}
    (\var{lexsort}~(\var{rots}~l))_{(\var{bwn}~l)} = l
\end{equation*}

\begin{lemma}[findIndex\_correct]
  For all elements $x$ and lists $l$,
  \begin{equation*}
    (\exists y \in l,~x \equiv y) \implies l_{(\var{findIndex}~x~xs)} \equiv x
  \end{equation*}
  \begin{proof}
    This follows by induction on $l$: At every step, either the
    current element is equivalent to $x$ and we return $0$, or there
    must be such a $y$ in the rest of the list.
  \end{proof}
\end{lemma}

\begin{lemma}[orig\_in\_sorted\_rots]
  For every non-empty list $l$,
  \begin{equation*}
    \exists y \in \var{lexsort}~(\var{rots}~l),~x = y
  \end{equation*}
  \begin{proof}
    Clearly $l$ is in $\var{rots}~l$ at index 0, and because
    \var{lexsort} produces a permutation of its input, $l$ must be
    in $\var{lexsort}~(\var{rots}~l)$ as well.
  \end{proof}
\end{lemma}

\begin{theorem}[bwn\_correct]
  For every non-empty list $xs$,
  \begin{equation*}
    (\var{lexsort}~(\var{rots}~xs))_{(\var{bwn}~xs)} = xs
  \end{equation*}
  \begin{proof}
    This follows from \var{findIndex\_correct} if we can show that
    $\exists y \in xs,~x = y$, which we have by \var{orig\_in\_sorted\_rots}.
  \end{proof}
\end{theorem}

\section{The Inverse BWT}
\label{sec:inverse_BWT}

Now we want to construct a function \var{unbwt} such that for all $l$,
\begin{equation}
  \var{unbwt}~(\var{bwn}~l)~(\var{bwp}~l) = l
  \label{eq:unbwt}
\end{equation}
If we could recreate the entire sorted rotation matrix from
$\var{bwp}~l$, it would be simple to recover $l$ by indexing into
the matrix at $\var{bwn}~l$.

More precisely, suppose we had a function function \var{recreate}
such that
\begin{equation}
  \var{recreate}~(\var{bwp}~l) = \var{lexsort}~(\var{rots}~l)
  \label{eq:recreate}
\end{equation}
Then, we could define \var{unbwt} as
\begin{lstlisting}
Definition unbwt (i : nat) (l : list A) : list A :=
  nth i (recreate l) l.
\end{lstlisting}
and \Cref{eq:unbwt} would follow directly from the \var{bwn\_correct}:
\begin{align}
     \label{eq:unbwt_correct_start}
     \var{unbwt}~(\var{bwn}~l)~(\var{bwp}~l)
  &= (\var{recreate}~(\var{bwp}~l))_{(\var{bwn})~l} &&\\
  &= (\var{lexsort}~(\var{rots}~l))_{(\var{bwn})~l}
     &&\text{\{by \Cref{eq:recreate}\}}\\
  &= l &&\text{\{\var{recreate\_correct}\}}
     \label{eq:unbwt_correct_end}
\end{align}

How, then, do we implement \var{recreate} such that
\Cref{eq:recreate} holds? One idea is to recreate the sorted rotation
matrix column by column. We define $\var{cols}~j$ to take the first
$j$ columns of a matrix:
\lstinputlisting[
  widthgobble=2,
  firstline=19,
  lastline=20,
]{../theories/BWT/Columns.v}
Then we introduce a parameter $j$ to \var{recreate}, and our new
specification is that $\var{recreate}~j$ recreates the first $j$
columns of the sorted rotation matrix:
\begin{theorem}[recreate\_correct\_ind]
  For all lists $l$ and $j \le \var{length}~l$,
  \begin{equation}
    \var{recreate}~j~(\var{bwp}~l) =
    \var{cols}~j~(\var{lexsort}~(\var{rots}~l))
    \label{eq:recreate_correct_ind}
  \end{equation}
\end{theorem}

Now we can redefine \var{unbwt} as
\lstinputlisting[
  widthgobble=2,
  firstline=196,
  lastline=197,
]{../theories/BWT/BurrowsWheeler.v}

Assuming that we can prove \var{recreate\_correct\_ind}, the
correctness of of \text{unbwt} follows:

\begin{theorem}[cols\_id]
  For all $j$ and matrices $m : \var{list}~(\var{list}~A)$,
  \begin{gather*}
    (\forall r \in m,~\var{length}~r \le n) \implies \\
    \var{cols}~n~m = m
  \end{gather*}
\end{theorem}
\begin{proof}
  $\var{cols}~j~m = \var{map}~(\var{firstn}~j)~m$, and we
  know that $m = \var{map}~(\lambda x.x)~m$. So, it suffices to show that
  for every $r \in m$, the mapping functions agree, that is
  $\var{firstn}~j~r = r$. This follows from \var{firstn\_all2}
  in the Coq standard library.
\end{proof}

\begin{theorem}[iter\_preserves]
  We are given $f$, $n$, $z$ and a predicate $P : A \to \mathbb{P}$.
  Suppose $f$ preserves $P$, which is to say that
  \begin{equation*}
    \forall x,~P~x \implies P~(f~x)
  \end{equation*}
  then,
  \begin{gather*}
     P~z \implies \forall x \in \var{iter}~f~n~z,~P~x
  \end{gather*}
\end{theorem}
\begin{proof}
  This follows from induction on $n$.
\end{proof}

\begin{lemma}[sort\_rots\_all\_len]
  For all lists $l$,
  \begin{equation*}
    \forall x \in \var{lexsort}~(\var{rots}~l),~\var{length}~x = \var{length}~l
  \end{equation*}
\end{lemma}
\begin{proof}
  Because \var{lexsort} permutes its argument, it suffices to show
  that all the rows of $\var{rots}~l$ have the same length as $l$.
  This follows from \var{iter\_preserves} and the fact that
  \var{lrot} preserves length.
\end{proof}

\begin{corollary}[recreate\_correct]
  For all lists $l$,
  \begin{equation*}
    \var{recreate}~(\var{length}~l)~(\var{bwp}~l) =
    \var{lexsort}~(\var{rots}~l)
  \end{equation*}
\end{corollary}
\begin{proof}
  This follows from \var{recreate\_correct\_ind},
  \var{cols\_id}, and \var{sort\_rots\_all\_len}.
\end{proof}

\begin{theorem}[unbwt\_correct]
  For all lists $l$,
  \begin{equation*}
    \var{unbwt}~(\var{bwn}~l)~(\var{bwp}~l) = l
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof is almost identical to that given in
  \Crefrange{eq:unbwt_correct_start}{eq:unbwt_correct_end}.
\end{proof}

To implement \var{recreate}, we need expressions for
$\var{recreate}~0$ and $\var{recreate}~(j+1)$ that satisfy
\Cref{eq:recreate_correct_ind} in \var{recrate\_correct\_ind}:
\begin{equation}
  \var{recreate}~j~(\var{bwp}~l) =
  \var{cols}~j~(\var{lexsort}~(\var{rots}~l))
  \tag{\ref{eq:recreate_correct_ind}}
\end{equation}

To derive an implementation, we can substitute for $j$ on the right-hand
side of \Cref{eq:recreate_correct_ind} and manipulate that expression
algebraically until it is in the form \var{\_ (bwp l)}. Then we can
define $\var{recreate}~0$ and $\var{recreate}~(j+1)$ to be
precisely those \var{\_}s, and by running our derivation in
reverse, we obtain a proof.

We start with the base case, $\var{recreate}~0$:
\begin{align*}
     \var{cols}~0~(\var{lexsort}~(\var{rots}~l))
  &= \var{map}~(\lambda x.[])~(\var{lexsort}~(\var{rots}~l))) \\
  &= \var{repeat}~[]~(\var{length}~(\var{lexsort}~(\var{rots}~l))) \\
  &= \var{repeat}~[]~(\var{length}~(\var{bwp}~l)) \\
     \var{cols}~0~(\var{lexsort}~(\var{rots}~l))
  &= \var{map}~(\lambda x.[])~(\var{bwp}~l)
\end{align*}

From this, we can define $\var{recreate}~0~l = \var{map}~(\lambda
x.[])~l$ and we already have the proof for the base case of the
correctness specification.

For the inductive case, we have the expression
\begin{equation*}
  \var{cols}~(j+1)~(\var{lexsort}~(\var{rots}~l))
\end{equation*}
on the right-hand side. Our plan is to express $\var{cols}~(j+1)$
in terms of $\var{cols}~j$, and then to use the induction
hypothesis
\begin{equation*}
  \var{cols}~j~(\var{lexsort}~(\var{rots}~l)) = \var{recreate}~j
\end{equation*}
to produce a well-formed recursive definition.

One way to do this is to note that taking the first $j+1$ columns of a
matrix is the same as moving the first column to the back, taking the
first $j$ columns, and then sticking the last column back on the
front:
\begin{equation*}
  \var{cols}~(j+1)~l =
  \var{prepend\_col}~(\var{map}~\var{hd}~l)~(\var{cols}~j~(\var{map}~\var{lrot}~l))
\end{equation*}
If we substitute $\var{map}~\var{rrot}~l$ for $l$ we find a simpler expression
\begin{equation}
  \label{eq:cols_rrot}
  \var{cols}~(j+1)~(\var{map}~\var{rrot}~l) =
  \var{prepend\_col}~(\var{map}~\var{last}~l)~(\var{cols}~j~l)
\end{equation}

Now we would like to find a way of introducing a $\var{map}~\var{rrot}$
before the $\var{lexsort}~(\var{rots}~l)$. The insight here is that we can
assume that \var{lexsort} is actually a radix-sort. That is, that
\begin{equation*}
\var{lexsort}~l = (\var{hdsort} \comp \var{map}~\var{rrot})^{\var{length}~l}~l
\end{equation*}
where \var{hdsort} sorts the matrix stably on the the first column.

We can do this because there is only one way to sort the matrix, so
assuming \var{lexsort} and the radix-sort are correct, they must be
doing the same thing. We will prove this in
\cref{sec:unique,sec:sort}.

Using that we can show
\begin{align*}
     \var{lexsort}~(\var{rots}~x)
  &= \var{lexsort}~(\var{rrot}~(\var{rots}~x))
     && \text{because rrot permutes its elements} \\
  &= \var{lexsort}~(\var{hdsort}~(\var{rrot}~(\var{rots}~x)))
     && \text{because hdsort permutes its elements} \\
  &= (\var{hdsort} \comp \var{rrot})^n~(\var{hdsort}~(\var{rrot}~(\var{rots}~x)))
     && \text{radix-sort} \\
  &= (\var{hdsort} \comp \var{rrot})^{n+1}~(\var{rots}~x)
  && \text{rep\_absorb\_r} \\
  &= \var{hdsort}~(\var{rrot}~((\var{hdsort} \comp \var{rrot})^n~(\var{rots}~x)))
     && \text{ rep\_split\_l } \\
  &= \var{hdsort}~(\var{rrot}~(\var{lexsort}~(\var{rots}~x)))
     && \text{ radix-sort }
\end{align*}

In seeking to introduce an extra \var{map rrot} term, we can note that
rotating and sorting an extra time doesn't change anything when we are
sorting the rotation matrix. This works because rotating the last
column to the front produces a matrix that is sorted on its last $n -
1$ characters, so sorting stably on the first column will re-sort the
matrix. Crucially though, rotating the last column forwards also
permutes the entire matrix, so in addition to having a sorted list, we
have a sorted list of the same elements.

More precisely, we can characterize how $\var{map}~\var{rrot}$
permutes \var{rots}.
\begin{equation*}
  \var{map}~\var{rrot}~(\var{rots}~x) = \var{rrot}~(\var{rots}~x)
\end{equation*}

From here we can calculate
\begin{align*}
  &\var{cols}~(j+1)~(\var{lexsort}~(\var{rots}~l)) \\
&= \var{cols}~(j+1)~(\var{hdsort}~(\var{map}~\var{rrot}~(\var{lexsort}~(\var{rots}~l)))) \\
&= \var{hdsort}~(\var{cols}~(j+1)~(\var{map}~\var{rrot}~(\var{lexsort}~(\var{rots}~l))))
&&\text{ \var{hdsort} commutes with $\var{cols}~(j+1)$ } \\
&= \var{hdsort}~(\var{prepend\_col}~(\var{map}~\var{last}~(\var{lexsort}~(\var{rots}~l)))~(\var{cols}~j~(\var{lexsort}~(\var{rots}~l)))
  &&\text{ reduce $\var{cols}~(j+1)$ } \\
&= \var{hdsort}~(\var{prepend\_col}~(\var{bwp}~l)~(\var{recreate}~j~(\var{bwp}~l)))
  &&\text{ by IH and def of bwp } \\
&= (\lambda x.~\var{hdsort}~(\var{prepend\_col}~x~(\var{recreate}~j~x)))~\var{bwp}
\end{align*}

We can use this to define \var{recreate}:
\lstinputlisting[
  widthgobble=2, firstline=156, lastline=160,
]{../theories/BWT/BurrowsWheeler.v}

\section{\var{Sorted}, \var{Permutation}, and \var{StablePermutation}}
\label{sec:sorted_perm_stable}

The previous section's derivation relied on the correctness and stability of
various sorts. To make those parts of the derivation precise, we
define here what it means for a sorting function to be correct, and
what it means for a sort to be stable.

Intuitively, a sorting function must produce a sorted list that
contains all of the elements that its input contained. So, given a function $f : \var{list}~A \to \var{list}~A$, we say that $\var{Sort}~f$ precisely when for all $l$,
\begin{equation*}
  \var{Sorted}~(f~l) \land \var{Permutation}~(f~l)~l
\end{equation*}
Now we need to define \var{Sorted} and \var{Permutation} to capture
the idea that a list is in sorted order, and that two lists contain
all of the same elements.

\subsection{\var{Sorted}}
\label{subsec:sorted}

For a sorting function to be correct, its output must be sorted with
respect to the ordering defined by the \var{Preord} instance. There a
a few ways we might define what it means for a list to be sorted. In
mathematics, we might say that a list $l$ is sorted if for all indices
$i$ and $j$,
\begin{equation*}
  i \le j \implies l_i \le l_j
\end{equation*}
We will call this property \var{SortedIx}.

However, it is often easier to reason about inductively defined
properties, so we would like to define \var{Sorted} inductively. So,
we can say that as a base case, the empty list is sorted, and given a
sorted list, we can add an element that is smaller than all the
existing elements and the result will also be sorted:
\begin{align*}
  \infer[\var{Sorted\_nil}]{\var{Sorted}~[]}{}
  &&
  \infer[\var{Sorted\_cons}]{\var{Sorted}~(a :: l)}{%
    (\forall~x \in l,~a < x) & \var{Sorted}~l
  }
\end{align*}

By transitivity it suffices to check that the new element is
smaller than just the first element of the sorted list. That leads to a third definition of sorted:
\begin{gather*}
  \begin{align*}
    \infer[\var{SortedLocal\_nil}]{\var{SortedLocal}~[]}{}
    &&
    \infer[\var{SortedLocal\_1}]{\var{SortedLocal}~[a]}{}
  \end{align*}
  \\
  \\
  \infer[\var{SortedLocal\_cons}]{\var{SortedLocal}~(a :: b :: l)}{%
    a < b & \var{SortedLocal}~(b :: l)
  }
\end{gather*}

Although these two inductive definitions are intuitive, they are not
obviously equivalent to the first definition, which is more clearly
correct. We prove them equivalent all:
\begin{theorem}[SortedLocal\_iff]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~l \iff \var{SortedLocal}~l
  \end{equation*}
\end{theorem}
\begin{proof}
  The $\implies$ direction follows by induction. The $\impliedby$
  direction follows by induction on $l$, reasoning about the empty
  list, the singleton list, and lists with at least two elements. In
  the inductive case $l = a :: b :: l'$, we have that $a \le b$, and
  that $\forall x \in l',~b \le x$, which gives us $\forall x \in b :: l',~a \le x$ by
  transitivity.
\end{proof}

\begin{theorem}[SortedIx\_iff]
  A list $l$ is sorted under the inductive definition (\var{Sorted}) if
  and only if it is sorted under the index definition (\var{SortedIx}).
\end{theorem}
\begin{proof}
  We reason by induction on $l$. The base cases are easy, so we
  address the inductive case, $l = a :: l'$

  ($\implies$) We have to prove that $(a :: l')_i \le (a :: l')_j$ given
  $i \le j$ and $\var{Sorted}~l$. We reason by cases on $i$ and $j$:
  \begin{itemize}[noitemsep]
  \item $i = j = 0$. By reflexivity, $a \le a$.
  \item $i = 0,~j=j' + 1$. We need to show $a \le l'_{j'}$. We know that
    $a \le x$ for all $x \in l'$ by $\var{SortedIx}~(a::l')$, so we are
    done.
  \item $i = i' +1,~j = j' + 1$. We need to show $l'_{i'} \le l'_{j'}$,
    which follows from the induction hypothesis.
  \end{itemize}

  ($\impliedby$) We have $\var{SortedIx}~(a :: l')$, and we need to
  show $\var{Sorted}~a :: l'$ given the induction hypothesis
  $\var{SortedIx}~l' \implies \var{Sorted}~l'$. By
  \var{SortedLocal\_iff}, we can just prove $\var{SortedLocal}~(a ::
  l')$. If $l' = []$, we can apply $\var{SortedLocal\_1}$, so assume
  $l' = b :: l''$. To apply \var{SortedLocal\_cons} we need to show
  $\var{SortedLocal}~(b :: l'')$, which we have by the induction
  hypothesis and $\var{SortedIx}~(a :: b :: l'')$, and that $a \le b$.
  That follows because $a = (a :: b :: l'')_0$ and $b = (a :: b ::
  l'')_1$, so by $\var{SortedIx}~(a :: b :: l'')$, we are done.
\end{proof}

\subsection{\var{Permutation}}
\label{subsec:perm}

The word \textit{permutation} has two related meanings that are often
used interchangably. We sometimes say that two lists are permutations of each
other if they contain all of the same elements, just appearing in a
different order. However, in mathematics we often say that a
permutation is actually the bijection on indices that defines the
correspondence between the two lists. For example, we say that $p$ is
the permutation that relates $l$ to $l'$ if for all indices $i$ and
$j$,
\begin{equation*}
  l_{p(i)} = l'_i
\end{equation*}
For consistency with Coq's built-in \var{Permuation} relation, in a
situation like this we will say that $l$ and $l'$ are permutations of
each other, and that $p$ is the \textit{permutation function} that
relates $l$ to $l'$. We say that two lists $l$ and $l'$ are
permutations of each other is precisely when
\vspace{-\parskip}
\begin{itemize}[nolistsep]
  \item $\var{length}~l = \var{length}~l' = n$
  \item there exists a permutation function $p : \mathbb{N}_{<n} \to
    \mathbb{N}_{<n}$, where $p$ is a bijection, such that for all indices $i$,
    \begin{equation*}
      l_{p(i)} = l'_i
    \end{equation*}
\end{itemize}

In Coq, we can represent a finite function as a list of its outputs:
$f : \mathbb{N}_{<n} \to A$ is represented by the $\var{list}~A$
$[f(0),~f(1),\ldots,f(n-1)]$. For permutations, this is consistent with the
notation where we denote by $2~1~0$ the permutation $p(0) = 2$, $p(1)
= 1$, $p(2) = 0$. So, $p$ here would be represented by $[2,~1,~0]$.

To restrict the range to $\mathbb{N}_{<n}$ we say that $\forall i,~i \in p
\implies ~i<n$, to make the function surjective, we strengthen that to
$\forall i,~i \in p \iff ~i<n$, and to make it injective we require that $p$ not
contain any duplicate entries. The inductive property \var{NoDup} from
the Coq standard library captures that requirement:
\begin{align*}
  \infer[\var{NoDup\_nil}]{\var{NoDup}~[]}{}
  &&
  \infer[\var{NoDup\_cons}]{\var{NoDup}~(x :: l)}{%
    x \notin l & \var{NoDup}~l
  }
\end{align*}

This leads to the following definition of the property
$\var{PermFun}~n~p$, which holds when $p : \var{list}~\mathbb{N}$
represents a permutation function on lists of length $n$:
\lstinputlisting[
  firstline=14, lastline=15,
]{../theories/BWT/Sorting/PermFun.v}
Then, we can define \var{apply}, which permutes a list according to a
given permutation:
\lstinputlisting[
  widthgobble=2, firstline=161, lastline=165,
]{../theories/BWT/Sorting/PermFun.v}
And finally, we can define \var{PermutationEx}, which holds between
two lists when they are permutations of each other.
\lstinputlisting[
  widthgobble=2, firstline=510, lastline=511,
]{../theories/BWT/Sorting/PermFun.v}

As with \var{Sorted}, we would also like an inductive definition of
\var{Permutation}. The Coq standard library includes one:
\begin{align*}
  \infer[\var{perm\_nil}]{\var{Permutation}~[]~[]}{}
  &&
  \infer[\var{perm\_skip}]{\var{Permutation}~(x :: l)~(x :: l')}{%
    \var{Permutation}~l~l'
  }
  \\
  \\
  \infer[\var{perm\_swap}]{\var{Permutation}~(x :: y :: l)~(y :: x :: l)}{}
  &&
  \infer[\var{perm\_trans}]{\var{Permutation}~l~l''}{
    \var{Permutation}~l~l' & \var{Permutation}~l'~l''
  }
\end{align*}

We will prove these two definitions equivalent, but first we confirm that our definition of \var{PermFun} ensures that .

We know we will
eventually need to reason inductively about permutation functions, so
we would like to define the relationship between applying a
permutation function $i :: p$ and applying $p$. Conceptually, the
first element of $\var{apply}~(i::p)~l$ will be $l_i$. The rest will
be the result of applying some new permutation $p'$, where everything
greater than $i$ has been shifted down by one, to some new list $l'$,
which has its $i$th element removed. So, we define \var{rem\_PermFun}
and \var{rem\_nth} respectively, hoping to prove that
\begin{equation*}
  \var{apply}~(i::p)~l =
  l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l)
\end{equation*}
\lstinputlisting[
  widthgobble=4, firstline=229, lastline=229,
]{../theories/BWT/Sorting/PermFun.v}
\vspace{\parskip}
\lstinputlisting[
  widthgobble=2, firstline=599, lastline=607,
]{../theories/BWT/Lib/List.v}

These two theorems define how \var{rem\_nth} affects indexing.
\begin{theorem}[nth\_lt\_rem\_nth, nth\_ge\_rem\_nth]
  For all lists $l$ and indices $i$ and $j$,
  \begin{equation*}
    j < i \implies (\var{rem\_nth}~i~l)_j = l_j
  \end{equation*}
  and
  \begin{equation*}
    j \ge i \implies (\var{rem\_nth}~i~l)_j = l_{j+1}
  \end{equation*}
\end{theorem}
\begin{proof}
  Both theorems follow by induction on $i$.
\end{proof}

As we expect, removing the $n$th element and sticking it on the front
preserves all elements.
\begin{theorem}[rem\_nth\_Perm]
  For all list $l$ and indices $i$
  \begin{equation*}
    \var{Permutation}~l~(l_i :: \var{rem\_nth}~i~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows from induction on $l$.
\end{proof}

\begin{theorem}[rem\_PermFun\_correct]
  For all permutation functions $p$, lists $l$, and indices $i$.
  \begin{equation*}
    \var{apply}~(i::p)~l =
    l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  Working backwards from our goal,
  \begin{align*}
    \var{apply}~(i::p)~l &=
    l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l) \\
    l_i :: \var{map}~(\lambda j.~l_j)~p &=
    l_i :: \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_j)~(\var{rem\_PermFun}~i~p) \\
    \var{map}~(\lambda j.~l_j)~p &=
    \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_j)~(\var{map}~(\lambda j. \bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j)~p) \\
    \var{map}~(\lambda j.~l_j)~p &=
    \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_{\bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j})~p \\
  \end{align*}
  From here, we need to prove that the mapping functions agree on all
  the elements of $p$, namely that for all $j \in p$
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_{\bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j}
  \end{equation*}
  We know that $i \neq j$, because $j \in p$ and $\var{NoDup}~(i :: p)$.
  Therefore there are two cases, $i < j$ and $i > j$. When $i < j$, we have
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_{j-1}
  \end{equation*}
  by \var{nth\_ge\_rem\_nth}, and when $i > j$, we have
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_j
  \end{equation*}
  by \var{nth\_lt\_rem\_nth}.
\end{proof}

\begin{theorem}[apply\_correct]
  For all permutation functions $p$ and lists $l$,
  \begin{equation*}
    \var{Permutation}~(\var{apply}~p~l)~l
  \end{equation*}
\end{theorem}
\begin{proof}
  First we let $n = \var{length}~l$, so that we can reason by
  induction on $n$. The base case is uninteresting. In the inductive
  case, we have the induction hypothesis that for all lists of length
  $n$ and permutation functions on such lists,
  \begin{equation*}
    \var{Permutation}~(\var{apply}~p~l)~l
  \end{equation*}
  and we must show that
  \begin{equation*}
    \var{Permutation}~(\var{apply}~(i :: p)~(a :: l))~(a :: l)
  \end{equation*}

  We reason as follows, using the notation $\eqperm$ to
  chain together \var{Permutation}s:
  \begin{align*}
    \var{apply}~(i :: p')~(a :: l) &\eqperm
    (a :: l)_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~(a::l)) \\
    &\eqperm (a :: l)_i :: \var{rem\_nth}~i~(a::l) \\
    &\eqperm a :: l
  \end{align*}
  where the first step follows from \var{rem\_PermFun\_correct}, the
  second from the induction hypothesis, and the last from
  \var{rem\_nth\_Perm}.
\end{proof}

\begin{theorem}[PermutationEx\_iff]
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{Permutation}~l~l' \iff \var{PermutationEx}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  The $\impliedby$ direction follows from \var{apply\_correct}.

  ($\implies$) We reason by induction on the evidence of
  $\var{Permutation}~l~l'$. The empty case is simple.
  \begin{itemize}
  \item We have a permutation function $p$ from the induction
    hypothesis such that $\var{apply}~p~l = l'$, and we need to
    construct a permutation function $p'$ such that
    $\var{apply}~p'~(x::l) = x :: l'$. We let $p' = 0 ::
    \var{map}~(+1)~p$, shifting every index up by one and mapping
    the first element of the list to itself.
  \item We have to construct a permutation function $p$ such that
    $\var{apply}~p~(y :: x :: l) = x :: y :: l$. We let $p = 1 :: 0 ::
    \var{map}~(+2)~(\var{seq}~0~(\var{length}~l))$, leaving every
    element in place except for the first 2.
  \item We have permutation functions $p_1$ and $p_2$ from our two
    induction hypotheses, such that
    \begin{align*}
      \var{apply}~p_1~l = l' && \var{apply}~p_2~l' = l''
    \end{align*}
    We need to construct a $p'$ such that $\var{apply}~p'~l=l''$, so
    we let $p' = p_2 \comp p_1$. For the definition of composition of
    permutations, see \cref{appendix:perm_comp}. Then,
    \begin{equation*}
      \var{apply}~(p_2 \comp p_1)~l =
      \var{apply}~p_2~(\var{apply}~p_1~l) = l'' \qedhere
    \end{equation*}
  \end{itemize}
\end{proof}

For completeness, we will briefly mention a third definition of a
permutation relation that says that two lists are permutations of one
another when any element occurs an equal number of times in each:
\lstinputlisting[
  widthgobble=2, firstline=14, lastline=15,
]{../theories/BWT/Sorting/PermutationCount.v}
\var{count\_occ} counts the number of occurences of a given variable
using a decision procedure for Leibniz equality. Here that decision
procedure is \texttt{equiv\_dec} from an \var{EqDec} instance where we
require the equivalence relation itself by Leibniz equality.

\begin{theorem}[PermutationCount\_iff]
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{Permutation}~l~l' \iff \var{PermutationCount}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  The ($\implies$) direction follows from induction in the evidence of
  $\var{Permutation}~l~l'$.

  In the ($\impliedby$) direction, we reason by induction on $l$. In
  the inductive case, we have to show $\var{Permutation}~(a::l)~l'$.
  We know that $a \in l'$ because $\var{count\_occ}~a~l' =
  \var{count\_occ}~a~(a::l) > 0$. That means we can deconstruct $l'$
  as $L \dplus a :: R$, and
  \begin{equation*}
    L \dplus a :: R \eqperm a :: L \dplus R \eqperm a :: l
  \end{equation*}
  where the last step follows from the induction hypothesis.
\end{proof}

\subsection{\var{StablePerm}}
\label{subsec:stable}

A stable sort is a sort that doesn't swap any two elements considered
equivalent by the ordering. This is relevant when the induced
equivalence relation does not imply equality, so that we can
distinguish between elements that the sort considers identical. A good
example of this is in radixsort: when we sort on a single column, the
sort can't tell the difference between strings that contain equal
characters in that column, but it is crucial that it respect the
initial ordering in cases of a tie.

For stable sorts, we need a stronger property than \var{Permutation}.
We can extend our mathematical definition. We say that two lists $l$
and $l'$ are stable permutations of each other when we have a
permutation $p$ that relates them, and that satisfies an additional
stability property: for all indices $i$ and $j$ where
$(\var{apply}~p~l)_i \equiv (\var{apply}~p~l)_j$
\begin{gather*}
  i < j \implies p(i) < p(j)
\end{gather*}
Equivalently, we can say that for all indices $i$ and $j$ where $l_j \equiv
l_j$ we must have
\begin{gather*}
  i < j \implies p^{-1}(i) < p^{-1}(j)
\end{gather*}

Based on this, we define a stronger version of the \var{PermFun}
property, \var{StablePermFun}. However, while $\var{PermFun}~n~p$
means that $p$ represents a permutation function on lists of length
$n$, we need to specify the list with resepct to which $p$ is stable,
so we say $\var{StablePermFun}~l~p$ for some list $l$.
\lstinputlisting[
  widthgobble=2, firstline=557, lastline=561,
]{../theories/BWT/Sorting/PermFun.v}
As above, we can also define this property using the inverse
permutation:
\lstinputlisting[
  widthgobble=2, firstline=563, lastline=567,
]{../theories/BWT/Sorting/PermFun.v}
As usual, we will prove these definitions equivalent. First, though,
we need to define \var{image} and \var{preimage}.
\lstinputlisting[
  widthgobble=2, firstline=85, lastline=86,
]{../theories/BWT/Sorting/PermFun.v}
We will not prove it here, but $\var{preimage}~p$ and $\var{image}~p$
are both of bijections of type $\mathbb{N}_{<n} \to \mathbb{N}_{<n}$
and are invereses of each other. We also have the following two
theorems:
\begin{theorem}[nth\_preimage\_apply, nth\_image\_apply]
  For all lists $l$, permutations $p$, and indices $i$,
  \begin{align*}
    (\var{apply}~p~l)_{p^{-1}(i)} = l_i && l_{p(i)} = (\var{apply}~p~l)_i
  \end{align*}
\end{theorem}
\begin{proof}
  The first equation (\var{nth\_preimage\_apply}) holds because:
  \begin{align*}
       (\var{apply}~p~l)_{p^{-1}(i)}
    &= (\var{map}~(\lambda j.~l_j)~p)_{\var{findIndex}~i~p} \\
    &= l_{p_{(\var{findIndex}~i~p)}} \\
    &= l_i
  \end{align*}
  and the second follows from this:
  \begin{equation*}
      (\var{apply}~p~l)_i
    = (\var{apply}~p~l)_{p^{-1}(p(i))}
    = l_{p(i)}
    \qedhere
  \end{equation*}
\end{proof}

\begin{theorem}[StablePermFun\_iff]
  For all lists $l$ and permutation $p$,
  \begin{equation*}
    \var{StablePermFun}~l~p \iff \var{StablePermFun\_preimage}~l~p
  \end{equation*}
\end{theorem}
\begin{proof}
  We will only prove the $\implies$ direction; the other direction is
  very similar. We have that $l_i \equiv l_j$, and that $i < j$, and we
  have to show that $p^{-1}(i) < p^{-1}(j)$. We will show this by
  proving that $p^{-1}(i) \ge p^{-1}(j)$ leads to a contradiction.

  Suppose $p^{-1}(i) \ge p^{-1}(j)$. We actually know that $p^{-1}(i) >
  p^{-1}(j)$ because $i \neq j$. From $l_i \equiv l_j$ and
  $\var{StablePermFun}~l~p$, along with $p^{-1}(j) < p^{-1}(i)$, we
  know that $p(p^{-1}(j)) < p(p^{-1}(i))$. However, this implies $j <
  i$, which is a contradiction. Therefore, $p^{-1}(i) < p^{-1}(j)$.
\end{proof}

Now, we can define \var{StablePermEx} to describe when two lists are
stable permutations of each other:
\lstinputlisting[
  widthgobble=2, firstline=607, lastline=608,
]{../theories/BWT/Sorting/PermFun.v}

While \var{StablePermEx} corresponds to \var{PermutationEx}, how can
we modify \var{Permutation} to create an inductive relation for stable
lists? Intuitively, the problematic inference rule is
\var{Permutation\_swap}: we can only swap two elements and preserve
stability if they are not equivalent. With this fix, we have \var{StablePermInd}:
\begin{gather*}
  \infer[\var{StablePermInd\_nil}]{\var{StablePermInd}~[]~[]}{}
  \\ \\
  \infer[\var{StablePermInd\_skip}]{\var{StablePermInd}~(x :: l)~(x :: l')}{%
    \var{StablePermInd}~l~l'
  }
  \\ \\
  \infer[\var{StablePermInd\_swap}]
        {\var{StablePermInd}~(x :: y :: l)~(y :: x :: l)}
        {x \not\equiv y}
  \\ \\
  \infer[\var{StablePermInd\_trans}]{\var{StablePermInd}~l~l''}{
    \var{StablePermInd}~l~l' & \var{StablePermInd}~l'~l''
  }
\end{gather*}

Unforunately, neither of these two definitions will be particularly
useful for proving the theorems we would like to prove with them. The
best definition for stability does not come by analogy with
\var{Permutation}. The intuition is that for any two lists that are
stable permutations of each other, if we filter each list by
equivalence with any element, the resulting lists should be identical.
This is because they must have the same elements, since the original
lists are permutations of one another, and those elements must be in
the same order, since the original lists are specifically stable
permutations of one another.
\lstinputlisting[
  widthgobble=2, firstline=21, lastline=22,
]{../theories/BWT/Sorting/Stable.v}

We will prove equivalence between these three definitions, but first
we will mention some properties of \var{Stable}. \var{Stable} is an
equivalence relation, meaning it is reflexive, transitive, and symmetric.

\begin{lemma}[StablePerm\_app]
  For all lists $l$, $l'$, $m$ and $m'$,
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{StablePerm}~m~m' \implies
    \var{StablePerm}~(l \dplus m)~(l' \dplus m')
  \end{equation*}
\end{lemma}
\begin{proof}
  This follows from the distributivity of \var{filter} over \var{app}.
\end{proof}

\begin{lemma}[StablePerm\_skip]
  For all lists $l$ and $l'$, and elements $a$,
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{StablePerm}~(a::l)~(a::l')
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show $\var{filter}~(\equiv x)~(a::l) =
  \var{filter}~(\equiv x)~(a::l')$. If $x \equiv a$, $a$ is added to both lists,
  and if $x \not\equiv a$ it is added to neither.
\end{proof}

\begin{lemma}[StablePerm\_swap]
  For all lists $l$ and $l'$, and elements $a$ and $b$ where $a \not\equiv b$,
  \begin{equation*}
     \var{StablePerm}~(a::b::l)~(b::a::l)
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show $\var{filter}~(\equiv x)~(a::b::l) =
  \var{filter}~(\equiv x)~(b::a::l)$. The proof follows from case analysis
  on $x \equiv a$ and $x \equiv b$.
\end{proof}

\begin{lemma}[StablePerm\_cons\_app]
  For all lists $l$, $l_1$ and $l_2$, and elements $a$ such that $\forall b
  \in l_1,~a \not\equiv b$,
  \begin{equation*}
     \var{StablePerm}~l~(l_1 \dplus l_2) \implies \var{StablePerm}~(a
     :: l)~(l_1 \dplus a :: l_2)
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show
  \begin{equation*}
    \var{filter}~(\equiv x)~(a::l) = \var{filter}~(\equiv x)~(l_1 \dplus a :: l_2)
  \end{equation*}
  If $x \not\equiv a$ the proof is easy, so assume $x \equiv a$. Then we have
  \begin{align*}
      \var{filter}~(\equiv x)~(l_1 \dplus a :: l_2)
    &= \var{filter}~(\equiv x)~l_1 \dplus \var{filter}~(a :: l_2) \\
    &= [] \dplus a::\var{filter}~l_2 \\
    &= a::\var{filter}~l
  \end{align*}
  where we know that $\var{filter}~(\equiv x)~l_1 = []$ because $\forall b \in
  l_1,~a \not\equiv b$, and the last step follows from
  $\var{StablePerm}~l~(l_1 \dplus l_2)$.
\end{proof}

\begin{theorem}[StablePerm\_destr]
  For all lists $l$ and $l'$, and elements $h$ and $h'$ such that $h
  \not\equiv h'$,
  \begin{gather*}
    \var{StablePerm}~(h :: l)~(h' :: l') \implies \\
    \exists~l_1~l_2, ~l = l_1 \dplus h' :: l_2 \land (\forall x \in l_1, h' \not\equiv x)
  \end{gather*}
\end{theorem}
\begin{proof}
  Let $l_1 = \var{take\_while}~(\not\equiv h')~l$ and $l_2 =
  \var{drop\_while}~(\not\equiv h')~l$. Clearly $l = l_1 \dplus l_2$, and
  because $\var{StablePerm}~(h :: l)~(h' :: l')$, we know the first
  element of $l_2$ must be $h'$. Let $l_2'$ be the rest of $l_2$, and
  we have two lists $l_1$ and $l_2'$ that satisfy the theorem.
\end{proof}

\begin{theorem}[StablePerm\_length]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{length}~l = \var{length}~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $n = \var{length}~l$. In the inductive
  case, we need to show that $\var{length}~(h::t) =
  \var{length}~(h'::t')$ given that $\var{StablePerm}~(h::t)~(h'::t')$
  and the induction hypothesis that for all lists of length $n$,
  \var{StablePerm} implies their lengths are equal.

  If $h \equiv h'$, then from $\var{StablePerm}~(h::t)~(h'::t')$ we know
  that $h = h'$. We can then remove $h$ from the front of $h::t$ and
  $h::t'$ and apply the induction hypothesis.

  If $h \not\equiv h'$, we can destructure $t$ and $t'$ as $l_1 \dplus h'
  :: l_2$ and $l_1' \dplus h :: l_2'$ respectively, using
  \var{StablePerm\_destr}. Then,
  \begin{equation*}
      \var{length}~t
    = \var{length}~(h' :: l_1  \dplus l_2)
    = \var{length}~(h  :: l_1' \dplus l_2')
    =\var{length}~t'
  \end{equation*}
  where the first and last steps follow from the induction hypothesis,
  using the fact that $\var{StablePerm}~t~(h'::l_1 \dplus l_2)$ and
  $\var{StablePerm}~t'~(h::l_1' \dplus l_2')$.
\end{proof}

\begin{theorem}[StablePerm\_Perm]
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{Permutation}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by strong induction on the length of both lists (we know
  the lengths are equal by \var{StablePerm\_length}). In the inductive
  case, we need to show $\var{Permutation}~(h :: t)~(h' :: t')$. If $h
  \equiv h$, we know that $h = h'$, and the proof follows by
  \var{perm\_skip}.

  If $h \not\equiv$, we again destructure $t$ and $t'$ as $l_1 \dplus h' ::
  l_2$ and $l_1' \dplus h :: l_2'$ respectively, using
  \var{StablePerm\_destr}.
  \begin{align*}
    h::t &= h :: l_1 \dplus h' :: l_2 \\
    &\eqperm h :: h' :: l_1 \dplus l_2 \\
    &\eqperm h' :: h :: l_1 \dplus l_2 \\
    &\eqperm h' :: h :: l_1' \dplus l_2' \\
    &\eqperm h' :: l_1' \dplus h :: l_2' = h'::t
  \end{align*}
  The crucial step is $l_1 \dplus l_2 \eqperm l_1' \dplus l_2'$, which
  follows from the induction hypothesis.
\end{proof}

\begin{theorem}[StablePermInd\_iff]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \iff \var{StablePermInd}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  The ($\impliedby$) direction follows easily by induction on
  $\var{StablePermInd}~l~l'$, using \var{StablePerm\_skip},
  \var{StablePerm\_swap}, and \var{StablePerm\_trans}.

  The ($\implies$) direction is proved very similarly to
  \var{StablePerm\_Perm}, using \var{StablePerm\_destr} to split up
  the lists.
\end{proof}

Now we prove equivalence with \var{StablePermEx}.

\begin{theorem}[apply\_correct\_stable]
  For all lists $l$ and permutation functions $p$,
  \begin{equation*}
    \var{StablePermFun}~l~p \implies \var{StablePerm}~(\var{apply}~p~l)~l
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the length of $l$. In the inductive case,
  we need to show
  \begin{equation*}
    \var{StablePerm}~(\var{apply}~(i :: p)~(a :: l))~(a :: l)
  \end{equation*}

  Using $\eqstable$ to chain \var{StablePerm}s transitively,
  \begin{align*}
    \var{apply}~(i :: p)~(a :: l)
    &= (a :: l)_i  ::
    \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~(a :: l)) \\
    &\eqstable (a :: l)_i :: \var{rem\_nth}~i~(a :: l) \\
    &\eqstable (a :: l)_i
  \end{align*}
  where we rely on the induction hypothesis between the first and
  second lines.
\end{proof}

\begin{theorem}[StablePermEx\_iff]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \iff \var{StablePermEx}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We use \var{StablePermInd\_iff} and prove $\var{StablePermInd}~l~l'
  \iff \var{StablePermEx}~l~l'$.

  The $\implies$ direction is very similar to the $\implies$ direction
  of the \var{PermutationEx\_iff} proof: we reason by induction on
  $\var{StablePermInd}~l~l'$, constructing the same permutation
  functions in each case, but this time we have to prove that it is
  stable as well. The $\impliedby$ direction follows from
  \var{apply\_correct\_stable}.
\end{proof}

\section{Uniqueness of Stable Sorts}
\label{sec:unique}

Using our definitions of \var{StablePerm} and \var{Sorted}, we can
prove the theorem that allowed us to replace \var{lexsort} with
radixsort.

\begin{theorem}[stable\_sort\_unique]
  \begin{equation*}
    \var{Sorted}~l \implies \var{Sorted}~l' \implies
    \var{StablePerm}~l~l' \implies l = l'.
  \end{equation*}
\end{theorem}

With \var{lexsort}, we are guaranteed stability because we are using
an order, not just a preorder, and for orders all sorts are stable.

\begin{theorem}[all\_perm\_stable]
  If we have that equivalence implies Leibniz equality, then for all
  lists $l$ and $l'$
  \begin{equation*}
      \var{Permutation}~l~l' \implies \var{StablePerm}~l~l'
  \end{equation*}
\end{theorem}

\section{Insertion Sort, Radixsort, and \texttt{lexsort}}
\label{sec:sort}

\lstinputlisting[
  widthgobble=2, firstline=11, lastline=21,
]{../theories/BWT/Sorting/InsertionSort.v}

\begin{theorem}[sort\_sorted]
  \begin{equation*}
    \var{Sorted}~(\var{sort}~l)
  \end{equation*}
\end{theorem}

\begin{theorem}[sort\_stable]
  \begin{equation*}
    \var{Stable}~(\var{sort}~l)
  \end{equation*}
\end{theorem}

\lstinputlisting[
  widthgobble=2, firstline=22, lastline=23,
]{../theories/BWT/Sorting/RadixSort.v}

\begin{theorem}[radixsort\_sorted]
  \begin{equation*}
    (\forall x \in l,~\var{length}~x = n) \implies \var{Sorted}~(\var{radixsort}~l~n)
  \end{equation*}
\end{theorem}

\appendix

\section{Proving BWT Correct}
\label{appendix:BWT_proof}

In this section we prove the lemmas from the derivation that we left out:
\begin{enumerate}
\item
\begin{equation*}
  \var{map}~\var{rrot}~(\var{rots}~x) = \var{rrot}~(\var{rots}~x)
\end{equation*}

\item \var{hdsort} commutes with $\var{cols} (j+1)$.
  This proof is cool because in theory it can be proven via
  parametricity, but that's a metatheorem in Coq.

\item
  \begin{equation*}
    \var{cols}~(j+1)~(\var{map}~\var{rrot}~l) = \var{prepend\_col}~(\var{map}~\var{last}~l)~(\var{cols}~j~l)
  \end{equation*}
\end{enumerate}


\section{Permutation Composition}
\label{appendix:perm_comp}

\begin{theorem}[compose\_apply]
  For all lists $l$ and permutation functions $p_1$ and $p_2$
  \begin{equation*}
      \var{apply}~(p_2 \comp p_1)~l = \var{apply}~p2~(\var{apply}~p1~l)
  \end{equation*}
\end{theorem}

\printbibliography{}

\end{document}
