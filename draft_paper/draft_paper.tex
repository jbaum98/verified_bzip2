\documentclass[11pt, titlepage]{thesis}

\title{Towards a Verified Bzip2: A Functional Specification for Burrows--Wheeler}
\author{Jake Waksbaum}
\date{May 6, 2019}

\begin{document}

\maketitle

\tableofcontents
\clearpage

\section{Introduction}
\label{sec:intro}

The Burrows--Wheeler transform (BWT) is an invertible transformation
that makes a string more amenable to compression by other methods
\cite{bw}. It accomplishes this by creating runs of repeated letters
in the produced string. Of course, a simple way to create runs is to
sort the string, but that would be useless for compression because the
original string would be unrecoverable.

The forwards transform is straightforward to describe: for a string of
length \(n\), form the \(n \times n\) matrix of all cyclic shifts of
the original string (also referred to as the rotation matrix), sort
the rows lexicographically, and take the last column of the matrix. In
addition, we note the index where the original string appears in the
sorted rotation matrix. As shown in \Cref{fig:bw_ex}, applying the
algorithm to the string \var{abracadabra!} produces the new string
\var{ard!rcaaabb} and the index \var{3}. The transformation tends to
produce runs like the three \var{a}s or the two \var{b}s in
\var{ard!rcaaabb}, and it is this feature that makes the resulting
string easier to compress. Although it is not obvious, the transformed
string, together with the index, provide enough information to recover
the the original string. The BWT is discussed more in \cref{sec:bwt}.

\begin{figure}[!hb]
  \centering
  \begin{tt}
  \begin{tabular}{rc}
    0  & abracadabra! \\
    1  & bracadabra!a \\
    2  & racadabra!ab \\
    3  & acadabra!abr \\
    4  & cadabra!abra \\
    5  & adabra!abrac \\
    6  & dabra!abraca \\
    7  & abra!abracad \\
    8  & bra!abracada \\
    9  & ra!abracadab \\
    10 & a!abracadabr \\
    11 & !abracadabra
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{rc}
    0  & !abracadabr\textbf{a} \\
    1  & a!abracadab\textbf{r} \\
    2  & abra!abraca\textbf{d} \\
    \textit{3} & \textit{abracadabra\textbf{!}} \\
    4  & acadabra!ab\textbf{r} \\
    5  & adabra!abra\textbf{c} \\
    6  & bra!abracad\textbf{a} \\
    7  & bracadabra!\textbf{a} \\
    8  & cadabra!abr\textbf{a} \\
    9  & dabra!abrac\textbf{a} \\
    10 & ra!abracada\textbf{b} \\
    11 & racadabra!a\textbf{b}
  \end{tabular}
  \end{tt}
  \caption{BWT acting on \var{abracadabra!}}
  \label{fig:bw_ex}
\end{figure}

Bird and Mu \cite{birdmu,pearls} analyze the Burrows--Wheeler algorithm
in a functional setting. They express the algorithm as the composition
of many smaller functions, and then derive an expression for the
inverse transform from the specification that it inverts the forwards
transform. In addition to providing an intuitive derivation of the
inverse transform, this provides the outline of a proof of the
correctness of the Burrows--Wheeler algorithm. We give our version of
their derivation in \cref{sec:forwards_BWT,sec:inverse_BWT}.

Our main contribution is a formalization in the Coq proof assistant of
the BWT, together with a machine-checked proof that our implementation
is correct. Specifically, we implement the functions $\var{bwl} :
\var{list}~A \to \var{list}~A$ and $\var{bwn} : \var{list}~A \to
\mathbb{N}$ that produce the string and index part of the BWT
respectively, and the function $\var{unbwt} : \mathbb{N} \to
\var{list}~A \to \var{list}~A$ that reverses the transform. We prove the
following theorem:
\begin{theorem}[unbwt\_correct]
  \label{thm:unbwt_correct}
  For all lists $l$,
  \begin{equation*}
    \var{unbwt}~(\var{bwn}~l)~(\var{bwl}~l) = l
  \end{equation*}
\end{theorem}
The implementation of the functions and the outline of the proof are
based on Bird et~al.'s derivations.

The implementation is useful in its own right as a functional
specification for a potential verified implementation of a program
that relies on Burrows--Wheeler \cite{appel-func-spec}. But more
generally, this provides evidence that Bird's approach of program
calculation, in which programs are derived from their specifications
via algebraic manipulations, is useful for proving the correctness of
algorithms in a machine-checked setting. Bird has applied this
approach to greedy algorithms, the Boyer-Moore algorithm, the
Knuth-Morris-Pratt algorithm, Sudoku solvers, arithmetic coding, the
Schorr-Waite algorithm, the Johnson-Trotter algorithm, and many more
problems \cite{pearls}.

As a part of the proof of \Cref{thm:unbwt_correct}, we proved two
other noteworthy results that have not, to our knowledge, been proved
before in Coq. We implemented a least-significant-digit (LSD)
radixsort and proved its correctness,
\begin{theorem}[radixsort\_correct]
  \label{thm:radixsort_correct}%
  For all matrices $m$ with $n$ columns,
  \begin{equation*}
    \var{Sorted}~(\var{radixsort}~m~n) \land \var{Permutation}~(\var{radixsort}~m~n)~m
  \end{equation*}
\end{theorem}
and we proved that any two stable sorts are interchangeable,
\begin{restatable*}[StableSort\_unique]{theorem}{stablesortunique}
  For all functions $f, g: \var{list}~A \to \var{list}~A$,
  \begin{equation*}
    \var{StableSort}~f \implies \var{StableSort}~g \implies
    (\forall l,~f~l = g~l)
  \end{equation*}
\end{restatable*}
This in turn required developing suitable definitions for
\var{Sorted}, \var{Permutation}, and \var{StablePerm}. We explored
multiple definitions for each of these properties, and proved
equivalence between them.

The rest of this paper is organized as follows. \Cref{sec:bwt} details
the history of the BWT and its applications. \Cref{sec:ord} defines
what we need to know about a type $A$ to sort a $\var{list}~A$ as a
part of BWT. Following Bird et~al., \cref{sec:forwards_BWT} defines
the implementation of the forwards BWT in Coq, and
\cref{sec:inverse_BWT} derives an implementation of the inverse BWT
via program calculation. \Cref{sec:sorted_perm_stable} provides
multiple definitions of \var{Sorted}, \var{Permutation}, and
\var{StablePermutation}, and proves equivalence between them.
\Cref{sec:sort} implements insertion sort and radixsort, and proves
their correctness.

The full source code can be found at
\url{https://github.com/jbaum98/verifed_bzip2/tree/2019-05-06}.

\section{History and applications of the BWT}
\label{sec:bwt}

The Burrows--Wheeler transform was originally introduced as a pass in a
compression algorithm. First the input is transformed with BWT, which
tends to group together characters. The intuition for why this happens
is that all adjacent characters in the output string are followed in
the input string by characters that are adjacent in the sorted first
column. If there is any correlation between a character and the
character preceding it, bringing together the characters in the first
column will tend to bring together similar characters in the last
column.

The next pass is a Move-to-Front (MTF) encoding, in which a character
is represented by the number of different characters encountered since
the last occurrence of this character. This creates many runs of zeroes
from the runs in the output of BWT. On English texts, often 50-60\% of
the values produced by the MTF pass are zeroes \cite{fenwick2007}.
Often a Run-Length Encoding (RLE) pass is then used to encode only the
runs of zeroes with their lengths, using a bijective base-2 encoding
\cite{bw-analysis, tsai_2016}. Finally, some sort of zeroth-order
coder like a Huffman coder is used.

BWT is also at the heart of algorithms that exploit the relationship
between the rotation matrix in BWT and the suffix array data structure
in order to search for patterns in a compressed text
\cite{ferragina_index}. These techniques have been applied to enable
fast matching of DNA subsequences.

\section{Orders and preorders}
\label{sec:ord}

Although it is commonly demonstrated on strings, the BWT can be
applied to any list whose elements are comparable. To make that
precise, if we have a type $A$ and a relation on that type $\le$, we say
that this forms a preorder on $A$ when $\le$ is reflexive and transitive, that is
$x \le x$ for all $x:A$, and
\begin{equation*}
  x \le y \implies y \le z \implies x \le z
\end{equation*}
for all $x,y,z:A$.

We say that we have a \textit{total} preorder if for any two elements
$x$ and $y$, $x \le y \lor y \le x$.

We say that we have a \textit{decidable} preorder if we have a
decision procedure for determining whether $x \le y$ or $x \not\le y$.

We represent a total, decidable preorder on type with the
\var{Preord} typeclass in Coq:
\lstinputlisting[firstline=9, lastline=14]{../theories/BWT/Sorting/Ord.v}
We do not explicitly require reflexivity because it follows from
totality. Given a \var{Preord} instance for a type $A$, we will be
able to sort $\var{list}~A$ as shown in \cref{sec:sort}.

From $\le$, we can define
\begin{align*}
x \ge y \coloneqq y \le x && x < y \coloneqq \lnot (y \le x) && x > y \coloneqq \lnot (x \le y)
\end{align*}

We can also define
\begin{equation*}
  x \equiv y \coloneqq x \le y \land y \le x
\end{equation*}
$\equiv$ is a
decidable equivalence relation, which means it is reflexive,
transitive, and symmetric, and we have a decision procedure for it.

The builtin class \var{EqDec} is used to describe types with decidable
equivalence relations, so for any \var{Preord} instance we have the
\var{EqDec} instance \var{Preord\_EqDec}. In Coq, the notation
\var{=\hspace{0pt}=\hspace{0pt}=} is used for the equivalence relation of an \var{EqDec}
instance and \var{=\hspace{0pt}=} for its decision procedure.

We say that we have a total, decidable \textit{order} when we have
that for all $x$ and $y$,
\begin{equation*}
  x \equiv y \implies x = y
\end{equation*}
that is, the induced equivalence relation of the preorder ($\equiv$)
implies Leibniz equality ($=$). We use the \var{Ord} class in Coq
for this:
\lstinputlisting[firstline=175, lastline=176, widthgobble=2]{../theories/BWT/Sorting/Ord.v}
For the BWT, we require an \var{Ord} instance for $A$, but
elsewhere, such as in sorting, \var{Preord} will suffice.

\section{The forwards BWT}
\label{sec:forwards_BWT}

First, we define the left and right rotations of a list. \var{lrot}
moves the first element of a list to the end.
\lstinputlisting[widthgobble=2, firstline=24, lastline=28]{../theories/BWT/Rotation/Rotation.v}
\var{rrot} moves the last element of a list to the front, and
relies on \var{init}, which drops the last element of a list.
\lstinputlisting[widthgobble=2, firstline=18, lastline=22]{../theories/BWT/Lib/List.v}
\lstinputlisting[widthgobble=2, firstline=17, lastline=21]{../theories/BWT/Rotation/Rotation.v}
Note that in order to ensure that $\var{last}$ is total, it takes
an extra argument to return in the case of an empty list.

Now we produce the rotation matrix by repeatedly applying \var{lrot}.
We will use \var{iter}, which takes a function $f : A \to A$, a number
$n$, and an initial value $z$, and returns the list of intermediate
values obtained by applying $f$ to $z$ $n$ times. Or,
\begin{equation}
  \var{iter}~f~n~z = [z,~f~z,~f~(f~z),~\ldots,~f^{n-1} z]
  \label{eq:iter}
\end{equation}
\lstinputlisting[
  widthgobble=2,
  firstline=14,
  lastline=18
]{../theories/BWT/Lib/Iterate.v}

We can prove that this definition satisfies \Cref{eq:iter} by defining
the function \var{rep} for repeated function application.
\lstinputlisting[
  widthgobble=2,
  firstline=8,
  lastline=12
]{../theories/BWT/Lib/Repeat.v}
In proofs, we will still use the notation $f^i z$ for $\var{rep}~f~i~z$.

\begin{theorem}[rep\_r]
  For all $f$, $n$, and $z$,
  \begin{equation}
     f^i~(f~z) = f^{i+1}~z
  \end{equation}
\end{theorem}
\begin{proof}
  The theorem follows by induction on $n$.
\end{proof}

\begin{theorem}[iter\_length]
  For all $f$, $n$, and $z$,
  \begin{equation}
    \var{length}~(\var{iter}~f~n~z) = n
  \end{equation}
\end{theorem}
\begin{proof}
  The theorem follows by induction on $n$.
\end{proof}

\begin{theorem}[iter\_nth]
  For all $f$, $n$, $z$, and $i < n$,
  \begin{equation}
    (\var{iter}~f~n~z)_i = f^i~z
  \end{equation}
\end{theorem}
\begin{proof}
  The theorem follows by induction on $i$. The inductive case is
  \begin{align*}
       (\var{iter}~f~(n+1)~z)_{i+1}
    &= (z :: \var{iter}~f~n~(f~z))_{i+1} &&\{\text{\var{iter} def.}\} \\
    &= (\var{iter}~f~n~(f~z))_i          &&\{\text{by \var{nth} def.}\} \\
    &= f^i~(f~z)                            &&\{\text{by \var{rep\_r}}\} \\
    &= f^{i+1}~z && \qedhere
  \end{align*}
\end{proof}

Now we can define \var{rots} to produce the rotation matrix.
\lstinputlisting[
  widthgobble=2,
  firstline=17,
  lastline=18,
]{../theories/BWT/Rotation/Rots.v}

\var{bwl} produces the transformed string by taking the last column
of the sorted rotation matrix. \var{lexsort} sorts the rotation
matrix lexicographically using the \var{Ord} instance; we will
discuss its implementation in \cref{sec:sort}.
\lstinputlisting[
  widthgobble=2,
  firstline=31,
  lastline=35
]{../theories/BWT/BurrowsWheeler.v}

\var{bwn} returns the index in the sorted rotation matrix where the
original string appears. It relies on \var{findIndex}, which is
defined with respect to some decidable equivalence relation.
\lstinputlisting[
  widthgobble=2,
  firstline=12,
  lastline=19,
]{../theories/BWT/Lib/FindIndex.v}

When we use \var{findIndex} in \var{bwn}, we are using the
induced equivalence relation from the \var{Ord} instance, so it
finds the row that is Leibniz equal to $l$.
\lstinputlisting[
  widthgobble=2,
  firstline=73,
  lastline=74,
]{../theories/BWT/BurrowsWheeler.v}

Given these definitions, we can quickly prove that \var{bwn}
satisfies its specification, namely that for all non-empty $l$
\begin{equation*}
    (\var{lexsort}~(\var{rots}~l))_{(\var{bwn}~l)} = l
\end{equation*}

\begin{lemma}[findIndex\_correct]
  For all elements $x$ and lists $l$,
  \begin{equation*}
    (\exists y \in l,~x \equiv y) \implies l_{(\var{findIndex}~x~xs)} \equiv x
  \end{equation*}
  \begin{proof}
    This follows by induction on $l$: At every step, either the
    current element is equivalent to $x$ and we return $0$, or there
    must be such a $y$ in the rest of the list.
  \end{proof}
\end{lemma}

\begin{lemma}[orig\_in\_sorted\_rots]
  For every non-empty list $l$,
  \begin{equation*}
    \exists y \in \var{lexsort}~(\var{rots}~l),~x = y
  \end{equation*}
  \begin{proof}
    Clearly $l$ is in $\var{rots}~l$ at index 0, and because
    \var{lexsort} produces a permutation of its input, $l$ must be
    in $\var{lexsort}~(\var{rots}~l)$ as well.
  \end{proof}
\end{lemma}

\begin{theorem}[bwn\_correct]
  For every non-empty list $xs$,
  \begin{equation*}
    (\var{lexsort}~(\var{rots}~xs))_{(\var{bwn}~xs)} = xs
  \end{equation*}
  \begin{proof}
    This follows from \var{findIndex\_correct} if we can show that
    $\exists y \in xs,~x = y$, which we have by \var{orig\_in\_sorted\_rots}.
  \end{proof}
\end{theorem}

\section{The inverse BWT}
\label{sec:inverse_BWT}

Now we want to construct a function \var{unbwt} such that for all $l$,
\begin{equation}
  \var{unbwt}~(\var{bwn}~l)~(\var{bwl}~l) = l
  \label{eq:unbwt}
\end{equation}
If we could recreate the entire sorted rotation matrix from
$\var{bwl}~l$, it would be simple to recover $l$ by indexing into
the matrix at $\var{bwn}~l$.

More precisely, suppose we had a function function \var{recreate}
such that
\begin{equation}
  \var{recreate}~(\var{bwl}~l) = \var{lexsort}~(\var{rots}~l)
  \label{eq:recreate}
\end{equation}
Then, we could define \var{unbwt} as
\begin{lstlisting}
Definition unbwt (i : nat) (l : list A) : list A :=
  nth i (recreate l) l.
\end{lstlisting}
and \Cref{eq:unbwt} would follow directly from the \var{bwn\_correct}:
\begin{align*}
     \var{unbwt}~(\var{bwn}~l)~(\var{bwl}~l)
  &= (\var{recreate}~(\var{bwl}~l))_{(\var{bwn})~l}\\
  &= (\var{lexsort}~(\var{rots}~l))_{(\var{bwn})~l} \\
  &= l
\end{align*}

How, then, do we implement \var{recreate} such that \Cref{eq:recreate}
holds? A key insight is that because any two methods of sorting the
rotation matrix must produce the same output, we can pretend that
\var{lexsort} is implemented using an sorting algorithm we find
convenient. Specifically, if we examine how an LSD radixsort sorts the
rotation matrix (\Cref{fig:radixsort_rots}) we notice an interesting
pattern. Each column of the rotation matrix is a permutation of the
original string, so when we sort a on particular column, we always
produce the same result in that column. As we move right-to-left, the
column containing the sorted input string moves with us until it
reaches the left edge of the matrix.

\begin{figure}[!hb]
  \centering
  \begin{tt}
  \setlength{\tabcolsep}{0pt}
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    abracadabra&!& \\
    bracadabra!&a& \\
    cadabra!abr&a& \\
    dabra!abrac&a& \\
    bra!abracad&a& \\
    !abracadabr&a& \\
    racadabra!a&b& \\
    ra!abracada&b& \\
    adabra!abra&c& \\
    abra!abraca&d& \\
    acadabra!ab&r& \\
    a!abracadab&r&
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    racadabra&!&ab \\
    bracadabr&a&!a \\
    acadabra!&a&br \\
    a!abracad&a&br \\
    dabra!abr&a&ca \\
    bra!abrac&a&da \\
    cadabra!a&b&ra \\
    !abracada&b&ra \\
    abra!abra&c&ad \\
    ra!abraca&d&ab \\
    abracadab&r&a! \\
    adabra!ab&r&ac
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    a&!&abracadabr \\
    r&a&!abracadab \\
    d&a&bra!abraca \\
    !&a&bracadabra \\
    r&a&cadabra!ab \\
    c&a&dabra!abra \\
    a&b&ra!abracad \\
    a&b&racadabra! \\
    a&c&adabra!abr \\
    a&d&abra!abrac \\
    b&r&a!abracada \\
    b&r&acadabra!a
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &!&abracadabra \\
    &a&!abracadabr \\
    &a&bra!abracad \\
    &a&bracadabra! \\
    &a&cadabra!abr \\
    &a&dabra!abrac \\
    &b&ra!abracada \\
    &b&racadabra!a \\
    &c&adabra!abra \\
    &d&abra!abraca \\
    &r&a!abracadab \\
    &r&acadabra!ab
  \end{tabular}
  \end{tt}
  \caption{Sorting the rotation matrix with LSD radixsort. The
    highlighted column contains the sorted original string.}
  \label{fig:radixsort_rots}
\end{figure}

We also can notice that the columns to the right of the sorted column
move with it for similar reasons. The column directly to its right is
the result of sorting the input string, and then stably sorting it
again by the character that precedes it in the original string. The
next column is the result of sorting by each character, sorting by the
preceding character, and then sorting by the character 2 spaces in
front.

All this means that the columns to the left of the sorted column seem
like they don't matter, which suggests that we can recreate the entire
rotation matrix from just the last column (\Cref{fig:recreate}).
However, the column directly to the left of the sorted column is very
important, as it is the next column to be sorted. And we can see that
this column changes, only becoming the eventual last column of the
matrix in the second to last iteration. Why then, does it still work
if we use the last column everywhere?

We can think of the process of sorting the matrix as computing the
last column subject to the constraints of the lexicographic ordering.
For example, we know that the collection of all characters preceding
an \texttt{a} in the sorted column must be some permutation of
$[\texttt{c},~\texttt{d},~\texttt{r},~\texttt{r},~\texttt{!}]$,
because those are the characters that precede \texttt{a}s in the
original string. However, the order of these characters in the final
string
($[\texttt{r},~\texttt{d},~\texttt{!},~\texttt{r},~\texttt{c}]$) is
important. An \texttt{r} must be first, because in the original string
there is an \texttt{a} following an \texttt{r} which is itself
followed by a \texttt{!}, the smallest character. Similarly, although
the \texttt{d} and \texttt{!} precede \texttt{a}s that are both
followed by \texttt{bra}s, the next character for the \texttt{d} is a
\texttt{!}, and the next character for the \texttt{!} is a \texttt{c}.
This requires that the \texttt{d} precede the \texttt{!} in the last
column.

Generally, there are constraints on the relative order of the letters
in the column before the sorted column, and in each step we are
ensuring that these constraints are not violated one more character
deep in the string. By the time we have reached the last column, we
have satisfied all the constraints.

We can also think of this as computing the fixpoint of $\var{hdsort} \comp
\var{map}~\var{rrot}$: we continually move the rightmost column to the
front and sort on it, until we have sorted the whole matrix and this
becomes a no-op. The fact that the last column corresponds to the
fixpoint justifies why in \cref{fig:recreate} we can replace every
column left of the sorted column with the output of the BWT.

\begin{figure}[!hb]
  \centering
  \begin{tt}
  \setlength{\tabcolsep}{0pt}
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{aaaaaaaaaaa}&!& \\
    \textcolor{lightgray}{rrrrrrrrrrr}&a& \\
    \textcolor{lightgray}{ddddddddddd}&a& \\
    \textcolor{lightgray}{!!!!!!!!!!!}&a& \\
    \textcolor{lightgray}{rrrrrrrrrrr}&a& \\
    \textcolor{lightgray}{ccccccccccc}&a& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&b& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&b& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&c& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&d& \\
    \textcolor{lightgray}{bbbbbbbbbbb}&r& \\
    \textcolor{lightgray}{bbbbbbbbbbb}&r&
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{aaaaaaaaa}&!&ab \\
    \textcolor{lightgray}{rrrrrrrrr}&a&!a \\
    \textcolor{lightgray}{ddddddddd}&a&br \\
    \textcolor{lightgray}{!!!!!!!!!}&a&br \\
    \textcolor{lightgray}{rrrrrrrrr}&a&ca \\
    \textcolor{lightgray}{ccccccccc}&a&da \\
    \textcolor{lightgray}{aaaaaaaaa}&b&ra \\
    \textcolor{lightgray}{aaaaaaaaa}&b&ra \\
    \textcolor{lightgray}{aaaaaaaaa}&c&ad \\
    \textcolor{lightgray}{aaaaaaaaa}&d&ab \\
    \textcolor{lightgray}{bbbbbbbbb}&r&a! \\
    \textcolor{lightgray}{bbbbbbbbb}&r&ac
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{a}&!&abracadabr \\
    \textcolor{lightgray}{r}&a&!abracadab \\
    \textcolor{lightgray}{d}&a&bra!abraca \\
    \textcolor{lightgray}{!}&a&bracadabra \\
    \textcolor{lightgray}{r}&a&cadabra!ab \\
    \textcolor{lightgray}{c}&a&dabra!abra \\
    \textcolor{lightgray}{a}&b&ra!abracad \\
    \textcolor{lightgray}{a}&b&racadabra! \\
    \textcolor{lightgray}{a}&c&adabra!abr \\
    \textcolor{lightgray}{a}&d&abra!abrac \\
    \textcolor{lightgray}{b}&r&a!abracada \\
    \textcolor{lightgray}{b}&r&acadabra!a
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &!&abracadabra \\
    &a&!abracadabr \\
    &a&bra!abracad \\
    &a&bracadabra! \\
    &a&cadabra!abr \\
    &a&dabra!abrac \\
    &b&ra!abracada \\
    &b&racadabra!a \\
    &c&adabra!abra \\
    &d&abra!abraca \\
    &r&a!abracadab \\
    &r&acadabra!ab
  \end{tabular}
  \end{tt}
  \caption{Recreating the rotation matrix from the last column.}
  \label{fig:recreate}
\end{figure}

We implement \var{recreate} as follows, introducing a parameter $j$ so
that we can recursively recreate the first $j$ columns of the matrix,
one column at a time. Here $l$ is meant to be the output of the BWT,
the last column of the sorted rotation matrix.
\lstinputlisting[
  widthgobble=2, firstline=156, lastline=160,
]{../theories/BWT/BurrowsWheeler.v}
This definition relies on \var{prepend\_col} and \var{zipWith}, defined below.
\lstinputlisting[
  widthgobble=2, firstline=47, lastline=48,
]{../theories/BWT/Columns.v}
\lstinputlisting[
  widthgobble=2, firstline=16, lastline=20,
]{../theories/BWT/Lib/ZipWith.v}

Now we can redefine \var{unbwt} as
\lstinputlisting[
  widthgobble=2,
  firstline=196,
  lastline=197,
]{../theories/BWT/BurrowsWheeler.v}

Since we recreate the matrix column by column, we can define an
invariant using $\var{cols}~j$, which takes the first $j$ columns of a matrix.
\lstinputlisting[
  widthgobble=2,
  firstline=19,
  lastline=20,
]{../theories/BWT/Columns.v}

\begin{theorem}[recreate\_correct\_inv]
  For all lists $l$ and $j \le \var{length}~l$,
  \begin{equation*}
    \var{recreate}~j~(\var{bwl}~l) =
    \var{cols}~j~(\var{lexsort}~(\var{rots}~l))
  \end{equation*}
\end{theorem}

Assuming we can prove this theorem, we will be able to prove that our
implementation of \var{unbwt} is correct along the same lines as before.

To prove this invariant, we need to prove a few other theorems that
have already come up when explaining the algorithm conceptually.
First, we note that moving the last column of the rotation matrix to the front
is the same as moving the last row to the top, because the rotation
matrix is symmetric.

\begin{theorem}[map\_rrot\_rots]
  For all lists $l$,
  \begin{equation*}
    \var{map}~\var{rrot}~(\var{rots}~l) = \var{rrot}~(\var{rots}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  Letting $n = \var{length}~l$
  \begin{align*}
       &\var{map}~\var{rrot}~(\var{rots}~l) \\
    ={}&\var{map}~\var{rrot}~[l,~\var{lrot}~l,~\var{lrot}^2~l,~\ldots,~\var{lrot}^{n-1}~l] \\
    ={}&[\var{rrot}~l,~l,~\var{lrot}~l,~\ldots,~\var{lrot}^{n-2}~l] \\
    ={}&[\var{lrot}^{n-1}~l,~l,~\var{lrot}~l,~\ldots,~\var{lrot}^{n-2}~l] \\
    ={}&\var{rrot}~[l,~\var{lrot}~l,~\ldots,~\var{lrot}^{n-1}~l] \\
    ={}&\var{rrot}~(\var{rots}~l) \qedhere
  \end{align*}
\end{proof}

Now we can prove that the sorted rotation matrix is indeed a fixpoint.
\begin{theorem}[lexsort\_rots\_hdsort]
  For all lists $l$,
  \begin{equation*}
    (\var{hdsort} \comp
    \var{map}~\var{rrot})~(\var{lexsort}~(\var{rots}~l)) =
    \var{lexsort}~(\var{rots}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  By \var{StablePerm\_Sorted\_eq}, proved in \cref{sec:unique}, any
  two sorts are equivalent, and we can replace \var{lexsort} with an
  LSD radixsort implemented as
  \begin{equation*}
    \var{radixsort}~m \coloneqq (\var{hdsort} \comp \var{map}~\var{rrot})^n~m
  \end{equation*}
  where $m$ is a matrix with $n$ columns. In this case, we are sorting
  the rotation matrix and we let $n = \var{length}~l$. Then we have
  \begin{align*}
       &(\var{hdsort} \comp \var{map}~\var{rrot})~(\var{lexsort}~(\var{rots}~l)) \\
    ={}&(\var{hdsort} \comp \var{map}~\var{rrot})~((\var{hdsort} \comp \var{map}~\var{rrot})^n~(\var{rots}~l)) \\
    ={}&(\var{hdsort} \comp \var{map}~\var{rrot})^{n+1}~(\var{rots}~l) \\
    ={}&(\var{hdsort} \comp \var{map}~\var{rrot})^n~((\var{hdsort} \comp \var{map}~\var{rrot})~(\var{rots}~l)) \\
    ={}&\var{lexsort}~((\var{hdsort} \comp \var{map}~\var{rrot})~(\var{rots}~l)) \\
    ={}&\var{lexsort}~((\var{hdsort} \comp \var{rrot})~(\var{rots}~l)) \\
    ={}&\var{lexsort}~(\var{rots}~l)
  \end{align*}
  The last step follows because the function $\var{hdsort} \comp
  \var{rrot}$ permutes the rotation matrix
\end{proof}

We need two more lemmas that describe the interaction between
\var{cols} and $\var{map}~\var{rrot}$.

\begin{theorem}[cols\_hdsort\_comm]
  For all matrices $m$ and naturals $j$,
  \begin{equation*}
    \var{cols}~(j+1)~(\var{hdsort}~m) = \var{hdsort}~(\var{cols}~(j+1)~m)
  \end{equation*}
\end{theorem}
\begin{proof}
  \var{hdsort} only examines the first elements of the rows of $m$,
  and $\var{cols}~(j+1)$ doesn't alter the first elements of the rows
  of $m$.\footnote{This argument is made precise by
    \var{key\_sort\_inv} in \cref{sec:lex_ord}.}
\end{proof}

\begin{theorem}[cols\_map\_rrot]
  For all $j$ and matrices $m$ that has rows all of length less than $j$,
  \begin{equation*}
    \var{cols}~(j+1)~(\var{map}~\var{rrot}~m) =
    \var{prepend\_col}~(\var{map}~\var{last}~m)~(\var{cols}~j~m)
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $m$. In the inductive case we need to show that
  \begin{equation*}
    \var{cols}~(j+1)~(\var{map}~\var{rrot}~(r::m)) =
    \var{prepend\_col}~(\var{map}~\var{last}~(r::m))~(\var{cols}~j~(r::m))
  \end{equation*}
  Simplifying, we have that the tails are equal by the induction
  hypothesis, and we must show that the heads are equal.
  \begin{align*}
    \var{firstn}~(j+1)~(\var{rrot}~r) &= \var{last}~r :: \var{firstn}~j~r \\
    \var{firstn}~(j+1)~(\var{last}~r :: \var{init}~r) &= \var{last}~r
    :: \var{firstn}~j~r \\
    \var{last}~r :: \var{firstn}~j~(\var{init}~r) &= \var{last}~r :: \var{firstn}~j~r \\
    \var{firstn}~j~(\var{init}~r) &= \var{firstn}~j~r
  \end{align*}
  The bottom equation is true by another theorem we will not prove here.
\end{proof}

Now we are ready to prove the invariant:
\begin{theorem}[recreate\_correct\_inv]
  For all lists $l$ and $j \le \var{length}~l$,
  \begin{equation*}
    \var{recreate}~j~(\var{bwl}~l) =
    \var{cols}~j~(\var{lexsort}~(\var{rots}~l))
  \end{equation*}
\end{theorem}
\begin{proof}
\begin{align*}
  &\var{cols}~(j+1)~(\var{lexsort}~(\var{rots}~l)) \\
={}& \var{cols}~(j+1)~(\var{hdsort}~(\var{map}~\var{rrot}~(\var{lexsort}~(\var{rots}~l)))) \\
={}&
\var{hdsort}~(\var{cols}~(j+1)~(\var{map}~\var{rrot}~(\var{lexsort}~(\var{rots}~l)))) \\
={}&
\var{hdsort}~(\var{prepend\_col}~(\var{map}~\var{last}~(\var{lexsort}~(\var{rots}~l)))~(\var{cols}~j~(\var{lexsort}~(\var{rots}~l))) \\
={}&
\var{hdsort}~(\var{prepend\_col}~(\var{bwl}~l)~(\var{recreate}~j~(\var{bwl}~l))) \\
={}& \var{recreate}~j~(\var{bwl}~l) \qedhere
\end{align*}
\end{proof}

\section{\var{Sorted}, \var{Permutation}, and \var{StablePermutation}}
\label{sec:sorted_perm_stable}

The previous section's proofs relied on the correctness and stability
of various sorts. The Coq standard library includes definitions for
\var{Sorted} and \var{Permutation}, but no definition for stability.
The Coq standard library also includes an implementation of mergesort.
However, it uses Coq's module system to enable parametricity over
ordered types, which is much less convenient to use than typeclasses.
In this section, we define our own version of \var{Sorted} and
\var{StablePerm} that are based on our \var{Preord} typeclass.

When defining these properties, it is very important that they mean
what we would like them to mean. If we prove our implementations
satisfy our specifications, but our specifications are incorrect, then
the proof is worthless. In addition, it would be nice if the
properties are defined in a way that is elegant to reason about. So,
we define each property in multiple ways and prove equivalence between
them: some definitions are more obviously correct, while others might
be easier to reason about it different situations. Even if a
definition is never used, proving equivalence gives us confidence
about the correctness of the definitions.

\subsection{\var{Sorted}}
\label{subsec:sorted}

For a sorting function to be correct, its output must be sorted with
respect to the ordering defined by the \var{Preord} instance. There a
a few ways we might define what it means for a list to be sorted. In
mathematics, we might say that a list $l$ is sorted if for all indices
$i$ and $j$,
\begin{equation*}
  i \le j \implies l_i \le l_j
\end{equation*}
We will call this property \var{SortedIx}.

However, it is often easier to reason about inductively defined
properties, so we would like to define \var{Sorted} inductively. We
can say that as a base case, the empty list is sorted, and given a
sorted list, we can add an element that is smaller than all the
existing elements and the result will also be sorted:
\begin{align*}
  \infer[\var{Sorted\_nil}]{\var{Sorted}~[]}{}
  &&
  \infer[\var{Sorted\_cons}]{\var{Sorted}~(a :: l)}{%
    (\forall~x \in l,~a < x) & \var{Sorted}~l
  }
\end{align*}

By transitivity it suffices to check that the new element is
smaller than just the first element of the sorted list. That leads to a third definition of sorted:
\begin{gather*}
  \begin{align*}
    \infer[\var{SortedLocal\_nil}]{\var{SortedLocal}~[]}{}
    &&
    \infer[\var{SortedLocal\_1}]{\var{SortedLocal}~[a]}{}
  \end{align*}
  \\
  \\
  \infer[\var{SortedLocal\_cons}]{\var{SortedLocal}~(a :: b :: l)}{%
    a < b & \var{SortedLocal}~(b :: l)
  }
\end{gather*}

Although these two inductive definitions are intuitive, they are not
obviously equivalent to the first definition, which is more clearly
correct. We will prove all three equivalent.

\begin{theorem}[SortedLocal\_iff]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~l \iff \var{SortedLocal}~l
  \end{equation*}
\end{theorem}
\begin{proof}
  Both directions follow by induction on $l$, making use of the
  transitivity of $\le$.
\end{proof}

\begin{theorem}[SortedIx\_iff]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~l \iff \var{SortedIx}~l
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $l$. The base cases are easy, so we
  address the inductive case, $l = a :: l'$

  ($\implies$) We have to prove that $(a :: l')_i \le (a :: l')_j$ given
  $i \le j$ and $\var{Sorted}~l$. We reason by cases on $i$ and $j$:
  \begin{itemize}
  \item $i = j = 0$. By reflexivity, $a \le a$.
  \item $i = 0,~j=j' + 1$. We need to show $a \le l'_{j'}$. We know that
    $a \le x$ for all $x \in l'$ by $\var{Sorted}~(a::l')$, so we are
    done.
  \item $i = i' +1,~j = j' + 1$. We need to show $l'_{i'} \le l'_{j'}$,
    which follows from the induction hypothesis.
  \end{itemize}

  ($\impliedby$) We have $\var{SortedIx}~(a :: l')$, and we need to
  show $\var{Sorted}~a :: l'$ given the induction hypothesis
  $\var{SortedIx}~l' \implies \var{Sorted}~l'$. By
  \var{SortedLocal\_iff}, we can just prove $\var{SortedLocal}~(a ::
  l')$. If $l' = []$, we can apply $\var{SortedLocal\_1}$, so assume
  $l' = b :: l''$. To apply \var{SortedLocal\_cons} we need to show
  $\var{SortedLocal}~(b :: l'')$, which we have by the induction
  hypothesis and $\var{SortedIx}~(a :: b :: l'')$, and that $a \le b$.
  That follows because $a = (a :: b :: l'')_0$ and $b = (a :: b ::
  l'')_1$, so by $\var{SortedIx}~(a :: b :: l'')$, we are done.
\end{proof}

\subsection{\var{Permutation}}
\label{subsec:perm}

The word \textit{permutation} has two related meanings that are often
used interchangeably. We sometimes say that two lists are permutations of each
other if they contain all of the same elements, just appearing in a
different order. However, in mathematics we often say that a
permutation is actually the bijection on indices that defines the
correspondence between the two lists. For example, we say that $p$ is
the permutation that relates $l$ to $l'$ if for all indices $i$ and
$j$,
\begin{equation*}
  l_{p(i)} = l'_i
\end{equation*}
For consistency with Coq's built-in \var{Permuation} relation, in a
situation like this we will say that $l$ and $l'$ are permutations of
each other, and that $p$ is the \textit{permutation function} that
relates $l$ to $l'$. We say that two lists $l$ and $l'$ are
permutations of each other is precisely when
\begin{itemize}
  \item $\var{length}~l = \var{length}~l' = n$
  \item there exists a permutation function $p : \mathbb{N}_{<n} \to
    \mathbb{N}_{<n}$, where $p$ is a bijection, such that for all indices $i$,
    \begin{equation*}
      l_{p(i)} = l'_i
    \end{equation*}
\end{itemize}

In Coq, we can represent a finite function as a list of its outputs:
$f : \mathbb{N}_{<n} \to A$ is represented by the $\var{list}~A$
$[f(0),~f(1),\ldots,f(n-1)]$. For permutations, this is consistent with the
notation where we denote by $2~1~0$ the permutation $p(0) = 2$, $p(1)
= 1$, $p(2) = 0$. So, $p$ here would be represented by $[2,~1,~0]$.

To restrict the range to $\mathbb{N}_{<n}$ we say that $\forall i,~i \in p
\implies ~i<n$; to make the function surjective, we strengthen that to
$\forall i,~i \in p \iff ~i<n$; and to make it injective we require that $p$ not
contain any duplicate entries. The inductive property \var{NoDup} from
the Coq standard library captures that requirement:
\begin{align*}
  \infer[\var{NoDup\_nil}]{\var{NoDup}~[]}{}
  &&
  \infer[\var{NoDup\_cons}]{\var{NoDup}~(x :: l)}{%
    x \notin l & \var{NoDup}~l
  }
\end{align*}

This leads to the following definition of the property
$\var{PermFun}~n~p$, which holds when $p : \var{list}~\mathbb{N}$
represents a permutation function on lists of length $n$:
\lstinputlisting[
  firstline=14, lastline=15,
]{../theories/BWT/Sorting/PermFun.v}
Then, we can define \var{apply}, which permutes a list according to a
given permutation:
\lstinputlisting[
  widthgobble=2, firstline=161, lastline=165,
]{../theories/BWT/Sorting/PermFun.v}
And finally, we can define \var{PermutationEx}, which holds between
two lists when they are permutations of each other.
\lstinputlisting[
  widthgobble=2, firstline=510, lastline=511,
]{../theories/BWT/Sorting/PermFun.v}

As with \var{Sorted}, we would also like an inductive definition of
\var{Permutation}. The Coq standard library includes one:
\begin{align*}
  \infer[\var{perm\_nil}]{\var{Permutation}~[]~[]}{}
  &&
  \infer[\var{perm\_skip}]{\var{Permutation}~(x :: l)~(x :: l')}{%
    \var{Permutation}~l~l'
  }
  \\
  \\
  \infer[\var{perm\_swap}]{\var{Permutation}~(x :: y :: l)~(y :: x :: l)}{}
  &&
  \infer[\var{perm\_trans}]{\var{Permutation}~l~l''}{
    \var{Permutation}~l~l' & \var{Permutation}~l'~l''
  }
\end{align*}

We know we will eventually need to reason inductively about
permutation functions, so we would like to define the relationship
between applying a permutation function $i :: p$ and applying $p$.
Conceptually, the first element of $\var{apply}~(i::p)~l$ will be
$l_i$. The rest will be the result of applying some new permutation
$p'$, where everything greater than $i$ has been shifted down by one,
to some new list $l'$, which has its $i$th element removed. So, we
define \var{rem\_PermFun} and \var{rem\_nth} respectively, hoping to
prove that
\begin{equation*}
  \var{apply}~(i::p)~l =
  l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l)
\end{equation*}
\lstinputlisting[
  widthgobble=4, firstline=229, lastline=229,
]{../theories/BWT/Sorting/PermFun.v}
\vspace{\parskip}
\lstinputlisting[
  widthgobble=2, firstline=599, lastline=607,
]{../theories/BWT/Lib/List.v}

These two theorems define how \var{rem\_nth} affects indexing.
\begin{theorem}[nth\_lt\_rem\_nth, nth\_ge\_rem\_nth]
  For all lists $l$ and indices $i$ and $j$,
  \begin{equation*}
    j < i \implies (\var{rem\_nth}~i~l)_j = l_j
  \end{equation*}
  and
  \begin{equation*}
    j \ge i \implies (\var{rem\_nth}~i~l)_j = l_{j+1}
  \end{equation*}
\end{theorem}
\begin{proof}
  Both theorems follow by induction on $i$.
\end{proof}

As we expect, removing the $n$th element and sticking it on the front
preserves all elements.
\begin{theorem}[rem\_nth\_Perm]
  For all list $l$ and indices $i$
  \begin{equation*}
    \var{Permutation}~l~(l_i :: \var{rem\_nth}~i~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows from induction on $l$.
\end{proof}

\begin{theorem}[rem\_PermFun\_correct]
  For all permutation functions $p$, lists $l$, and indices $i$.
  \begin{equation*}
    \var{apply}~(i::p)~l =
    l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  Working backwards from our goal,
  \begin{align*}
    \var{apply}~(i::p)~l &=
    l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l) \\
    l_i :: \var{map}~(\lambda j.~l_j)~p &=
    l_i :: \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_j)~(\var{rem\_PermFun}~i~p) \\
    \var{map}~(\lambda j.~l_j)~p &=
    \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_j)~(\var{map}~(\lambda j. \bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j)~p) \\
    \var{map}~(\lambda j.~l_j)~p &=
    \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_{\bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j})~p \\
  \end{align*}
  From here, we need to prove that the mapping functions agree on all
  the elements of $p$, namely that for all $j \in p$
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_{\bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j}
  \end{equation*}
  We know that $i \neq j$, because $j \in p$ and $\var{NoDup}~(i :: p)$.
  Therefore there are two cases, $i < j$ and $i > j$. When $i < j$, we have
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_{j-1}
  \end{equation*}
  by \var{nth\_ge\_rem\_nth}, and when $i > j$, we have
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_j
  \end{equation*}
  by \var{nth\_lt\_rem\_nth}.
\end{proof}

\begin{theorem}[apply\_correct]
  For all permutation functions $p$ and lists $l$,
  \begin{equation*}
    \var{Permutation}~(\var{apply}~p~l)~l
  \end{equation*}
\end{theorem}
\begin{proof}
  First we let $n = \var{length}~l$, so that we can reason by
  induction on $n$. The base case is uninteresting. In the inductive
  case, we have the induction hypothesis that for $p :
  \var{list}~\mathbb{N}$ and $l : \var{list}~A$
  \begin{equation*}
    \var{lenth}~l = n \implies \var{PermFun}~n~p \implies
    \var{Permutation}~(\var{apply}~p~l)~l
  \end{equation*}
  Given $\var{PermFun}~(n+1)~(i::p')$, we must show that
  \begin{equation*}
    \var{Permutation}~(\var{apply}~(i :: p')~(a :: l))~(a :: l)
  \end{equation*}

  We reason as follows, using the notation $\eqperm$ to
  chain together \var{Permutation}s:
  \begin{align*}
    &\var{apply}~(i :: p')~(a :: l) \\
    \eqperm{}& \text{\{ by \var{rem\_PermFun\_correct} \}} \\
    &(a :: l)_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~(a::l))
    \\
    \eqperm{}& \text{\{ by IH \}} \\
    &(a :: l)_i :: \var{rem\_nth}~i~(a::l)
    \\
    \eqperm{}& \text{\{ \var{rem\_nth\_Perm} \}} \\
    & a :: l \qedhere
  \end{align*}
\end{proof}

\begin{theorem}[PermutationEx\_iff]
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{Permutation}~l~l' \iff \var{PermutationEx}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  The $\impliedby$ direction follows from \var{apply\_correct}.

  ($\implies$) We reason by induction on the evidence of
  $\var{Permutation}~l~l'$. The empty case is simple.
  \begin{itemize}
  \item We have a permutation function $p$ from the induction
    hypothesis such that $\var{apply}~p~l = l'$, and we need to
    construct a permutation function $p'$ such that
    $\var{apply}~p'~(x::l) = x :: l'$. We let $p' = 0 ::
    \var{map}~(+1)~p$, shifting every index up by one and mapping
    the first element of the list to itself.
  \item We have to construct a permutation function $p$ such that
    $\var{apply}~p~(y :: x :: l) = x :: y :: l$. We let $p = 1 :: 0 ::
    \var{map}~(+2)~(\var{seq}~0~(\var{length}~l))$, leaving every
    element in place except for the first 2.
  \item We have permutation functions $p_1$ and $p_2$ from our two
    induction hypotheses, such that
    \begin{align*}
      \var{apply}~p_1~l = l' && \var{apply}~p_2~l' = l''
    \end{align*}
    We need to construct a $p'$ such that $\var{apply}~p'~l=l''$, so
    we let $p' = p_2 \comp p_1$. For the definition of composition of
    permutations, see \cref{appendix:perm_comp}. Then,
    \begin{equation*}
      \var{apply}~(p_2 \comp p_1)~l =
      \var{apply}~p_2~(\var{apply}~p_1~l) = l'' \qedhere
    \end{equation*}
  \end{itemize}
\end{proof}

For completeness, we will briefly mention a third definition of a
permutation relation that says that two lists are permutations of one
another when any element occurs an equal number of times in each:
\lstinputlisting[
  widthgobble=2, firstline=14, lastline=15,
]{../theories/BWT/Sorting/PermutationCount.v}
\var{count\_occ} counts the number of occurrences of a given variable
using a decision procedure for Leibniz equality. Here that decision
procedure is \texttt{equiv\_dec} from an \var{EqDec} instance where we
require the equivalence relation itself by Leibniz equality.

\begin{theorem}[PermutationCount\_iff]
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{Permutation}~l~l' \iff \var{PermutationCount}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  The ($\implies$) direction follows from induction in the evidence of
  $\var{Permutation}~l~l'$.

  In the ($\impliedby$) direction, we reason by induction on $l$. In
  the inductive case, we have to show $\var{Permutation}~(a::l)~l'$.
  We know that $a \in l'$ because $\var{count\_occ}~a~l' =
  \var{count\_occ}~a~(a::l) > 0$. That means we can deconstruct $l'$
  as $L \dplus a :: R$, and
  \begin{equation*}
    L \dplus a :: R \eqperm a :: L \dplus R \eqperm a :: l
  \end{equation*}
  where the last step follows from the induction hypothesis.
\end{proof}

\subsection{\var{StablePerm}}
\label{subsec:stable}

A stable sort is a sort that doesn't swap any two elements considered
equivalent by the ordering. This is relevant when the induced
equivalence relation does not imply equality, so that we can
distinguish between elements that the sort considers identical. A good
example of this is in radixsort: when we sort on a single column, the
sort can't tell the difference between strings that contain equal
characters in that column, but it is crucial that it respect the
initial ordering in cases of a tie.

For stable sorts, we need a stronger property than \var{Permutation}.
We can extend our mathematical definition. We say that two lists $l$
and $l'$ are stable permutations of each other when we have a
permutation $p$ that relates them, and that satisfies an additional
stability property: for all indices $i$ and $j$ where
$(\var{apply}~p~l)_i \equiv (\var{apply}~p~l)_j$
\begin{gather*}
  i < j \implies p(i) < p(j)
\end{gather*}
Equivalently, we can say that for all indices $i$ and $j$ where $l_j \equiv
l_j$ we must have
\begin{gather*}
  i < j \implies p^{-1}(i) < p^{-1}(j)
\end{gather*}

Based on this, we define a stronger version of the \var{PermFun}
property, \var{StablePermFun}. However, while $\var{PermFun}~n~p$
means that $p$ represents a permutation function on lists of length
$n$, we need to specify the list with respect to which $p$ is stable,
so we say $\var{StablePermFun}~l~p$ for some list $l$.
\lstinputlisting[ widthgobble=2, firstline=557, lastline=561,
]{../theories/BWT/Sorting/PermFun.v} As above, we can also define this
property using the inverse permutation: \lstinputlisting[
  widthgobble=2, firstline=563, lastline=567,
]{../theories/BWT/Sorting/PermFun.v} As usual, we will prove these
definitions equivalent. First, though, we need to define \var{image}
and \var{preimage}. \lstinputlisting[ widthgobble=2, firstline=85,
  lastline=86, ]{../theories/BWT/Sorting/PermFun.v} We will not prove
it here, but $\var{preimage}~p$ and $\var{image}~p$ are both of
bijections of type $\mathbb{N}_{<n} \to \mathbb{N}_{<n}$ and are
inverses of each other. We also have the following two theorems:
\begin{theorem}[nth\_preimage\_apply, nth\_image\_apply]
  For all lists $l$, permutations $p$, and indices $i$,
  \begin{align*}
    (\var{apply}~p~l)_{p^{-1}(i)} = l_i && l_{p(i)} = (\var{apply}~p~l)_i
  \end{align*}
\end{theorem}
\begin{proof}
  The first equation (\var{nth\_preimage\_apply}) holds because:
  \begin{align*}
       (\var{apply}~p~l)_{p^{-1}(i)}
    &= (\var{map}~(\lambda j.~l_j)~p)_{\var{findIndex}~i~p} \\
    &= l_{p_{(\var{findIndex}~i~p)}} \\
    &= l_i
  \end{align*}
  and the second follows from this:
  \begin{equation*}
      (\var{apply}~p~l)_i
    = (\var{apply}~p~l)_{p^{-1}(p(i))}
    = l_{p(i)}
    \qedhere
  \end{equation*}
\end{proof}

\begin{theorem}[StablePermFun\_iff]
  For all lists $l$ and permutation $p$,
  \begin{equation*}
    \var{StablePermFun}~l~p \iff \var{StablePermFun\_preimage}~l~p
  \end{equation*}
\end{theorem}
\begin{proof}
  We will only prove the $\implies$ direction; the other direction is
  very similar. We have that $l_i \equiv l_j$, and that $i < j$, and we
  have to show that $p^{-1}(i) < p^{-1}(j)$. We will show this by
  proving that $p^{-1}(i) \ge p^{-1}(j)$ leads to a contradiction.

  Suppose $p^{-1}(i) \ge p^{-1}(j)$. We actually know that $p^{-1}(i) >
  p^{-1}(j)$ because $i \neq j$. From $l_i \equiv l_j$ and
  $\var{StablePermFun}~l~p$, along with $p^{-1}(j) < p^{-1}(i)$, we
  know that $p(p^{-1}(j)) < p(p^{-1}(i))$. However, this implies $j <
  i$, which is a contradiction. Therefore, $p^{-1}(i) < p^{-1}(j)$.
\end{proof}

Now, we can define \var{StablePermEx} to describe when two lists are
stable permutations of each other:
\lstinputlisting[
  widthgobble=2, firstline=607, lastline=608,
]{../theories/BWT/Sorting/PermFun.v}

While \var{StablePermEx} corresponds to \var{PermutationEx}, how can
we modify \var{Permutation} to create an inductive relation for stable
lists? Intuitively, the problematic inference rule is
\var{Permutation\_swap}: we can only swap two elements and preserve
stability if they are not equivalent. With this fix, we have \var{StablePermInd}:
\begin{gather*}
  \infer[\var{StablePermInd\_nil}]{\var{StablePermInd}~[]~[]}{}
  \\ \\
  \infer[\var{StablePermInd\_skip}]{\var{StablePermInd}~(x :: l)~(x :: l')}{%
    \var{StablePermInd}~l~l'
  }
  \\ \\
  \infer[\var{StablePermInd\_swap}]
        {\var{StablePermInd}~(x :: y :: l)~(y :: x :: l)}
        {x \not\equiv y}
  \\ \\
  \infer[\var{StablePermInd\_trans}]{\var{StablePermInd}~l~l''}{
    \var{StablePermInd}~l~l' & \var{StablePermInd}~l'~l''
  }
\end{gather*}

Unfortunately, neither of these two definitions will be particularly
useful for proving the theorems we would like to prove with them. The
best definition for stability does not come by analogy with
\var{Permutation}. The intuition is that for any two lists that are
stable permutations of each other, if we filter each list by
equivalence with any element, the resulting lists should be identical.
This is because they must have the same elements, since the original
lists are permutations of one another, and those elements must be in
the same order, since the original lists are specifically stable
permutations of one another.
\lstinputlisting[
  widthgobble=2, firstline=21, lastline=22,
]{../theories/BWT/Sorting/StablePerm.v}

We will prove equivalence between these three definitions, but first
we will mention some properties of \var{Stable}. \var{Stable} is an
equivalence relation, meaning it is reflexive, transitive, and symmetric.

\begin{lemma}[StablePerm\_app]
  For all lists $l$, $l'$, $m$ and $m'$,
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{StablePerm}~m~m' \implies
    \var{StablePerm}~(l \dplus m)~(l' \dplus m')
  \end{equation*}
\end{lemma}
\begin{proof}
  This follows from the distributivity of \var{filter} over \var{app}.
\end{proof}

\begin{lemma}[StablePerm\_skip]
  For all lists $l$ and $l'$, and elements $a$,
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{StablePerm}~(a::l)~(a::l')
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show $\var{filter}~(\equiv x)~(a::l) =
  \var{filter}~(\equiv x)~(a::l')$. If $x \equiv a$, $a$ is added to both lists,
  and if $x \not\equiv a$ it is added to neither.
\end{proof}

\begin{lemma}[StablePerm\_swap]
  For all lists $l$ and $l'$, and elements $a$ and $b$ where $a \not\equiv b$,
  \begin{equation*}
     \var{StablePerm}~(a::b::l)~(b::a::l)
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show $\var{filter}~(\equiv x)~(a::b::l) =
  \var{filter}~(\equiv x)~(b::a::l)$. The proof follows from case analysis
  on $x \equiv a$ and $x \equiv b$.
\end{proof}

\begin{lemma}[StablePerm\_cons\_app]
  For all lists $l$, $l_1$ and $l_2$, and elements $a$ such that $\forall b
  \in l_1,~a \not\equiv b$,
  \begin{equation*}
     \var{StablePerm}~l~(l_1 \dplus l_2) \implies \var{StablePerm}~(a
     :: l)~(l_1 \dplus a :: l_2)
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show
  \begin{equation*}
    \var{filter}~(\equiv x)~(a::l) = \var{filter}~(\equiv x)~(l_1 \dplus a :: l_2)
  \end{equation*}
  If $x \not\equiv a$ the proof is easy, so assume $x \equiv a$. Then we have
  \begin{align*}
      \var{filter}~(\equiv x)~(l_1 \dplus a :: l_2)
    &= \var{filter}~(\equiv x)~l_1 \dplus \var{filter}~(a :: l_2) \\
    &= [] \dplus a::\var{filter}~l_2 \\
    &= a::\var{filter}~l
  \end{align*}
  where we know that $\var{filter}~(\equiv x)~l_1 = []$ because $\forall b \in
  l_1,~a \not\equiv b$, and the last step follows from
  $\var{StablePerm}~l~(l_1 \dplus l_2)$.
\end{proof}

\begin{theorem}[StablePerm\_destr]
  For all lists $l$ and $l'$, and elements $h$ and $h'$ such that $h
  \not\equiv h'$,
  \begin{gather*}
    \var{StablePerm}~(h :: l)~(h' :: l') \implies \\
    \exists~l_1~l_2, ~l = l_1 \dplus h' :: l_2 \land (\forall x \in l_1, h' \not\equiv x)
  \end{gather*}
\end{theorem}
\begin{proof}
  Let $l_1 = \var{take\_while}~(\not\equiv h')~l$ and $l_2 =
  \var{drop\_while}~(\not\equiv h')~l$. Clearly $l = l_1 \dplus l_2$, and
  because $\var{StablePerm}~(h :: l)~(h' :: l')$, we know the first
  element of $l_2$ must be $h'$. Let $l_2'$ be the rest of $l_2$, and
  we have two lists $l_1$ and $l_2'$ that satisfy the theorem.
\end{proof}

\begin{theorem}[StablePerm\_length]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{length}~l = \var{length}~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $n = \var{length}~l$. In the inductive
  case, we need to show that $\var{length}~(h::t) =
  \var{length}~(h'::t')$ given that $\var{StablePerm}~(h::t)~(h'::t')$
  and the induction hypothesis that for all lists of length $n$,
  \var{StablePerm} implies their lengths are equal.

  If $h \equiv h'$, then from $\var{StablePerm}~(h::t)~(h'::t')$ we know
  that $h = h'$. We can then remove $h$ from the front of $h::t$ and
  $h::t'$ and apply the induction hypothesis.

  If $h \not\equiv h'$, we can destructure $t$ and $t'$ as $l_1 \dplus h'
  :: l_2$ and $l_1' \dplus h :: l_2'$ respectively, using
  \var{StablePerm\_destr}. Then,
  \begin{equation*}
      \var{length}~t
    = \var{length}~(h' :: l_1  \dplus l_2)
    = \var{length}~(h  :: l_1' \dplus l_2')
    =\var{length}~t'
  \end{equation*}
  where the first and last steps follow from the induction hypothesis,
  using the fact that $\var{StablePerm}~t~(h'::l_1 \dplus l_2)$ and
  $\var{StablePerm}~t'~(h::l_1' \dplus l_2')$.
\end{proof}

\begin{theorem}[StablePerm\_Perm]
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{Permutation}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by strong induction on the length of both lists (we know
  the lengths are equal by \var{StablePerm\_length}). In the inductive
  case, we need to show $\var{Permutation}~(h :: t)~(h' :: t')$. If $h
  \equiv h$, we know that $h = h'$, and the proof follows by
  \var{perm\_skip}.

  If $h \not\equiv$, we again destructure $t$ and $t'$ as $l_1 \dplus h' ::
  l_2$ and $l_1' \dplus h :: l_2'$ respectively, using
  \var{StablePerm\_destr}.
  \begin{align*}
    h::t ={}& h :: l_1 \dplus h' :: l_2 \\
    \eqperm{}& h :: h' :: l_1 \dplus l_2 \\
    \eqperm{}& h' :: h :: l_1 \dplus l_2 \\
    \eqperm{}& h' :: h :: l_1' \dplus l_2' \\
    \eqperm{}& h' :: l_1' \dplus h :: l_2' \\
    ={}& h'::t
  \end{align*}
  The crucial step is $l_1 \dplus l_2 \eqperm l_1' \dplus l_2'$, which
  follows from the induction hypothesis.
\end{proof}

\begin{theorem}[StablePermInd\_iff]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \iff \var{StablePermInd}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  The ($\impliedby$) direction follows easily by induction on
  $\var{StablePermInd}~l~l'$, using \var{StablePerm\_skip},
  \var{StablePerm\_swap}, and \var{StablePerm\_trans}.

  The ($\implies$) direction is proved very similarly to
  \var{StablePerm\_Perm}, using \var{StablePerm\_destr} to split up
  the lists.
\end{proof}

Now we prove equivalence with \var{StablePermEx}.

\begin{theorem}[apply\_correct\_stable]
  For all lists $l$ and permutation functions $p$,
  \begin{equation*}
    \var{StablePermFun}~l~p \implies \var{StablePerm}~(\var{apply}~p~l)~l
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the length of $l$. In the inductive case,
  we need to show
  \begin{equation*}
    \var{StablePerm}~(\var{apply}~(i :: p)~(a :: l))~(a :: l)
  \end{equation*}

  Using $\eqstable$ to chain \var{StablePerm}s transitively,
  \begin{align*}
    \var{apply}~(i :: p)~(a :: l)
    ={}& (a :: l)_i  ::
    \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~(a :: l)) \\
    \eqstable{}& (a :: l)_i :: \var{rem\_nth}~i~(a :: l) \\
    \eqstable{}& (a :: l)_i
  \end{align*}
  where we rely on the induction hypothesis between the first and
  second lines.
\end{proof}

\begin{theorem}[StablePermEx\_iff]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \iff \var{StablePermEx}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We use \var{StablePermInd\_iff} and prove $\var{StablePermInd}~l~l'
  \iff \var{StablePermEx}~l~l'$.

  The $\implies$ direction is very similar to the $\implies$ direction
  of the \var{PermutationEx\_iff} proof: we reason by induction on
  $\var{StablePermInd}~l~l'$, constructing the same permutation
  functions in each case, but this time we have to prove that it is
  stable as well. The $\impliedby$ direction follows from
  \var{apply\_correct\_stable}.
\end{proof}

\section{Uniqueness of stable sorts}
\label{sec:unique}

Using our definitions of \var{StablePerm} and \var{Sorted}, we can
prove the theorem that allowed us to replace \var{lexsort} with
radixsort.

\begin{theorem}[StablePerm\_Sorted\_eq]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~l \implies \var{Sorted}~l' \implies
    \var{StablePerm}~l~l' \implies l = l'.
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $l$. In the inductive case, we have to
  show that
  \begin{equation*}
    h :: t = h' :: t'
  \end{equation*}
  given $\var{Sorted}~(h :: t)$, $\var{Sorted}~(h' :: t')$, and
  $\var{StablePerm}~(h :: t)~(h' :: t')$.

  First, we can say that $h \equiv h'$: we know that $h \in h' :: t'$ and $h'
  \in h :: t$. If $h = h'$, then we are done, so assuming $h \neq h'$ we
  know $h \in t'$ and $h' \in t$. Because both $h::t$ and $h'::t'$ are
  sorted, we know that $h \le h'$ and $h' \le h$, implying $h \equiv h'$.

  This means that $h = h'$: from $\var{StablePerm}~(h :: t)~(h' ::
  t')$ we have
  \begin{align*}
    \var{filter}~(\equiv h)~(h :: t) &= \var{filter}~(\equiv h)~(h' :: t') \\
    h :: \var{filter}~(\equiv h)~t &= h' :: \var{filter}~(\equiv h)~t'
  \end{align*}
  We also have $t = t'$ by the induction hypothesis, so we are done.
\end{proof}

If we define \var{Sort} and \var{StableSort} as we did
\cref{sec:sorted_perm_stable}, we can restate the theorem slightly
more elegantly.
\lstinputlisting[
  widthgobble=2, firstline=15, lastline=19,
]{../theories/BWT/Sorting/Sort.v}

\begin{proof}
  This follows from \var{StablePerm\_Sorted\_eq}.
\end{proof}

However, we do not want to require that \var{lexsort} be implemented
using a stable sorting algorithm. Luckily, because we require that $A$
be equipped with an order and not just a preorder, we know that
equivalent elements are actually indistinguishable. This means that
all sorting algorithms can be considered to be stable: it is
impossible to tell if any two equivalent elements were swapped because
you cannot distinguish between equivalent elements.

\begin{theorem}[all\_perm\_stable]
  If we have that equivalence implies Leibniz equality, then for all
  lists $l$ and $l'$
  \begin{equation*}
      \var{Permutation}~l~l' \implies \var{StablePerm}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the evidence of $\var{Permutation}~l~l'$.
  Every case other than the \var{perm\_swap} case follows easily.
  In the \var{perm\_swap} case, we must show that
  \begin{equation*}
    \var{StablePerm}~(y :: x :: l)~(x :: y :: l)
  \end{equation*}
  for all $x$ and $y$. If $x \not\equiv y$, we can just apply
  \var{StablePerm\_swap}. If $x \equiv y$, then we know that $x = y$, and
  we can show
  \begin{equation*}
    \var{StablePerm}~(x :: x :: l)~(x :: x :: l)
  \end{equation*}
  by applying \var{perm\_skip} twice.
\end{proof}

\begin{corollary}[Sort\_StableSort\_Ord]
  When sorting using an order,
  \begin{equation*}
    \var{Sort}~f \iff \var{StableSort}~f
  \end{equation*}
  for any $f$.
\end{corollary}
\begin{proof}
  This follows from \var{all\_perm\_stable}.
\end{proof}

\stablesortunique
\begin{proof}
  This follows from \var{Sort\_StableSort\_Ord} and \var{StableSort\_unique}.
\end{proof}

\section{Lexicographic orders and keys}
\label{sec:lex_ord}

Given a preorder on a type $A$, we can define a lexicographic preorder
on $\var{list}~A$. Intuitively, we want to say that to compare two
lists, we compare their elements from left to right. As soon as we
encounter two elements that are not equivalent, their relationship
defines the relationship between the two lists. If we make it all the
way through one of the lists and all the elements are equivalent, we
say that the shorter list is smaller. If they are the same length and
all the elements are equivalent, then the lists are equivalent. To
capture this, we define the inductive relation \var{lex\_le}, which we
will denote by $\lexle$ in proofs.
\begin{gather*}
  \infer[\var{lex\_le\_nil}]{[] \lexle ys}{}
  \\
  \\
  \infer[\var{lex\_le\_cons\_lt}]{(x :: xs) \lexle (y :: ys)}{%
    x < y
  }
  \\
  \\
  \infer[\var{lex\_le\_cons\_eq}]{(x :: xs) \lexle (y :: ys)}{%
    x \equiv y & xs \lexle ys
  }
\end{gather*}

Given a preorder on a type $K$ and a key function $\var{key} : A \to K$,
we can define a preorder on $A$ that compares all elements using the
key function. That is, we can define the relation $\le_{\var{key}}$ on
$A$ such that for all $x, y : A$
\begin{equation*}
  x \le_{\var{key}} y \coloneqq (\var{key}~x) \le (\var{key}~y)
\end{equation*}

\begin{theorem}[key\_sort\_inv]
  Given a key function $\var{key} : A \to K$ and a function $f : A \to A$
  such that for all $x : A$,
  \begin{equation*}
    \var{key}~(f~x) = \var{key}~x
  \end{equation*}
  we have that for all lists $l$
  \begin{equation*}
    \var{sort}~(\var{map}~\var{f}~l) = \var{map}~\var{f}~(\var{sort}~l).
  \end{equation*}
  where \var{sort} is insertion sort.\footnote{This theorem actually
    holds for any function with type $\forall X, (X \to X \to \mathbb{B}) \to
    \var{list}~X \to \var{list}~X$ due to parametricity \cite{free}.}
\end{theorem}
\begin{proof}
  The proof follows by induction, unfolding the definition of
  \var{sort}. All decisions are made by calling \var{key}, so both
  sides are equal.
\end{proof}

Taken together, we can define prefix lexicographic orderings on lists,
where the lexicographic ordering is only allowed to compare up to the
first $n$ elements of two lists. We define the relation $\le_n$ on
$\var{list}~A$ such that for all $xs, ys : \var{list}~A$,
\begin{equation*}
  xs \le_n ys \coloneqq (\var{firstn}~n~xs) \lexle (\var{firstn}~n~ys)
\end{equation*}
For convenience, we define $\var{PrefixSorted}~n$ to hold when a list
of lists is sorted if you only look at the first $n$ elements of each:
\lstinputlisting[
  widthgobble=2, firstline=323, lastline=324,
]{../theories/BWT/Sorting/Key.v}

We can describe the relationship between orders that can look at fewer
or more characters with the following two theorems. We will briefly
offer an explanation of why these theorems makes sense, but these
explanations do not mirror the Coq proofs we have for these theorems.

\begin{theorem}[key\_lt\_firstn\_ge]
  For all $j$ and $k$ such that $j \le k$,
  \begin{equation*}
    x <_j y \implies x <_k y
  \end{equation*}
\end{theorem}
\begin{proof}
  If $x <_j y$, then there must be some index $i < j$ such that $x_i <
  y_i$. Then $i < k$ as well, so $x <_k y$.
\end{proof}

\begin{theorem}[key\_le\_firstn\_ge]
  For all $j$ and $k$ such that $k \ge j$,
  \begin{equation*}
    x \le_k y \implies x \le_j y
  \end{equation*}
\end{theorem}
\begin{proof}
  If $x \le_k y$, then either the first $k$ characters are all
  equivalent, in which case the first $j$ characters are all
  equivalent as well, or there was some $i < k$ such that $x_i < y_i$,
  but at every index before $i$ $x$ and $y$ are equivalent. If $j <
  i$, then the first $j$ characters are still equivalent, and we have
  $x \equiv y \implies x \le_j y$. If $j \ge i$, then the deciding character is
  among the first $j$ and $x \le_j k$.
\end{proof}

If we consider the implications this has for stability, a weaker order
can distinguish between fewer elements, which means that it considers
more elements equivalent. This means that stability with respect to a
weaker order is actually a stronger property, because the weaker
order, considering more elements equivalent, is more restrictive about
which swaps it allows.

\begin{theorem}[StablePerm\_weaken]
  Suppose we have a two preorders $O$ and $O'$ on $A$, and two lists
  $l_1$ and $l_2$, with elements of type $A$.

  We also know that on the elements of $l_1$, $O'$ is stronger than
  $O$. If we denote by $\le$ and $\le'$ the relations for $O$ and $O'$
  respectively, this means that for all $x$ and $y$ in $l_1$,
  \begin{equation*}
    x \le' y \implies x \le y.
  \end{equation*}
  Then, denoting by \var{StablePerm} stability relative to the
  equivalence relation of $O$, and by $\var{StablePerm}'$ stability
  relative to the equivalence relation of $O'$, we have
  \begin{equation*}
    \var{StablePerm}~l_1~l_2 \implies \var{StablePerm}'~l_1~l_2
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason inductively on the proof of $\var{StablePerm}~l_1~l_2$
  using \var{StablePermInd\_iff}. The important case is the
  \var{StablePermInd\_swap} case, where we have to show
  \begin{equation*}
    \var{StablePerm}'~(y :: x :: l)~(x :: y :: l)
  \end{equation*}
  given that $x \not\equiv y$. To show this, we need to know that $x \not\equiv'
  y$, which follows from the fact that $O'$ is stronger than $O$.
\end{proof}

\section{Radixsort}
\label{sec:sort}

In a least-significant-digit radix sort, we lexicographically sort a
list of strings of equal length by stably sorting on each column, from
the end of the string to the front (\Cref{fig:lsd_radixsort}). This
works because the lexicographic order says that if two strings are
equal up to a column then the tie is broken in according to the rest
of the string. This is reflected in the fact that when we stably sort
on a column, ties are broken in favor or the existing order, which was
established by sorted on the following columns.

\begin{figure}[!hb]
  \centering
  \begin{tt}
    \begin{tabular}{c}
      d a \textbf{b} \\
      a d \textbf{d} \\
      c a \textbf{b} \\
      f a \textbf{d} \\
      f e \textbf{e} \\
      b a \textbf{d} \\
      d a \textbf{d} \\
      b e \textbf{e} \\
      f e \textbf{d} \\
      b e \textbf{d} \\
      e b \textbf{b} \\
      a c \textbf{e}
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c}
      d \textbf{a} b \\
      c \textbf{a} b \\
      e \textbf{b} b \\
      a \textbf{d} d \\
      f \textbf{a} d \\
      b \textbf{a} d \\
      d \textbf{a} d \\
      f \textbf{e} d \\
      b \textbf{e} d \\
      f \textbf{e} e \\
      b \textbf{e} e \\
      a \textbf{c} e
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c}
      \textbf{d} a b \\
      \textbf{c} a b \\
      \textbf{f} a d \\
      \textbf{b} a d \\
      \textbf{d} a d \\
      \textbf{e} b b \\
      \textbf{a} c e \\
      \textbf{a} d d \\
      \textbf{f} e d \\
      \textbf{b} e d \\
      \textbf{f} e e \\
      \textbf{b} e e
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c}
      a c e \\
      a d d \\
      b a d \\
      b e d \\
      b e e \\
      c a b \\
      d a b \\
      d a d \\
      e b b \\
      f a d \\
      f e d \\
      f e e
    \end{tabular}
  \end{tt}
  \caption{LSD Radixsort}
  \label{fig:lsd_radixsort}
\end{figure}

In a functional setting, we can define an LSD radixsort elegantly by
repeatedly moving the last column to the front of the matrix and
sorting on the first column. For a matrix with $n$ columns,
\begin{equation*}
  \var{radixsort} = (\var{hdsort} \comp \var{map}~\var{rrot})^n
\end{equation*}
Or, in Coq:
\lstinputlisting[
  widthgobble=2, firstline=26, lastline=27,
]{../theories/BWT/Sorting/RadixSort.v}
Here \var{hdsort} is just an insertion sort that uses a lexicographic
ordering restricted to the first element of a list.

First we want to prove that \var{radixsort} permutes the rows of the
matrix. We will start by proving the invariant that if we rotate the
last column to the front and sort $j$ times, the resulting matrix is a
permutation of simply rotating the last column to the front $j$
times.
\begin{lemma}[radixsort\_perm\_inv]
  For all matrices $m$ and $j : \mathbb{N}$,
  \begin{equation*}
    \var{Permutation}~((\var{map}~\var{rrot})^j~m)~
                      ((\var{hdsort} \comp \var{map}~\var{rrot})^j~m)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $j$. In the inductive case,
  \begin{align*}
    (\var{hdsort} \comp \var{map}~\var{rrot})^{j+1}~m ={}&
    \var{hdsort}~(\var{map}~\var{rrot}~(\var{hdsort} \comp
    \var{map}~\var{rrot})^j~m) \\
    \eqperm{}& \var{map}~\var{rrot}~((\var{hdsort} \comp
    \var{map}~\var{rrot})^j~m) \\
    \eqperm{}& \var{map}~\var{rrot}~((\var{map}~\var{rrot})^j~m) \\
    ={}& (\var{map}~\var{rrot})^{j+1}~m \qedhere
  \end{align*}
\end{proof}

\begin{theorem}[radixsort\_perm]
  For all matrices $m$ with rows of length $n$,
  \begin{equation*}
    \var{Permutation}~m~(\var{radixsort}~m~n)
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{align*}
    m ={}&
    \var{map}~\var{rrot}^n~m \\
    ={}& (\var{map}~\var{rrot})^n~m \\
    \eqperm{}& (\var{hdsort} \comp \var{map}~\var{rrot})^n~m \\
    ={}& \var{radixsort}~m~n \qedhere
  \end{align*}
\end{proof}

We will prove the stability of \var{radixsort} in a similar way, but
first we need a lemma.
\begin{lemma}[StablePerm\_map\_rrot]
  For all matrices $m$ and $m'$,
  \begin{equation*}
    \var{StablePerm}~m~m' \implies
    \var{StablePerm}~(\var{map}~\var{rrot}~m)~(\var{map}~\var{rrot}~m').
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on the evidence that $\var{StablePerm}~m~m'$
  using \var{StablePermInd\_iff}. The important case is the swap case,
  where we have to show
  \begin{equation*}
    \var{StablePerm}~(\var{map}~(y :: x :: l))~(\var{map}~(x :: y :: l))
  \end{equation*}
  given that $x \not\equivle y$. For this it suffices to show that
  $\var{rrot}~x \not\equivle \var{rrot}~y$. Since lexicographic
  equivalence means that all the pairs of elements at corresponding
  indices are equivalent, when \var{rrot} moves the first elements of
  both lists to the end of both lists, it preserves lexicographic
  equivalence. Therefore
  \begin{equation*}
    x \equivle y \iff \var{rrot}~x \equivle \var{rrot}~y \qedhere
  \end{equation*}
\end{proof}

\begin{lemma}[radixsort\_stable\_inv]
  For all matrices $m$ and $j : \mathbb{N}$,
  \begin{equation*}
    \var{StablePerm}~((\var{map}~\var{rrot})^j~m)~
                     ((\var{hdsort} \comp \var{map}~\var{rrot})^j~m)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $j$. In the inductive case,
  \begin{align*}
    (\var{hdsort} \comp \var{map}~\var{rrot})^{j+1}~m ={}&
    \var{hdsort}~(\var{map}~\var{rrot}~(\var{hdsort} \comp
    \var{map}~\var{rrot})^j~m) \\
    \eqstable{}& \var{map}~\var{rrot}~((\var{hdsort} \comp
    \var{map}~\var{rrot})^j~m) \\
    \eqstable{}& \var{map}~\var{rrot}~((\var{map}~\var{rrot})^j~m) \\
    ={}& (\var{map}~\var{rrot})^{j+1}~m
  \end{align*}
  Going from the first line to the second line requires using
  \var{StablePerm\_weaken} to show that the stability we are
  guaranteed by \var{hdsort}, namely stability with respect to an
  ordering that examines at most one element, implies stability with
  respect to a regular lexicographic ordering.
\end{proof}

\begin{theorem}[radixsort\_stable]
  For all matrices $m$ with rows of length $n$,
  \begin{equation*}
    \var{StablePerm}~m~(\var{radixsort}~m~n)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof is almost identical to the proof of \var{radixsort\_perm}.
\end{proof}

We would also like to establish an invariant to prove that
\var{radixsort} produces a sorted list. To do so, we need to be able
to reason about the process of adding a column to the front of a
sorted matrix and sorting on that column. Intuitively, we need to know
that if we have a matrix that is sorted on the first $j$ columns, and
we add a new column to the front and sort on it, the result list will
be sorted on the first $j+1$ columns. For a proof, see
\cref{appendix:hdsort_sorted_S}.
\begin{theorem}[hdsort\_sorted\_S]
  For all matrices $m$ and naturals $j$,
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{map}~\var{tl}~m) \implies
    \var{PrefixSorted}~(j+1)~(\var{hdsort}~m)
  \end{equation*}
\end{theorem}

We also need the following property about \var{prepend\_col}, that
states that prepending a column and then removing the first column is
a no-op.
\begin{theorem}[map\_tl\_prepend]
  For all matrices $m : \var{list}~(\var{list}~A)$ and columns $c :
  \var{list}~A$, where $c$ has at least as many elements as $m$ has columns,
  \begin{equation*}
      \var{map}~\var{tl}~(\var{prepend\_col}~c~m) = m
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $m$. In the inductive case, we have to
  prove that
  \begin{align*}
    &\var{map}~\var{tl}~(\var{prepend\_col}~(h_c :: t_c)~(r :: m)) \\
    ={}&\var{map}~\var{tl}~((h_c :: r) :: \var{prepend\_col}~t_c~m) \\
    ={}&r :: \var{map}~\var{tl}~(\var{prepend\_col}~t_c~m) \\
    ={}&r :: m
  \end{align*}
  where the last line follows from the induction hypothesis.
\end{proof}

\begin{corollary}
  For all matrices $m$ and rows $r$,
  \begin{equation*}
    \var{map}~\var{tl}~(\var{map}~\var{rrot}~m) = \var{map}~\var{init}~m
  \end{equation*}
\end{corollary}
\begin{proof}
  \begin{align*}
    \var{map}~\var{tl}~(\var{map}~\var{rrot}~m) &= \var{map}~\var{tl}~(\var{prepend\_col}~(\var{map}~\var{last}~m)~(\var{map}~\var{init}~m) \\
    &= \var{map}~\var{init}~m \qedhere
  \end{align*}
\end{proof}

Now we can establish our invariant:

\begin{lemma}[radixsort\_sorted\_inv]
  For all matrices $m$ with rows of length $n$, and $j \le n$,
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{radixsort}~m~j)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $j$. The base case is true because every
  list is sorted under ordering where you examine no elements of the
  list. In the inductive case, we reason backwards from our goal:
  \begin{align*}
       &\var{PrefixSorted}~(j+1)~(\var{radixsort}~(r :: m)~(j+1)) \\
    \iff{}&\var{PrefixSorted}~(j+1)~(\var{hdsort}~(\var{map}~\var{rrot}~(\var{radixsort}~(r :: m)~j))) \\
    \iff{}&\var{PrefixSorted}~j~(\var{map}~\var{tl}~(\var{map}~\var{rrot}~(\var{radixsort}~(r :: m)~j))) \\
    \iff{}&\var{PrefixSorted}~j~(\var{map}~\var{init}~(\var{radixsort}~(r :: m)~j)) \\
    \iff{}&\var{Sorted}~(\var{map}~(\var{firstn}~j)~(\var{map}~\var{init}~(\var{radixsort}~(r :: m)~j))) \\
    \iff{}&\var{Sorted}~(\var{map}~(\var{firstn}~j)~(\var{radixsort}~(r :: m)~j)) \\
    \iff{}&\var{PrefixSorted}~j~(\var{radixsort}~(r :: m)~j)
  \end{align*}
  The last line follows from the induction hypothesis.
\end{proof}

\begin{theorem}[radixsort\_sorted]
  For all matrices $m$ with rows of length $n$,
  \begin{equation*}
      \var{Sorted}~(\var{radixsort}~m~n)
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{align*}
      &\var{Sorted}~(\var{radixsort}~m~n) \\
    \iff{}&\var{Sorted}~(\var{map}~(\var{firstn}~n)~(\var{radixsort}~m~n)) \\
    \iff{}&\var{PrefixSorted}~n~(\var{radixsort}~m~n))
  \end{align*}
  The last line follows from \var{radixsort\_sorted\_inv}.
\end{proof}

\section{Relationship to the standard BWT}
\label{sec:opt}

It is important to note that standard implementations of the BWT do
not actually reconstruct the entire rotation matrix, as this would be
inefficient. Instead, they avoid this using the optimization described
in this section. Although, we did not prove or implement this
optimization in Coq, it would be straightforward to do so.

In \var{recreate}, we repeatedly prepend the last column to the front
of the matrix and sort on it. However, each time that column is the
same, so the permutation that takes us from the unsorted column to the
sorted column is the same. Therefore, we can calculate that
permutation function once and store it in a variable $p$. The
$i$th column of the recreated matrix is simply
$(\var{apply}~p)^i~(\var{bwl}~l)$, so we can define
\begin{lstlisting}
Definition recreate' (p : list nat) (l : list A) : list (list A) :=
  transpose (iter (apply p) (length l) (apply p l))
\end{lstlisting}
In the definition above, we are repeatedly applying \var{p} to the
first column of the rotation matrix, $\var{apply}~p~l$. Then we
take the transpose to create the matrix from the columns.

Since in $\var{unbwt}~i~l$, we want the $i$th row of the matrix, we
can eliminate the transpose by replacing the $\var{nth}~i$ with a
$\var{map}~\var{nth}~i$ Assuming we already have calculated $p$,
\begin{align*}
   \var{unbwt}~i~l &= (\var{recreate}~p~l)_i \\
  &= (\var{transpose}~(\var{iter}~(\var{apply}~p)~(\var{length}~l)~(\var{apply}~p~l)))_i \\
  &= \var{map}~(\lambda
  r.~r_i)~(\var{iter}~(\var{apply}~p)~(\var{length}~l)~(\var{apply}~p~l)) \\
  &= \var{map}~(\lambda r.~r_i)~(\var{tl}~(\var{iter}~(\var{apply}~p)~(1 +
  \var{length}~l)~l)) \\
  &= \var{tl}~(\var{map}~(\lambda r.~r_i)~(\var{iter}~(\var{apply}~p)~(1 + \var{length}~l)~l))
\end{align*}

Now, instead of indexing into each column at $i$, where the columns
are all just the result of successively applying $p$ to $l$, we will
successively take the image of $i$ under $p$ to generate the indices
into $l$ where those characters appear. This is justified via the
following theorem:
\begin{theorem}
  For all $n$,
  \begin{equation*}
  \var{map}~(\lambda r.~r_i)~(\var{iter}~(\var{apply}~p)~n~l)
  =
  \var{map}~(\lambda j.~l_j)~(\var{iter}~(\var{image}~p)~n~i)
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{align*}
       & \var{map}~(\lambda r.~r_i)~(\var{iter}~(\var{apply}~p)~n~l) \\
    ={}& \var{map}~(\lambda r.~r_i)~[l,~(\var{apply}~p)~l,~(\var{apply}~p)^2~l,~\ldots,~(\var{apply}~p)^{n-1}~l] \\
    ={}& [l_i,~((\var{apply}~p)~l)_i,~((\var{apply}~p)^2~l)_i,~\ldots,~((\var{apply}~p)^{n-1}~l)_i] \\
    ={}& [l_i,~l_{p(i)},~l_{p^2(i)},~\ldots,~l_{p^{n-1}~i}] \\
    ={}& \var{map}~(\lambda j.~l_j)~[i,~p(i),~p^2(i),~\ldots,~p^{n-1}~i] \\
    ={}& \var{map}~(\lambda j.~l_j)~(\var{iter}~(\var{image}~p)~n~i) \qedhere
  \end{align*}
\end{proof}

So assuming we have some function $\var{calc\_sort\_perm} :
\var{list}~A \to \var{list}~\mathbb{N}$ to calculate the permutation
function that sorts $l$, we redefine \var{unbwt} as
\begin{lstlisting}
Definition unbwt' (i : nat) (l : list A) : list A :=
  match l with
  | [] => []
  | d :: _ =>
    let p := calc_sort_perm l in
    let indices := tl (iter (image p) (S (length l)) i) in
    map (fun j => nth l j d) indices
\end{lstlisting}
\var{calc\_sort\_perm} can be implemented in linear time using a
counting sort, so if \var{nth} took constant time, then \var{unbwt}
would take linear time as well. As a functional specification, this
closely matches the standard imperative algorithm for the inverse BWT.

\section{Conclusions}
We have implemented and proved that the the inverse BWT actually
inverts the forwards BWT, basing our work on Bird et al.'s program
calculation derivation \cite{birdmu,pearls}. In the process, we
defined the properties of stable sorts, and implemented and proved
correct an LSD radixsort.

This paper shows that Bird's style of program calculation is very
useful for building machine-checked proofs. It leads to proofs that
are composed of many smaller lemmas, each justifying a single rewrite
step. Most of the time and effort went into proving the correctness of
radixsort. If we exclude the parts related to sorting or to other
general-purpose functions not specific to the BWT, the entire verified
implementation of the BWT consists of 42 lines of Gallina definitions,
7 lines of Gallina that are used only in proofs, and 823 lines of
theorem statements and Ltac code.\footnote{This includes
  \var{BurrowsWheeler.v}, \var{Columns.v}, \var{Rotations/Rotation.v},
  \var{Rotations/Rots.v}, \var{Lib/Iterate.v}, \var{Lib/Repeat.v} and
  \var{Lib/FindIndex.v}, which implement \var{bwl}, \var{bwn},
  \var{unbwt}, \var{cols}, \var{prepend\_col}, \var{lrot}, \var{rrot},
  \var{rots}, \var{iter}, and \var{rep}, \var{findIndex}.}

This implementation, including the optimizations mentioned in
\cref{sec:opt}, can be used as a functional specification to for a
verified implementation of bzip2. Of course, it would also be
necessary to have functional specifications for the other phases of
bzip2, such as Run-Length Encoding, Move-to-Front Encoding, and
arithmetic coding. As luck would have it, Bird has already analyzed
arithmetic coding \cite{pearls}.
\clearpage

\appendix

\section{Insertion sort}
\label{appendix:insertion_sort}

Insertion sort is a simple and elegant stable sort that is easy to
reason about. The theorems proved in \cref{sec:unique} allow us to
prove things using insertion sort that apply to any stable sort. For
example, our proof of correctness for radixsort relies on the fact
that it uses insertion sort as the inner sort, but
\var{StableSort\_unique} allows us to extend that proof to a radixsort
that uses any stable sort. We also use it as the implementation of
\var{lexsort} because it is convenient, but similarly
\var{Sort\_Ord\_unique} ensures that it can be replaced with any other
sort.

\lstinputlisting[
  widthgobble=2, firstline=15, lastline=26,
]{../theories/BWT/Sorting/InsertionSort.v}

\begin{lemma}[insert\_perm]
  For all lists $l$ and elements $x$,
  \begin{equation*}
    \var{Permutation}~(x::l)~(\var{insert}~x~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $l$. In the inductive case, we need to show
  \begin{equation*}
    \var{Permutation}~(x::h::t)~(\var{insert}~x~(h::t))
  \end{equation*}
  given the induction hypothesis that
  \begin{equation*}
    \var{Permutation}~(x::t)~(\var{insert}~x~t)
  \end{equation*}
  If $x \le h$, then $\var{insert}~x~(h::t) = x :: h :: t$ and we are
  done. Otherwise, $\var{insert}~x~(h::t) = h :: \var{insert}~x~t$ and
  we can apply the induction hypothesis.
\end{proof}

\begin{theorem}[sort\_perm]
  For all lists $l$,
  \begin{equation*}
    \var{Permutation~l}~(\var{sort}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof follows by induction on $l$. The inductive case is
  \begin{equation*}
    h :: t \eqperm h :: \var{sort}~t \eqperm
    \var{insert}~h~(\var{sort}~t) = \var{sort}~(h :: t)
  \end{equation*}
  where the first step follows from the induction hypothesis, and the
  second from \var{insert\_perm}.
\end{proof}

\begin{lemma}[insert\_sorted]
  For all lists $l$ and elements $x$,
  \begin{equation*}
    \var{Sorted}~l \implies \var{Sorted}~(\var{insert}~x~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on the proof that $\var{Sorted}~l$. In the
  inductive case we need prove that
  \begin{equation*}
    \var{Sorted}~(\var{insert}~x~(h::t))
  \end{equation*}
  If $x \le h$, then $\var{insert}~x~(h::t) = x :: h :: t$, and by
  \var{SortedLocal\_cons} we are done.

  If $x > h$, $\var{insert}~x~(h::t) = h :: \var{insert}~x~t$, and it
  suffices to show that $\forall y \in \var{insert}~x~t,~h \le y$. By
  \var{insert\_perm}, this is equivalent to $\forall y \in x::t,~h \le y$ From
  the the fact that $\var{Sorted}~(h::t)$, we know that $\forall y \in t,~h \le
  y$, and we have that $h < x$, so we are done.
\end{proof}

\begin{theorem}[sort\_sorted]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~(\var{sort}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof follows by induction on $l$ and \var{insert\_sorted}.
\end{proof}

\begin{lemma}[insert\_stable]
  For all lists $l$ and elements $x$,
  \begin{equation*}
    \var{StablePerm}~(x::l)~(\var{insert}~x~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $l$. In the inductive case, we need to show
  \begin{equation*}
    \var{StablePerm}~(x::h::t)~(\var{insert}~x~(h::t))
  \end{equation*}
  given the induction hypothesis that
  \begin{equation*}
    \var{StablePerm}~(x::t)~(\var{insert}~x~t)
  \end{equation*}
  If $x \le h$, then $\var{insert}~x~(h::t) = x :: h :: t$ and we are
  done. Otherwise, $x > h$ and $\var{insert}~x~(h::t) = h ::
  \var{insert}~x~t$.
  \begin{equation*}
    x :: h :: t \eqstable h :: x :: t \eqstable h :: \var{insert}~x~t
  \end{equation*}
  where the first step follows from the fact that $x \not\equiv h$ and the
  second from the induction hypothesis.
\end{proof}

\begin{theorem}[sort\_stable]
  For all lists $l$,
  \begin{equation*}
    \var{StablePerm}~l~(\var{sort}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof is almost identical to the proof for \var{sort\_perm}.
\end{proof}

\begin{theorem}[sort\_StableSort]
  \begin{equation*}
    \var{StableSort}~\var{sort}
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows from \var{sort\_sorted} and \var{sort\_stable}.
\end{proof}

\section{\var{hdsorted\_sorted\_S}}
\label{appendix:hdsort_sorted_S}

In this section, we prove the following theorem:
\begin{theorem}[hdsort\_sorted\_S]
  For all matrices $m$ and naturals $j$,
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{map}~\var{tl}~m) \implies
    \var{PrefixSorted}~(j+1)~(\var{hdsort}~m)
  \end{equation*}
\end{theorem}

First we prove this lemma:
\begin{lemma}[insert\_Sorted\_S]
  For all matrices $m$, rows $r$, and naturals $j$,
  \begin{gather*}
    \var{PrefixSorted}~(j+1)~(\var{hdsort}~m) \implies \\
    \var{PrefixSorted}~j~(\var{tl}~r :: \var{map}~{tl}~m) \implies \\
    \var{PrefixSorted}~(j+1)~(\var{insert}_1~r~(\var{hdsort}~m))
  \end{gather*}
  where $\var{insert}_1$ is \var{insert} that uses the 1-prefix
  lexicographic ordering used by \var{hdsort}.
\end{lemma}
\begin{proof}
  We can destruct $\var{insert}_1~r~(\var{hdsort}~m)$ as $m_1 \dplus r
  :: m_2$, where $m_1 \dplus m_2 = \var{hdsort}~m$. Then by
  \var{Sorted\_app\_cons}, it suffices to show
  \begin{gather*}
    \var{PrefixSorted}~(j+1)~m_1 \land \\
    \var{PrefixSorted}~(j+1)~(r :: m_2) \land \\
    (\forall x \in m_1,~ r >_{j+1} x)
  \end{gather*}
  \begin{itemize}
  \item $\var{PrefixSorted}~(j+1)~m_1$ follows from the fact that
    $\var{hdsort}~m = m_1 \dplus m_2$ and
    $\var{PrefixSorted}~(j+1)~(\var{hdsort}~m)$.
  \item By similar reasoning, we know that
    $\var{PrefixSorted}~(j+1)~m_2$, so in order to show
    $\var{PrefixSorted}~(j+1)~(r :: m_2)$ all we need to prove is that
    for every row $x$ in $m_2$,
    \begin{equation*}
      r \le_{j+1} x
    \end{equation*}
    We know from the fact that $\var{insert}_1~r~(\var{hdsort}~m) =
    m_1 \dplus r :: m_2$ that $r \le_1 x$. Then by \var{key\_firstn\_S},
    we only need to show that
    \begin{equation*}
      \var{tl}~r \le_j \var{tl}~x
    \end{equation*}
    This follows from $\var{PrefixSorted}~j~(\var{tl}~r :: \var{map}~{tl}~m)$.
  \item We have that for any $x \in m_1$, $r >_1 x$ by the fact that
    $\var{insert}_1~r~(\var{hdsort}~m) = m_1 \dplus r :: m_2$. This
    gives us $r >_{j+1} x$ by \var{key\_lt\_firstn\_ge}. \qedhere
  \end{itemize}
\end{proof}

\section{Permutation composition}
\label{appendix:perm_comp}

In this section, we define \var{compose} to compose permutations,
which we will denote using $\comp$ in proofs. We prove the following
property:

\begin{theorem}[compose\_apply]
  For all lists $l$ and permutation functions $p_1$ and $p_2$
  \begin{equation*}
      \var{apply}~(p_2 \comp p_1)~l = \var{apply}~p_2~(\var{apply}~p_1~l)
  \end{equation*}
\end{theorem}

Because we represent the permutations as lists of outputs, we can
simply apply one permutation to the other permutation to compose them.
\lstinputlisting[
  widthgobble=2, firstline=409, lastline=409
]{../theories/BWT/Sorting/PermFun.v}
This definition, while simple, does not make it obvious that
\var{compose\_apply} holds.
\begin{equation}
  \label{eq:compose_apply}
  \var{apply}~(\var{apply}~p_2~p_1)~l = \var{apply}~p_2~(\var{apply}~p_1~l)
\end{equation}

Conceptually, the theorem works because when we permute the list $l$,
we can think of it as permuting the original indices $\iota =
[0,~1,~\ldots,~n-1]$. If we rephrase \Cref{eq:compose_apply} the effect on
on $I$, we get
\begin{equation*}
  \var{apply}~(\var{apply}~p_2~p_1)~\iota = \var{apply}~p_2~(\var{apply}~p_1~\iota)
\end{equation*}
This equation clearly holds if we can prove that $\iota$ is the identity
of \var{apply}, giving as an outline of a proof.

We will use the function \var{combine} defined in the Coq standard
library to annotate every element of $l$ with its index.
\begin{lstlisting}
  Fixpoint combine (l : list A) (l' : list B) : list (A*B) :=
    match l,l' with
    | x :: tl, y :: tl' => (x,y) :: (combine tl tl')
    | _, _ => nil
    end.
\end{lstlisting}

To formalize the idea that we can reason about the indices of the
original list $l$ in place of $l$, we need the following theorem.
Intuitively, it says that if we have two lists of pairs, if we know
that the $x$-coordinates are equal and we know that the list of pairs
are permutations of one another, then we know the $y$-coordinates must
be equal as well, because the permutation condition guarantees that
each $x$-coordinate is paired with the same $y$-coordinate in both
lists.
\begin{theorem}[Permutation\_combine\_eq]
  For all lists $xs$, $ys_1$, and $ys_2$ with all the same length,
  if $xs$ has no duplicates then
  \begin{equation*}
    \var{Permutation}~(\var{combine}~xs~ys_1)~(\var{combine}~xs~ys_2) \implies ys_1 = ys_2
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the list $\var{combine}~xs~ys_1$. In the
  inductive case, we have to show that
  \begin{equation*}
    y_1 :: ys_1 = y_2 :: ys_2
  \end{equation*}
  given that
  \begin{equation*}
    (x,~y_1) :: \var{combine}~xs~ys_1 \eqperm (x, y_2) :: \var{combine}~xs~ys_2
  \end{equation*}
  This means that $(x, y_1) \in (x, y_2) :: \var{combine}~xs~ys_2$, so
  either $(x, y_1) = (x, y_2)$, in which case we are done, or $(x,
  y_1) \in \var{combine}~xs~ys_2$. In the latter case, we have a
  contradiction, because we know that $x :: xs$ contains no
  duplicates, so $x$ cannot be in $xs$.
\end{proof}

We also need to show that \var{apply} distributes over \var{combine}:
\begin{theorem}[apply\_combine]
  For all lists $xs$ and $ys$ with equal length,
  \begin{equation*}
    \var{apply}~p~(\var{combine}~xs~ys) =
    \var{combine}~(\var{apply}~p~xs)~(\var{apply}~p~ys)
  \end{equation*}
\end{theorem}
\begin{proof}
  Let $n = \var{length}~xs = \var{length}~ys$. We reason by induction
  on n. In the inductive case, we have to show
  \begin{align*}
       & \var{apply}~(i::p)~(\var{combine}~(x::xs)~(y::ys)) \\
    ={}& \var{combine}~(\var{apply}~(i::p)~(x::xs))~(\var{apply}~(i::p)~(y::ys))
  \end{align*}
  We can use \var{rem\_PermFun\_correct} to simplify both sides of the
  equation. We have that the heads of both lists must are equal:
  \begin{equation*}
    (\var{combine}~(x::xs)~(y::ys))_i = ((x::xs)_i, (y::ys)_i)
  \end{equation*}
  That the tails are equal follows from the induction hypothesis, so
  we are done.
\end{proof}

Now we show that $\iota$ is the identity of $\var{compose}$/$\var{apply}$.
\begin{theorem}[compose\_id\_l]
  For all permutation functions $p$ on lists of length $n$,
  \begin{equation*}
    \var{compose}~(\var{seq}~0~n)~p = p
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof follows from induction on $p$. In the inductive case,
  \begin{align*}
       \var{compose}~(\var{seq}~0~(n+1))~(i::p)
    &= \var{map}~(\lambda j.~(i::p)_j)~(\var{seq}~0~(n+1)) \\
    &= \var{map}~(\lambda j.~(i::p)_j)~(0 :: \var{seq}~1~(n+1)) \\
    &= i :: \var{map}~(\lambda j.~(i::p)_j)~(\var{seq}~1~(n+1)) \\
    &= i :: \var{map}~(\lambda j.~(i::p)_j)~(\var{map}~(\lambda j.~j+1)~(\var{seq}~0~n)) \\
    &= i :: \var{map}~(\lambda j.~(i::p)_{j+1})~(\var{seq}~0~n) \\
    &= i :: \var{map}~(\lambda j.~p_j)~(\var{seq}~0~n) \\
    &= i :: \var{compose}~(\var{seq}~0~n)~p \\
    &= i :: p \qedhere
  \end{align*}
\end{proof}

\begin{theorem}[compose\_id\_r]
  For all permutation functions $p$ on lists of length $n$,
  \begin{equation*}
    \var{compose}~p~(\var{seq}~0~n) = p
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{equation*}
       \var{compose}~p~(\var{seq}~0~n)
    = \var{map}~(\lambda j.~(\var{seq}~0~n)_j)~p
    = \var{map}~(\lambda j.~j)~p
    = p \qedhere
  \end{equation*}
\end{proof}

Now we can prove the main theorem:
\begin{theorem}[compose\_apply]
  For all lists $l$ and permutation functions $p_1$ and $p_2$
  \begin{equation*}
      \var{apply}~(p_2 \comp p_1)~l = \var{apply}~p_2~(\var{apply}~p_1~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  Letting $\iota = \var{seq}~0~(\var{length}~l)$, we annotate $l$ with
  indices. By \var{apply\_correct}, we have
  \begin{equation*}
     \var{combine}~\iota~l \eqperm \var{apply}~(p_2 \comp p_1)~(\var{combine}~\iota~l)
  \end{equation*}
  and
  \begin{equation*}
    \var{combine}~\iota~l
    \eqperm
    \var{apply}~p_1~(\var{combine}~\iota~l)
    \eqperm
    \var{apply}~p_2~(\var{apply}~p_1~(\var{combine}~\iota~l))
  \end{equation*}
  We combine these by transitivity, and then simplify, using \var{app}
  as an abbreviation for \var{apply}.
  \begin{align*}
    \var{pap}~(p_2 \comp p_1)~(\var{combine}~\iota~l)
    &\eqperm
    \var{app}~p_2~(\var{app}~p_1~(\var{combine}~\iota~l)) \\
    \var{combine}~(\var{app}~(p_2 \comp p_1)~\iota)~(\var{app}~(p_2 \comp p_1)~l)
    &\eqperm
    \var{app}~p_2~(\var{combine}~(\var{app}~p_1~\iota)~(\var{app}~p_1~l)) \\
    \var{combine}~(p_2 \comp p_1)~(\var{app}~(p_2 \comp p_1)~l)
    &\eqperm
    \var{app}~p_2~(\var{combine}~p_1~(\var{app}~p_1~l)) \\
    \var{combine}~(p_2 \comp p_1)~(\var{app}~(p_2 \comp p_1)~l)
    &\eqperm
    \var{combine}~(\var{app}~p_2~p_1)~(\var{app}~p_2~(\var{app}~p_1~l))
  \end{align*}
  Because $p_2 \comp p_1 = \var{app}~p_2~p_1$, we can apply
  \var{Permutation\_combine\_eq}.
\end{proof}

\nocite{*}
\printbibliography{}

\end{document}
