\documentclass[sigplan,10pt,anonymous,review]{thesis}

\acmConference
    [CPP'20]
    {ACM SIGPLAN International Conference on Certified Programs and Proofs}
    {January 20--21, 2020}{New Orleans, LA, USA}
\acmYear{2020}
\acmISBN{} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM} \acmDOI{} %
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
\citestyle{acmauthoryear}  %% For author/year citations
%\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].

\begin{document}

\title[Coq Formalization of Stable Sorts, Radix Sort, and BWT]{A Coq
  Formalization of Stable Sorts, Radix Sort, and the Burrows-Wheeler
  Transform}

\author{Jake Waksbaum}
\affiliation{
  \department{Department of Computer Science}
  \institution{Princeton University}
  \city{Princeton}
  \state{New Jersey}
  \postcode{08540}
  \country{USA}
}
\email{waksbaum@princeton.edu}

\author{Andrew Appel}
\affiliation{
  \department{Department of Computer Science}
  \institution{Princeton University}
  \city{Princeton}
  \state{New Jersey}
  \postcode{08540}
  \country{USA}
}
\email{appel@princeton.edu}

\begin{abstract}
Stable sorts are an important and fundamental type of sorting
algorithm. Some algorithms, such as radix sorts, rely upon the
stability of a sorting subroutine for their correctness. In other
cases, stable sorts are useful for reasoning about algorithms, often
because any two stable sorts are interchangeable. One such algorithm
is the Burrows--Wheeler transform (BWT), which has applications in
compression, full-text indexing and pattern searching.

There has not been much work on formally verifying stable sorts, or
rather, the stability of stable sorts. While Bird
et~al.\cite{birdmu,pearls} provide an elegant derivation and proof for
the BWT, formalizing it in Coq reveals that it relies on a few lemmas
that are not trivial to prove, including the correctness of
least-significant digit (LSD) radix sort. In this paper, we develop
more fully the theory of stable sorts, and use this theory to prove
the correctness of LSD radix sort and thereby the BWT.
\end{abstract}

%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML} <ccs2012>
<concept>
<concept_id>10003752.10003809.10010031.10002975</concept_id>
<concept_desc>Theory of computation~Data compression</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003809.10010031.10010033</concept_id>
<concept_desc>Theory of computation~Sorting and searching</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10010940.10010992.10010998</concept_id>
<concept_desc>Software and its engineering~Formal methods</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Data compression}
\ccsdesc[500]{Theory of computation~Sorting and searching}
\ccsdesc[500]{Software and its engineering~Formal methods}
%% End of generated code

\keywords{Formal methods, Sorting and searching, Data compression}

\maketitle

\section{Introduction}
\label{sec:intro}

Stable sorts are an important and fundamental type of sorting
algorithm. A sort is stable if it preserves the relative order of
equal elements. For example, if we are stably sorting the list
$[\var{banana},\ \allowbreak\var{armpit},\ \allowbreak\var{carrot},\ \allowbreak\var{apple}]$,
we have to produce
$[\var{armpit},\ \allowbreak\var{apple},\ \allowbreak\var{banana},\ \allowbreak\var{carrot}]$
and not
$[\var{apple},\ \allowbreak\var{armpit},\ \allowbreak\var{banana},\ \allowbreak\var{carrot}]$.
Even though both are sorted on the first character, because
\var{armpit} preceded \var{apple} in the original lists, it must also
precede \var{apple} in the stably sorted list.

Stability can be directly useful in certain contexts, such as when
sorting a table by a single column. However, stability is also crucial
to the correctness of certain general-purpose algorithms. One such
algorithm, or set of algorithms, is radix sort. Radix sorts are used
to sort data lexicographically, and can be more efficient than
comparison-based sorting methods, both in theory and in practice.
Radix sorts are especially well-suited to sorting strings
\cite{McIlroy93}. Many radix sorts rely on a some stable sorting
subroutine.

Stable sorts are also important for reasoning about sorting algorithms
more broadly. This is because any two stable sorting algorithms are
equivalent, in the sense that they will produce the same outputs for a
given input. Crucially, this allows us to reason using simple
algorithms and replace them with more efficient algorithms later on.
We make use of this technique in order to reason about LSD radix sort,
as well as the Burrows-Wheeler transform (BWT).

The Burrows--Wheeler transform is an invertible transformation that
makes a string more amenable to compression by other methods
\cite{bw}. Notably, the BWT is the core pass in the popular,
open-source bzip2 compression software \cite{tsai_2016}. In addition,
the BWT has been applied to full-text indexing and pattern searching,
for example in genomics \cite{ferragina_index, dna}.

There has not been much work on formally verifying stable sorts, or
rather, the stability of stable sorts. Some work has been done on
verifying the stability of mergesort
\cite{leroy_2018,Sternagel2013,Leino2015,deGouw2014}, but we do not
know of any formally verified implementation of a radix sort that
includes stability.

In addition, to our knowledge no one has produced a formally verified
implementation of the BWT, by which we mean an implementation with a
proof that the inverse-transform inverts the forwards-transform. Bird
et~al.\cite{alg-of-prog,birdmu} provide an excellent derivation of the
BWT and its inverse that we use as the basis of our implementation and
proof. However, it leaves a few crucial lemmas unproven, including the
correctness of LSD radix sort.

In this paper, we develop more fully the theory of stable sorts: we
give multiple definitions for permutations and stable permutations and
prove them equivalent. This allows us to choose whichever definition
makes the proof easiest when we are reasoning about stability. We also
proved that any two sorts are interchangeable.
\begin{restatable*}[StableSort\_unique]{theorem}{stablesortunique}
  \label{thm:stablesort_unique}
  For all functions $f, g: \var{list}~A \to \var{list}~A$,
  \begin{equation*}
    \var{StableSort}~f \implies \var{StableSort}~g \implies
    (\forall l,f~l = g~l)
  \end{equation*}
\end{restatable*}

Using \cref{thm:stablesort_unique}, we prove the correctness of an LSD
radix sort that relies on insertion sort to sort each column. This
choice was crucial to the proof: parametrizing radix sort over any
stable sort made it difficult to reason about, but reasoning about
insertion sort using induction is very elegant.
\cref{thm:stablesort_unique} allows us to recover the generality of
parametrizing over all stable sorts, because insertion sort is
equivalent to any other stable sort.

Finally, we use the correctness of radix sort to reason about the BWT.
Specifically, we implement the functions $\var{bwl} : \var{list}~A \to
\var{list}~A$ and $\var{bwn} : \var{list}~A \to \mathbb{N}$ that produce
the string and index part of the BWT respectively, and the function
$\var{unbwt} : \mathbb{N} \to \var{list}~A \to \var{list}~A$ that reverses
the transform. We then prove the following theorem:
\begin{restatable*}[unbwt\_correct]{theorem}{unbwtcorrect}
  \label{thm:unbwt_correct}
  For all lists $l$,
  \begin{equation*}
    \var{unbwt}~(\var{bwn}~l)~(\var{bwl}~l) = l
  \end{equation*}
\end{restatable*}

The implementation is useful in its own right as a functional
specification for a potential verified implementation of a program
that relies on Burrows--Wheeler, such as bzip2. But more generally,
this provides evidence that Bird's approach of program calculation, in
which programs are derived from their specifications via algebraic
manipulations, is useful for proving the correctness of algorithms in
a machine-checked setting. Bird has applied this approach to greedy
algorithms, the Boyer-Moore algorithm, the Knuth-Morris-Pratt
algorithm, Sudoku solvers, arithmetic coding, the Schorr-Waite
algorithm, the Johnson-Trotter algorithm, and many more problems
\cite{pearls}.

Every definition and theorem in this paper corresponds directly to a
definition or theorem in our Coq sources, which we provide as
supplementary materials. Definitions are copied verbatim, modulo
whitespace. Theorems are rephrased for clarity, with the name of the
corresponding Coq theorem given in parentheses. The only exceptions
are \cref{sec:opt} and places where we give hypothetical definitions
to drive the explanation.

\section{Main ideas}
\label{sec:main_ideas}

\subsection{Stable sorting}
\label{subsec:stable_sorting}

Whenver we sort a list, we do so with respect to some ordering on its
elements. For example, we could sort a list of students by first name,
last name, birthday, etc. More precisely, for sorting we need a total,
decideable preorder: a \textit{preorder} is a relation $\le$ that is
reflexive and transitive, it is \textit{total} if we can compare any
two elements, and it is \textit{decidable} if we have a procedure that
takes any two elements $x$ and $y$ and tells us if $x \le y$ or $x \ge y$.

Given a total decideable preorder, we can specify what contract a
function~$f$ must fulfill to be considered a sorting function.
Intuitively, it must produce a list that is sorted, and that contains
all of the same elements as the original list.
\begin{definition}[Sort]
  A function $f : \var{list}~A \to \var{list}~A$ is a \var{Sort} when
  for all inputs $l$,
  \begin{equation*}
    \var{Sorted}~(f~l) \land \var{Permutation}~(f~l)~l
  \end{equation*}
\end{definition}

Now we have to define \var{Sorted} and \var{Permutation}
appropriately. For both properties, and for other properties we will
discuss later, there are multiple reasonable definitions. We will
state all the definitions we can think of, and prove equivalence
between them. This increases our confidence that we have defined
things correctly, and allows us to choose whichever definition we find
most convenient for a given proof.

We can define \var{Sorted} in two main ways: %TODO cite coq book and knuth
\begin{enumerate}
\item A list $l$ is sorted when for all
  indices $i$ and $j$,
\begin{equation*}
  i \le j \implies l_i \le l_j
\end{equation*}
\item We can inductively define when a list $l$ is sorted, by saying
  that the empty list is sorted, and that given a sorted lists $t$, we
  may conclude that $h :: t$ is also sorted if $h$ is less~than or
  equal~to every element in $t$.
\end{enumerate}

Similarly, we can define \var{Permutation} in three main ways:
\begin{enumerate}
\item Two lists $l$ and $l'$ are permutations of one another if every
  $x$ occurs an equal number of times in $l$ and $l'$.
\item Two lists $l$ and $l'$ with lengths $n$ are permutations of one
  another when there exists a permutation function~$p$ that maps the
  indices of $l$ to indices of $l'$, so that for every index~$i$
  \begin{equation*}
      l_i = l'_{p(i)}
  \end{equation*}
  $p$ must be a bijection so that every index of $l$ is mapped to by
  exactly one index of $l'$.
\item We can define inductively when two lists are permutations of one another:
  \begin{itemize}
  \item As a base case, two empty lists are permutations of one
    another.
  \item If two lists $l$ and $l'$ are permutations of one another,
    then we can conclude that $x :: l$ and $x :: l'$ are permutations
    of one another.
  \item As a second base case, we can conclude that for any $x$, $y$,
    and $l$, that $x :: y :: l$ is a permutation of $y :: x :: l$.
  \item If $l_1$ is a permutation of $l_2$ and $l_2$ is a permutation
    of $l_3$, then $l_1$ is a permutation of $l_3$.
  \end{itemize}
\end{enumerate}

Now that we've defined what a sort is, we can start to consider stable
sorts. Stable sorts must preserve the relative orders of
``equivalent'' elements. This equivalence comes from the ordering we
are using: any total decidable preorder also give us a decideable
equivalence relation $\equiv$, where $x \equiv y$ when $x \le y$ and $x \ge y$.
\begin{definition}[StableSort]
  A function $f : \var{list}~A \to \var{list}~A$ is a \var{StableSort} when
  for all inputs $l$,
  \begin{equation*}
    \var{Sorted}~(f~l) \land \var{StablePerm}~(f~l)~l
  \end{equation*}
\end{definition}

Stable sorts still must produce sorted outputs, but the output cannot
be just any permutation of the input. So we need a stricter notion of
a permutation that respects equivalence, which we will call a stable
permutation.

To define stable permutations, we can tweak the each
definitions of permutation mentioned above:
\begin{enumerate}
\item Two lists $l$ and $l'$ are stable permutations of one another if
  for every $x$,
  \begin{equation*}
    \var{filter}~(\lambda y. y \equiv x)~l = \var{filter}~(\lambda y. y \equiv x)~l'
  \end{equation*}
  This is a stricter version of the first definition of permutation
  above, because that can be rephrased as $\var{filter}~(\lambda y. y = x)~l
  = \var{filter}~(\lambda y. y = x)~l'$.
\item For two lists $l$ and $l'$ of length $n$ to be stable
  permutations of one another, there must exists a \textit{stable}
  permutation function~$p$ that relates them. That means that $p$
  satisfies an additional stability property: for all indices $i < j$
  where $l_{i} \equiv l_{j}$, we must have $p(i) < p(j)$.
\item We can modify the third inference rule, which allows swapping
  $x$ and $y$, so that you can only apply it when $x \nequiv y$: the new
  rule says that for any $x$, $y$, and $l$ such that $x \nequiv y$ , you can
  conclude that $x :: y :: l$ is a permutation of $y :: x :: l$.
\end{enumerate}

From these definitions, we can see that stable sorts are restricted in
which permutation they produce. In fact, there is only one stable
permutation that is also sorted, so any two stable sorts must produce
the same results for every input, and are thus interchangeable.
\stablesortunique

This can be generalized a little further: even if a sorting algorithm
itself is not a stable sorting algorithm, if the ordering we are using
doesn't allow us to distinguish between equivalent elements, then we
will not be able to tell if equivalent elements are being swapped.
More precisely, notice that it isn't always true that $x \equiv y$ implies
$x = y$. For example, if we are sorting strings by their first
character, then $\text{\texttt{apple}} \equiv \text{\texttt{armpit}}$ under
this ordering, but clearly $\text{\texttt{apple}} \neq
\text{\texttt{armpit}}$. When we do have that the equivalence ($\equiv$)
implies Leibniz equality ($=$), we say that we have an \textit{order},
as we have when we are sorting strings using all of their characters.
Under an order, as opposed to a preorder, we can prove that every
permutation is a stable permutation and every sort is a stable sort,
which implies that every sort is equivalent to every other sort.
\begin{theorem}[all\_perm\_stable]
  If we have an equivalence that implies Leibniz equality, then for
  all lists $l$ and $l'$
  \begin{equation*}
      \var{Permutation}~l~l' \implies \var{StablePerm}~l~l'
  \end{equation*}
\end{theorem}
\begin{corollary}[Sort\_Ord\_unique]
  \label{thm:sort_ord_unique}
  When sorting using an order, given any two function $f, g:
  \var{list}~A \to \var{list}~A$,
  \begin{equation*}
    \var{Sort}~f \implies \var{Sort}~g \implies
    (\forall l, f~l = g~l)
  \end{equation*}
\end{corollary}

\subsection{LSD radix sort}
\label{sec:radix_sort}

LSD radix sort is a simple radix sort that is used to
lexicographically sort a list of strings of equal length. LSD radix
sort operates by stably sorting on each column, from the end of the
string to the front (\cref{fig:lsd_radixsort}). The algorithm works
because lexicographic order says that if two strings are equal up to a
column, then the tie is broken according to the rest of the string.
When we stably sort on a column, ties are broken in favor or the
existing order, which was established by sorted on the following
columns.

\begin{figure}[!hb]
  \centering
  \begin{tt}
    \begin{tabular}{c}
      d a \textbf{b} \\
      a d \textbf{d} \\
      c a \textbf{b} \\
      f a \textbf{d} \\
      f e \textbf{e} \\
      b a \textbf{d} \\
      d a \textbf{d} \\
      b e \textbf{e} \\
      f e \textbf{d} \\
      b e \textbf{d} \\
      e b \textbf{b} \\
      a c \textbf{e}
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c}
      d \textbf{a} b \\
      c \textbf{a} b \\
      e \textbf{b} b \\
      a \textbf{d} d \\
      f \textbf{a} d \\
      b \textbf{a} d \\
      d \textbf{a} d \\
      f \textbf{e} d \\
      b \textbf{e} d \\
      f \textbf{e} e \\
      b \textbf{e} e \\
      a \textbf{c} e
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c}
      \textbf{d} a b \\
      \textbf{c} a b \\
      \textbf{f} a d \\
      \textbf{b} a d \\
      \textbf{d} a d \\
      \textbf{e} b b \\
      \textbf{a} c e \\
      \textbf{a} d d \\
      \textbf{f} e d \\
      \textbf{b} e d \\
      \textbf{f} e e \\
      \textbf{b} e e
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c}
      a c e \\
      a d d \\
      b a d \\
      b e d \\
      b e e \\
      c a b \\
      d a b \\
      d a d \\
      e b b \\
      f a d \\
      f e d \\
      f e e
    \end{tabular}
  \end{tt}
  \caption{LSD Radixsort}
  \label{fig:lsd_radixsort}
\end{figure}

\subsection{The Burrows-Wheeler transform}
\label{subsec:bwt}

The Burrows--Wheeler transform was originally introduced as a pass in a
compression algorithm \cite{bw}, and continues to be used as a part of
the popular, open-source bzip2 compression software \cite{tsai_2016}.
Here we describe a standard compression algorithm built around BWT.
\begin{enumerate}
\item Apply the BWT, which tends to group create runs of identical characters.
\item Apply Move-to-Front (MTF) encoding, in which a character is
  represented by the number of different characters encountered since
  the last occurrence of this character. This creates many runs of
  zeroes from the runs in the output of BWT. On English texts, often
  50-60\% of the values produced by the MTF pass are zeroes
  \cite{fenwick2007}.
\item Apply Run-Length Encoding (RLE) to encode only the runs of
  zeroes with their lengths, using a bijective base-2 encoding
  \cite{bw-analysis, tsai_2016}.
\item Compress with some zeroth-order coder like a Huffman coder.
\end{enumerate}

The BWT is also at the heart of algorithms that exploit the
relationship between the rotation matrix in the BWT and the suffix
array data structure in order to search for patterns in a compressed
text \cite{ferragina_index}. These techniques have been applied to
enable fast matching of DNA subsequences \cite{dna}.

To apply the BWT to a string of length $n$:
\begin{enumerate}
\item Form the \(n \times n\) matrix of all cyclic shifts of the
  original string (referred to as the rotation matrix). Starting
  with the original string, each row is formed from the previous one
  by removing the first character and sticking it on the end.
\item Sort the rows lexicographically
\item Take the last column of the matrix. That string is the result of
  the transformation.
\item Note the index where the original string appears in the sorted
  rotation matrix. This piece of information is necessary to reverse
  the transformation.
\end{enumerate}
As shown in \cref{fig:bw_ex}, applying the algorithm to the string
\var{abracadabra!} produces the new string \var{ard!rcaaabb} and the
index \var{3}.

\begin{figure}[!hb]
  \centering
  \begin{tt}
  \begin{tabular}{rc}
    0  & abracadabra! \\
    1  & bracadabra!a \\
    2  & racadabra!ab \\
    3  & acadabra!abr \\
    4  & cadabra!abra \\
    5  & adabra!abrac \\
    6  & dabra!abraca \\
    7  & abra!abracad \\
    8  & bra!abracada \\
    9  & ra!abracadab \\
    10 & a!abracadabr \\
    11 & !abracadabra
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{rc}
    0  & !abracadabr\textbf{a} \\
    1  & a!abracadab\textbf{r} \\
    2  & abra!abraca\textbf{d} \\
    \textit{3} & \textit{abracadabra\textbf{!}} \\
    4  & acadabra!ab\textbf{r} \\
    5  & adabra!abra\textbf{c} \\
    6  & bra!abracad\textbf{a} \\
    7  & bracadabra!\textbf{a} \\
    8  & cadabra!abr\textbf{a} \\
    9  & dabra!abrac\textbf{a} \\
    10 & ra!abracada\textbf{b} \\
    11 & racadabra!a\textbf{b}
  \end{tabular}
  \end{tt}
  \caption{BWT acting on \var{abracadabra!}}
  \label{fig:bw_ex}
\end{figure}

The BWT tends to produce runs like the three \var{a}s or the two
\var{b}s in \var{ard!rcaaabb}, and it is this feature that makes the
resulting string easier to compress. It creates those runs by
exploiting the correlation between adjacent characters, which means
that by knowing one character, one can predict the character that
follows it with some success. Sorting the rotation matrix
lexicographically brings together rows that start with the same
characters. But, the beginning and end of each row were adjacent in
the original string, so this will also tend to bring together rows
that end with the same characters.

\subsection{Inverting the BWT}
\label{subsec:bwt}

Although it is not obvious, the transformed string and the index
provide enough information to recover the the original string.
Conceptually, we can find the original string by using the transformed
string to reconstruct the entire sorted rotation matrix, and then
using the index to find the row containing the original string. This,
though, relies on our ability to reconstruct the sorted rotation
matrix from just its last column.

A key insight is that because of \cref{thm:sort_ord_unique}, we can
pretend that during the forwards transform, the rotation matrix is
being sorted using any sorting algorithm we find convenient.
Specifically, if we examine how an LSD radixsort sorts the rotation
matrix (\cref{fig:radixsort_rots}) we notice an interesting pattern.
Each column of the rotation matrix is a permutation of the original
string, so when we sort a on particular column, we always produce the
same result. As we move right-to-left, the column containing the
sorted input string moves with us until it reaches the left edge of
the matrix.

\begin{figure}[!hb]
  \centering
  \begin{tt}
  \setlength{\tabcolsep}{0pt}
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    abracadabra&!& \\
    bracadabra!&a& \\
    cadabra!abr&a& \\
    dabra!abrac&a& \\
    bra!abracad&a& \\
    !abracadabr&a& \\
    racadabra!a&b& \\
    ra!abracada&b& \\
    adabra!abra&c& \\
    abra!abraca&d& \\
    acadabra!ab&r& \\
    a!abracadab&r&
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    racadabra&!&ab \\
    bracadabr&a&!a \\
    acadabra!&a&br \\
    a!abracad&a&br \\
    dabra!abr&a&ca \\
    bra!abrac&a&da \\
    cadabra!a&b&ra \\
    !abracada&b&ra \\
    abra!abra&c&ad \\
    ra!abraca&d&ab \\
    abracadab&r&a! \\
    adabra!ab&r&ac
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    a&!&abracadabr \\
    r&a&!abracadab \\
    d&a&bra!abraca \\
    !&a&bracadabra \\
    r&a&cadabra!ab \\
    c&a&dabra!abra \\
    a&b&ra!abracad \\
    a&b&racadabra! \\
    a&c&adabra!abr \\
    a&d&abra!abrac \\
    b&r&a!abracada \\
    b&r&acadabra!a
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &!&abracadabra \\
    &a&!abracadabr \\
    &a&bra!abracad \\
    &a&bracadabra! \\
    &a&cadabra!abr \\
    &a&dabra!abrac \\
    &b&ra!abracada \\
    &b&racadabra!a \\
    &c&adabra!abra \\
    &d&abra!abraca \\
    &r&a!abracadab \\
    &r&acadabra!ab
  \end{tabular}
  \end{tt}
  \caption{Sorting the rotation matrix with LSD radixsort. The
    highlighted column contains the sorted original string.}
  \label{fig:radixsort_rots}
\end{figure}

We also can notice that the columns to the right of the sorted column
move with it for similar reasons. The column directly to its right is
the result of sorting the input string, and then stably sorting it
again by the character that precedes it in the original string. The
next column is the result of sorting by each character, sorting by the
preceding character, and then sorting by the character 2 spaces in
front.

This suggests that we can recreate the matrix from the last column by
sorting it to produce the first column, then repeatedly prepending the
last column to the growing matrix and stably sorting by it
(\cref{fig:recreate}. As the matrix grows from right to left, we are
mimicing the process of LSD radix sort in \cref{fig:radixsort_rots},
in that the sorted portions to the right of the shaded column are
identical.

\begin{figure}[!hb]
  \centering
  \begin{tt}
  \setlength{\tabcolsep}{0pt}
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{aaaaaaaaaaa}&!& \\
    \textcolor{lightgray}{rrrrrrrrrrr}&a& \\
    \textcolor{lightgray}{ddddddddddd}&a& \\
    \textcolor{lightgray}{!!!!!!!!!!!}&a& \\
    \textcolor{lightgray}{rrrrrrrrrrr}&a& \\
    \textcolor{lightgray}{ccccccccccc}&a& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&b& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&b& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&c& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&d& \\
    \textcolor{lightgray}{bbbbbbbbbbb}&r& \\
    \textcolor{lightgray}{bbbbbbbbbbb}&r&
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{aaaaaaaaa}&!&ab \\
    \textcolor{lightgray}{rrrrrrrrr}&a&!a \\
    \textcolor{lightgray}{ddddddddd}&a&br \\
    \textcolor{lightgray}{!!!!!!!!!}&a&br \\
    \textcolor{lightgray}{rrrrrrrrr}&a&ca \\
    \textcolor{lightgray}{ccccccccc}&a&da \\
    \textcolor{lightgray}{aaaaaaaaa}&b&ra \\
    \textcolor{lightgray}{aaaaaaaaa}&b&ra \\
    \textcolor{lightgray}{aaaaaaaaa}&c&ad \\
    \textcolor{lightgray}{aaaaaaaaa}&d&ab \\
    \textcolor{lightgray}{bbbbbbbbb}&r&a! \\
    \textcolor{lightgray}{bbbbbbbbb}&r&ac
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{a}&!&abracadabr \\
    \textcolor{lightgray}{r}&a&!abracadab \\
    \textcolor{lightgray}{d}&a&bra!abraca \\
    \textcolor{lightgray}{!}&a&bracadabra \\
    \textcolor{lightgray}{r}&a&cadabra!ab \\
    \textcolor{lightgray}{c}&a&dabra!abra \\
    \textcolor{lightgray}{a}&b&ra!abracad \\
    \textcolor{lightgray}{a}&b&racadabra! \\
    \textcolor{lightgray}{a}&c&adabra!abr \\
    \textcolor{lightgray}{a}&d&abra!abrac \\
    \textcolor{lightgray}{b}&r&a!abracada \\
    \textcolor{lightgray}{b}&r&acadabra!a
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &!&abracadabra \\
    &a&!abracadabr \\
    &a&bra!abracad \\
    &a&bracadabra! \\
    &a&cadabra!abr \\
    &a&dabra!abrac \\
    &b&ra!abracada \\
    &b&racadabra!a \\
    &c&adabra!abra \\
    &d&abra!abraca \\
    &r&a!abracadab \\
    &r&acadabra!ab
  \end{tabular}
  \end{tt}
  \caption{Recreating the rotation matrix from the last column.}
  \label{fig:recreate}
\end{figure}

The choice of the last column is crucial, because as the ``original
string'' in the stable column sort, it acts as the tie-breaker. The
last column is the solution to the fixpoint equation
\begin{equation*}
  \var{hdsort}~(\var{prefix\_shift}~c~m) = m
\end{equation*}
where \var{hdsort} sorts on the first column of a matrix,
$\var{prefix\_shift}~c$ adds $c$ as the first column of a matrix,
dropping the last column, and $m$ is the rotation matrix. This means
as we repeatedly prefix $l$ and sort, we preserve the ``rest of the
matrix'', even the parts that we have yet to reconstruct.

We can think of the process of sorting the matrix as computing the
solution to the fixpoint equation, subject to the constraints of the
lexicographic ordering. For example, we know that the collection of
all characters preceding an \texttt{a} in the sorted column must be
some permutation of
$[\texttt{c},~\texttt{d},~\texttt{r},~\texttt{r},~\texttt{!}]$,
because those are the characters that precede \texttt{a}s in the
original string. However, the order of these characters in the final
string
($[\texttt{r},~\texttt{d},~\texttt{!},~\texttt{r},~\texttt{c}]$) is
important. An \texttt{r} must be first, because in the original string
there is an \texttt{a} following an \texttt{r} which is itself
followed by a \texttt{!}, the smallest character. Similarly, although
the \texttt{d} and \texttt{!} both precede \texttt{a}s that are
followed by \texttt{bra}s, the next character for the \texttt{d} is a
\texttt{!}, and the next character for the \texttt{!} is a \texttt{c}.
This requires that the \texttt{d} precede the \texttt{!} in the last
column.

Generally, there are constraints on the relative order of the letters
in the column before the sorted column, and in each step we are
ensuring that these constraints are not violated one more character
deep in the string. By the time we have reached the last column, we
have satisfied all the constraints.

\section{Implementing the forwards BWT}
\label{sec:forwards_BWT}

First, we define the left and right rotations of a list. \var{lrot}
moves the first element of a list to the end.
\begin{lstlisting}
Definition lrot (l: list A) : list A :=
  match l with
  | [] => []
  | hd :: tl => tl ++ [hd]
  end.
\end{lstlisting}
\var{rrot} moves the last element of a list to the front, and
relies on \var{init}, which drops the last element of a list.
\begin{lstlisting}
Definition rrot (l: list A) : list A :=
  match l with
  | [] => []
  | hd :: tl => last l hd :: init l
  end.
\end{lstlisting}
Note that in order to ensure that $\var{last}$ is total, it takes
an extra argument to return in the case of an empty list.

Now we produce the rotation matrix by repeatedly applying \var{lrot}.
We will use \var{iter}, which takes a function $f : A \to A$, a number
$n$, and an initial value $z$, and returns the list of intermediate
values obtained by applying $f$ to $z$, $n$ times. Or,
\begin{equation*}
  \var{iter}~f~n~z = [z,~f~z,~f~(f~z),~\ldots,~f^{n-1} z]
  \label{eq:iter}
\end{equation*}
We use the notation $f^i z$ for $\var{rep}~f~i~z$, which implements
repeated function application. We will also use the notation $l_i$ for
the $i$th element of a list. In Coq, that is given by
$\var{nth}~i~l~d$, where $d:A$ is a default value in case $i \ge
\var{length}~l$.

Now we can define \var{rots} to produce the rotation matrix.
\begin{lstlisting}
Definition rots (l : list A) : list (list A) :=
  iter lrot (length l) l.
\end{lstlisting}

\var{bwl} produces the transformed string by taking the last column of
the sorted rotation matrix. \var{lexsort} sorts the rotation matrix
using insertion sort (\cref{appendix:insertion_sort}) according to the
induced lexicographic ordering(\cref{sec:lex_ord}).
\begin{lstlisting}
Definition bwl (l : list A) : list A :=
  match l with
  | [] => []
  | hd :: _ =>
    List.map (fun x => last x hd) (lexsort (rots l))
  end.
\end{lstlisting}

\var{bwn} returns the index in the sorted rotation matrix where the
original string appears. It relies on \var{findIndex}, which searches
a list for an element $y$ that is equal to the given element $x$, and
returns the index of $y$.
\begin{lstlisting}
Definition bwn (l : list A) : nat :=
  findIndex l (lexsort (rots l)).
\end{lstlisting}

Given these definitions, we can quickly prove that \var{bwn}
satisfies its specification using a few lemmas.
\begin{lemma}[findIndex\_correct]
  For all elements $x$ and lists $l$,
  \begin{equation*}
    (\exists y \in l,~x \equiv y) \implies l_{(\var{findIndex}~x~xs)} \equiv x
  \end{equation*}
\end{lemma}
\begin{lemma}[orig\_in\_sorted\_rots]
  For every non-empty list $l$,
  \begin{equation*}
    \exists y \in \var{lexsort}~(\var{rots}~l),~x = y
  \end{equation*}
\end{lemma}
\begin{restatable}[bwn\_correct]{theorem}{bwncorrect}
  \label{thm:bwn_correct}
  For every non-empty list $xs$,
  \begin{equation*}
    (\var{lexsort}~(\var{rots}~xs))_{(\var{bwn}~xs)} = xs
  \end{equation*}
\end{restatable}

\section{Impelementing the inverse BWT}
\label{sec:inverse_BWT}

Now we want to construct a function \var{unbwt} such that for all $l$,
\begin{equation}
  \var{unbwt}~(\var{bwn}~l)~(\var{bwl}~l) = l
  \label{eq:unbwt}
\end{equation}
If we could recreate the entire sorted rotation matrix from
$\var{bwl}~l$, it would be simple to recover $l$ by indexing into
the matrix at $\var{bwn}~l$.

More precisely, suppose we had a function function \var{recreate}
such that
\begin{equation}
  \label{eq:recreate}
  \var{recreate}~(\var{bwl}~l) = \var{lexsort}~(\var{rots}~l)
\end{equation}
Then, we could define \var{unbwt} as
\begin{lstlisting}
Definition unbwt (i : nat) (l : list A) : list A :=
  nth i (recreate l) l.
\end{lstlisting}
and \cref{eq:unbwt} would follow directly from \cref{thm:bwn_correct}.

We implement \var{recreate} as follows, introducing a parameter $j$ so
that we can recursively recreate the first $j$ columns of the matrix,
one column at a time. Here $l$ is meant to be the output of the BWT,
the last column of the sorted rotation matrix.
\begin{lstlisting}
Fixpoint recreate (j : nat) (l : list A)
  : list (list A) :=
  match j with
  | O    => map (const []) l
  | S j' => hdsort (prepend_col l (recreate j' l))
  end.
\end{lstlisting}
This definition relies on \var{prepend\_col}, which prepends a column
to a matrix.

Now we can redefine \var{unbwt} as
\begin{lstlisting}
Definition unbwt (i : nat) (l : list A) : list A :=
  nth i (recreate (length l) l) l.
\end{lstlisting}

Since we recreate the matrix column by column, we can define an
invariant based on \cref{eq:recreate} that uses $\var{cols}~j$, which
takes the first $j$ columns of a matrix.
\begin{restatable*}[recreate\_correct\_inv]{theorem}{recreatecorrectinv}
  For all lists $l$ and $j \le \var{length}~l$,
  $\var{recreate}~j~(\var{bwl}~l) =
  \var{cols}~j~(\var{lexsort}~(\var{rots}~l))$
\end{restatable*}

If we can prove this theorem, we will be able to prove that our
implementation of \var{unbwt} is correct along the same lines as
before. To do so, we need to prove a few other theorems that
have already come up when explaining the algorithm conceptually.

First, we note that moving the last column of the rotation matrix to the front
is the same as moving the last row to the top, because the rotation
matrix is symmetric.
\begin{theorem}[map\_rrot\_rots]
  For all lists $l$,
  \begin{equation*}
    \var{map}~\var{rrot}~(\var{rots}~l) = \var{rrot}~(\var{rots}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  Letting $n = \var{length}~l$
  \begin{align*}
       &\var{map}~\var{rrot}~(\var{rots}~l) \\
    ={}&\var{map}~\var{rrot}~[l,~\var{lrot}~l,~\var{lrot}^2~l,~\ldots,~\var{lrot}^{n-1}~l] \\
    ={}&[\var{rrot}~l,~l,~\var{lrot}~l,~\ldots,~\var{lrot}^{n-2}~l] \\
    ={}&[\var{lrot}^{n-1}~l,~l,~\var{lrot}~l,~\ldots,~\var{lrot}^{n-2}~l] \\
    ={}&\var{rrot}~[l,~\var{lrot}~l,~\ldots,~\var{lrot}^{n-1}~l] \\
    ={}&\var{rrot}~(\var{rots}~l) \qedhere
  \end{align*}
\end{proof}

Now we can prove that the last column is indeed a fixpoint.
\begin{theorem}[lexsort\_rots\_hdsort]
  For all lists $l$,
  \begin{equation*}
    (\var{hdsort} \comp
    \var{map}~\var{rrot})~(\var{lexsort}~(\var{rots}~l)) =
    \var{lexsort}~(\var{rots}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  By \cref{thm:sort_ord_unique}, any two sorts are equivalent, and we
  can replace \var{lexsort} with an LSD radixsort implemented as
  \begin{equation*}
    \var{radixsort}~m \coloneqq (\var{hdsort} \comp \var{map}~\var{rrot})^n~m
  \end{equation*}
  where $m$ is a matrix with $n$ columns. In this case, we are sorting
  the rotation matrix and we let $n = \var{length}~l$. Then we have
  \begin{align*}
       &(\var{hdsort} \comp \var{map}~\var{rrot})~(\var{lexsort}~(\var{rots}~l)) \\
    ={}&(\var{hdsort} \comp \var{map}~\var{rrot})~((\var{hdsort} \comp \var{map}~\var{rrot})^n~(\var{rots}~l)) \\
    ={}&(\var{hdsort} \comp \var{map}~\var{rrot})^{n+1}~(\var{rots}~l) \\
    ={}&(\var{hdsort} \comp \var{map}~\var{rrot})^n~((\var{hdsort} \comp \var{map}~\var{rrot})~(\var{rots}~l)) \\
    ={}&\var{lexsort}~((\var{hdsort} \comp \var{map}~\var{rrot})~(\var{rots}~l)) \\
    ={}&\var{lexsort}~((\var{hdsort} \comp \var{rrot})~(\var{rots}~l)) \\
    ={}&\var{lexsort}~(\var{rots}~l)
  \end{align*}
  The last step follows because the function $\var{hdsort} \comp
  \var{rrot}$ permutes the rotation matrix.
\end{proof}

We need two more lemmas that describe the interaction between
\var{cols} and $\var{map}~\var{rrot}$.

\begin{theorem}[cols\_hdsort\_comm]
  For all matrices $m$ and naturals $j$,
  \begin{equation*}
    \var{cols}~(j+1)~(\var{hdsort}~m) = \var{hdsort}~(\var{cols}~(j+1)~m)
  \end{equation*}
\end{theorem}
\begin{proof}
  \var{hdsort} only examines the first elements of the rows of $m$,
  and $\var{cols}~(j+1)$ doesn't alter the first elements of the rows
  of $m$.\footnote{This argument is made precise by
    \var{key\_sort\_inv} in \cref{sec:lex_ord}.}
\end{proof}
\begin{theorem}[cols\_map\_rrot]
  For all $j$ and matrices $m$ that has rows all of length less than $j$,
  \begin{gather*}
    \var{cols}~(j+1)~(\var{map}~\var{rrot}~m) = \\
    \var{prepend\_col}~(\var{map}~\var{last}~m)~(\var{cols}~j~m)
  \end{gather*}
\end{theorem}
\begin{proof}
  We reason by induction on $m$. In the inductive case we need to show that
  \begin{gather*}
    \var{cols}~(j+1)~(\var{map}~\var{rrot}~(r::m)) = \\
    \var{prepend\_col}~(\var{map}~\var{last}~(r::m))~(\var{cols}~j~(r::m))
  \end{gather*}
  Simplifying, we have that the tails are equal by the induction
  hypothesis, and we must show that the heads are equal.
  \begin{align*}
    \var{firstn}~(j+1)~(\var{rrot}~r) &= \var{last}~r :: \var{firstn}~j~r \\
    \var{firstn}~(j+1)~(\var{last}~r :: \var{init}~r) &= \var{last}~r
    :: \var{firstn}~j~r \\
    \var{last}~r :: \var{firstn}~j~(\var{init}~r) &= \var{last}~r :: \var{firstn}~j~r \\
    \var{firstn}~j~(\var{init}~r) &= \var{firstn}~j~r
  \end{align*}
  The bottom equation is true because $j > \var{length}~r$, so if $j=
  0$, $r = []$.
\end{proof}

Now we are ready to prove the invariant:
\recreatecorrectinv
\begin{proof}
  Letting $m = \var{lexsort}~(\var{rots}~l)$,
  \begin{align*}
    &\var{cols}~(j+1)~m \\
    ={}& \var{cols}~(j+1)~(\var{hdsort}~(\var{map}~\var{rrot}~m)) \\
    ={}&
    \var{hdsort}~(\var{cols}~(j+1)~(\var{map}~\var{rrot}~m)) \\
    ={}&
    \var{hdsort}~(\var{prepend\_col}~(\var{map}~\var{last}~m)~(\var{cols}~j~m)) \\
    ={}&
    \var{hdsort}~(\var{prepend\_col}~(\var{bwl}~l)~(\var{recreate}~j~(\var{bwl}~l))) \\
    ={}& \var{recreate}~j~(\var{bwl}~l) \qedhere
  \end{align*}
\end{proof}

\begin{theorem}[cols\_id]
  For all matrices $m$ such that all the rows of $m$ have length less
  than or equal to $n$,
  \begin{equation*}
    \var{cols}~n~m = m
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows by induction on $m$.
\end{proof}

\unbwtcorrect
\begin{proof}
\begin{align*}
     \var{unbwt}~(\var{bwn}~l)~(\var{bwl}~l)
  &= (\var{recreate}~(\var{length}~l))(\var{bwl}~l))_{(\var{bwn})~l}\\
  &= (\var{cols}~(\var{length}~l)~(\var{lexsort}~(\var{rots}~l)))_{(\var{bwn})~l} \\
  &= (\var{lexsort}~(\var{rots}~l))_{(\var{bwn})~l} \\
  &= l \qedhere
\end{align*}
\end{proof}

\section{\var{Sorted}, \var{Permutation}, and \var{StablePermutation}}
\label{sec:sorted_perm_stable}

The previous section's proofs relied on the correctness and stability
of various sorts. The Coq standard library includes definitions for
\var{Sorted} and \var{Permutation}, but no definition for stability.
It also includes an implementation of mergesort, but it uses Coq's
module system to enable parametricity over ordered types, which is
much less convenient to use than typeclasses. In this section, we
define our own version of \var{Sorted} and \var{StablePerm} that are
based on our \var{Preord} typeclass.

In this section, we restate the definition outlined in
\cref{sec:main_ideas} more formally, and prove the appropriate
equivalences.

\subsection{\var{Sorted}}
\label{subsec:sorted}

\begin{definition}{SortedIx}
  A list $l$ is sorted if for all indices $i$ and $j$,
  \begin{equation*}
    i \le j \implies l_i \le l_j
  \end{equation*}
\end{definition}
\begin{definition}{Sorted}
  \begin{align*}
    \infer[\var{Sorted\_nil}]{\var{Sorted}~[]}{}
    &&
    \infer[\var{Sorted\_cons}]{\var{Sorted}~(a :: l)}{%
      (\forall~x \in l,~a < x) & \var{Sorted}~l
    }
  \end{align*}
\end{definition}

By transitivity it suffices to check that the new element is smaller
than just the first element of the sorted list. That leads to a third
definition of sorted:
\begin{definition}{SortedLocal}
  \begin{gather*}
    \infer[\var{SortedLocal\_nil}]{\var{SortedLocal}~[]}{}
    \\
    \\
    \infer[\var{SortedLocal\_1}]{\var{SortedLocal}~[a]}{}
    \\
    \\
    \infer[\var{SortedLocal\_cons}]{\var{SortedLocal}~(a :: b :: l)}{%
      a < b & \var{SortedLocal}~(b :: l)
    }
  \end{gather*}
\end{definition}

\begin{theorem}[SortedLocal\_iff]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~l \iff \var{SortedLocal}~l
  \end{equation*}
\end{theorem}
\begin{proof}
  Both directions follow by induction on $l$, making use of the
  transitivity of $\le$.
\end{proof}

\begin{theorem}[SortedIx\_iff]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~l \iff \var{SortedIx}~l
  \end{equation*}
\end{theorem}

\subsection{\var{Permutation}}
\label{subsec:perm}

The word \textit{permutation} has two related meanings that are often
used interchangeably. We sometimes say that two lists are permutations
of each other if they contain all of the same elements, just appearing
in a different order. However, in mathematics we often say that a
permutation is actually the bijection on indices that defines the
correspondence between the two lists. For example, we say that $p$ is
the permutation that relates $l$ to $l'$ if for all indices $i$ and
$j$,
\begin{equation*}
  l_{p(i)} = l'_i
\end{equation*}
For consistency with Coq's built-in \var{Permuation} relation, in a
situation like this we will say that $l$ and $l'$ are permutations of
each other, and that $p$ is the \textit{permutation function} that
relates $l$ to $l'$.

So, we can also say that two lists $l$ and $l'$ are permutations of
each other is precisely when
\begin{itemize}
\item $\var{length}~l = \var{length}~l' = n$
\item there exists a permutation function $p : \mathbb{N}_{<n} \to
  \mathbb{N}_{<n}$, where $p$ is a bijection, such that for all
  indices $i$,
  \begin{equation*}
    l_i = l'_{p(i)}
  \end{equation*}
\end{itemize}

\begin{definition}{PermutationCount}
  Two lists are permutations of one another when for any $x$,
  \begin{equation*}
    \var{count\_occ}~l~x = \var{count\_occ}~l'~x
  \end{equation*}
  where $\var{count\_occ}~l~x$ counts the number of elements in $l$
  that are equal to $x$ under some equivalence relation.
\end{definition}
\var{count\_occ} counts the number of occurrences of a given variable
using a decision procedure for Leibniz equality. Here that decision
procedure is \texttt{equiv\_dec} from an \var{EqDec} instance where we
require the equivalence relation itself by Leibniz equality.

In Coq, we can represent a finite function as a list of its outputs:
$f : \mathbb{N}_{<n} \to A$ is represented by the $\var{list}~A$
$[f(0),~f(1),\allowbreak\ldots,f(n-1)]$. For permutations, this is
consistent with the notation where we denote by $2~1~0$ the
permutation $p(0) = 2$, $p(1) = 1$, $p(2) = 0$. So, $p$ here would be
represented by $[2,~1,~0]$. To restrict the range to $\mathbb{N}_{<n}$
we say that $\forall i,~i \in p \implies ~i<n$; to make the function
surjective, we strengthen that to $\forall i,~i \in p \iff ~i<n$; and to make it
injective we require that $p$ not contain any duplicate entries. The
inductive property \var{NoDup} from the Coq standard library captures
that requirement. This leads to the following definition:
\begin{definition}{PermFun}
  A list of natural numbers $p$ is a permutation function on $n$
  elements if it contains no duplicate entries, and if for all natural
  numbers $i$, $i \in p \iff i < n$.
\end{definition}

Then, we can define \var{apply}, which permutes a list according to a
given permutation:
\begin{lstlisting}
  Definition apply p l : list A :=
  match l with
  | [] => []
  | d :: _ => map (fun i => nth i l d) p
  end.
\end{lstlisting}
We will use $p(l)$ as notation for $\var{apply}~p~l$ when $l$ is a
list.

And finally, we can define a notion of when two lists are permutations
of one another based on permutation functions.
\begin{definition}{PermutationEx}
  Two lists $l$ and $l'$ are permutations of one another when there
  exists a permutation function $p$ on $\var{length}~l$ elements such
  that $p(l) = l'$.
\end{definition}

\begin{theorem}[PermutationCount\_iff]
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{Permutation}~l~l' \iff \var{PermutationCount}~l~l'
  \end{equation*}
\end{theorem}

\subsection{\var{StablePerm}}
\label{subsec:stable}

\begin{definition}{StablePerm}
  Two lists are stable permutations of one aonther if for any $x$,
  \begin{equation*}
    \var{filter}~(\lambda y. y \equiv x)~l = \var{filter}~(\lambda y. y \equiv x)~l'
  \end{equation*}
\end{definition}

\begin{definition}{StablePermFun\_preimage}
  A permutation function $p$ is a stable permutation function on some
  $l : \var{list}~A$ when for all $i < j$ such that $l_i \equiv l_j$, we
  also have $p(i) < p(j)$.
\end{definition}
\begin{definition}{PermutationEx}
  Two lists $l$ and $l'$ are stable permutations of one another when
  there exists a stable permutation function~$p$ on $l$ such that
  $p(l) = l'$.
\end{definition}

\begin{definition}{StablePermInd}
  \begin{gather*}
    \infer[\var{StablePermInd\_nil}]{\var{StablePermInd}~[]~[]}{}
    \\ \\
    \infer[\var{StablePermInd\_skip}]{\var{StablePermInd}~(x :: l)~(x :: l')}{%
      \var{StablePermInd}~l~l'
    }
    \\ \\
    \infer[\var{StablePermInd\_swap}]
          {\var{StablePermInd}~(x :: y :: l)~(y :: x :: l)}
          {x \not\equiv y}
          \\ \\
          \infer[\var{StablePermInd\_trans}]{\var{StablePermInd}~l~l''}{
            \var{StablePermInd}~l~l' & \var{StablePermInd}~l'~l''
          }
  \end{gather*}
\end{definition}

\section{Uniqueness of stable sorts}
\label{sec:unique}

Using our definitions of \var{StablePerm} and \var{Sorted}, we can
prove the theorem that allowed us to replace \var{lexsort} with
radixsort.

\begin{theorem}[StablePerm\_Sorted\_eq]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~l \implies \var{Sorted}~l' \implies
    \var{StablePerm}~l~l' \implies l = l'.
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $l$. In the inductive case, we have to
  show that
  \begin{equation*}
    h :: t = h' :: t'
  \end{equation*}
  given $\var{Sorted}~(h :: t)$, $\var{Sorted}~(h' :: t')$, and
  $\var{StablePerm}~(h :: t)~(h' :: t')$, as well as $t = t'$ from the
  induction hypothesis. So it suffices to show $h = h'$.

  First, we can prove that $h \equiv h'$. We know that $h \in h' :: t'$ and
  $h' \in h :: t$ because they are permutations of one another. If $h =
  h'$, then we are done, so assuming $h \neq h'$ we know $h \in t'$ and $h'
  \in t$. Because both $h::t$ and $h'::t'$ are sorted, we know that $h \le
  h'$ and $h' \le h$, implying $h \equiv h'$.

  From this we can prove that $h = h'$. From $\var{StablePerm}~(h ::
  t)~(h' :: t')$ we have
  \begin{align*}
    \var{filter}~(\equiv h)~(h :: t) &= \var{filter}~(\equiv h)~(h' :: t') \\
    h :: \var{filter}~(\equiv h)~t &= h' :: \var{filter}~(\equiv h)~t'
  \end{align*}
  We also have $t = t'$ by the induction hypothesis, so we can
  conclude that $h = h'$.
\end{proof}

If we define \var{Sort} and \var{StableSort} as we did
\cref{sec:sorted_perm_stable}, we can restate the theorem slightly
more elegantly.
\begin{lstlisting}
  Definition Sort f :=
  forall l, Sorted (f l) /\ Permutation (f l) l.
\end{lstlisting}

\stablesortunique
\begin{proof}
  This follows from \var{StablePerm\_Sorted\_eq}.
\end{proof}

However, we do not want to require that \var{lexsort} be implemented
using a stable sorting algorithm. Luckily, because we require that $A$
be equipped with an order and not just a preorder, we know that
equivalent elements are actually indistinguishable. This means that
all sorting algorithms can be considered to be stable: it is
impossible to tell if any two equivalent elements were swapped because
you cannot distinguish between equivalent elements.

\begin{theorem}[all\_perm\_stable]
  If we have an equivalence that implies Leibniz equality, then for
  all lists $l$ and $l'$
  \begin{equation*}
    \var{Permutation}~l~l' \implies \var{StablePerm}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the evidence of $\var{Permutation}~l~l'$.
  Every case other than the \var{perm\_swap} case follows easily.
  In the \var{perm\_swap} case, we must show that
  \begin{equation*}
    \var{StablePerm}~(y :: x :: l)~(x :: y :: l)
  \end{equation*}
  for all $x$ and $y$. If $x \not\equiv y$, we can just apply
  \var{StablePerm\_swap}. If $x \equiv y$, then we know that $x = y$, and
  we can show
  \begin{equation*}
    \var{StablePerm}~(x :: x :: l)~(x :: x :: l)
  \end{equation*}
  by applying \var{perm\_skip} twice.
\end{proof}

\begin{corollary}[Sort\_StableSort\_Ord]
  \label{thm:stablesort_ord}
  When sorting using an order,
  \begin{equation*}
    \var{Sort}~f \iff \var{StableSort}~f
  \end{equation*}
  for any $f$.
\end{corollary}
\begin{proof}
  This follows from \var{all\_perm\_stable}.
\end{proof}

\begin{theorem}[Sort\_Ord\_unique]
  \label{thm:sort_ord_unique}
  When sorting using an order, given any two function $f, g:
  \var{list}~A \to \var{list}~A$,
  \begin{equation*}
    \var{Sort}~f \implies \var{Sort}~g \implies
    (\forall l,~f~l = g~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows from \cref{thm:stablesort_ord} and \cref{thm:stablesort_unique}.
\end{proof}

\section{Lexicographic orders and keys}
\label{sec:lex_ord}

Given a preorder on a type $A$, we can define a lexicographic preorder
on $\var{list}~A$. To capture this, we define the inductive relation
\var{lex\_le}, which we will denote by $\lexle$ in proofs.
\begin{gather*}
  \infer[\var{lex\_le\_nil}]{[] \lexle ys}{}
  \\
  \\
  \infer[\var{lex\_le\_cons\_lt}]{(x :: xs) \lexle (y :: ys)}{%
    x < y
  }
  \\
  \\
  \infer[\var{lex\_le\_cons\_eq}]{(x :: xs) \lexle (y :: ys)}{%
    x \equiv y & xs \lexle ys
  }
\end{gather*}

Given a preorder on a type $K$ and a key function $\var{key} : A \to K$,
we can define a preorder on $A$ that compares all elements using the
key function. That is, we can define the relation $\le_{\var{key}}$ on
$A$ such that for all $x, y : A$
\begin{equation*}
  x \le_{\var{key}} y \coloneqq (\var{key}~x) \le (\var{key}~y)
\end{equation*}

\begin{theorem}[key\_sort\_inv]
  Given a key function $\var{key} : A \to K$ and a function $f : A \to A$
  such that for all $x : A$,
  \begin{equation*}
    \var{key}~(f~x) = \var{key}~x
  \end{equation*}
  we have that for all lists $l$
  \begin{equation*}
    \var{sort}~(\var{map}~\var{f}~l) = \var{map}~\var{f}~(\var{sort}~l).
  \end{equation*}
  where \var{sort} is insertion sort.\footnote{This theorem actually
    holds for any function with type $\forall X, (X \to X \to \mathbb{B}) \to
    \var{list}~X \to \var{list}~X$ due to parametricity \cite{free}.}
\end{theorem}
\begin{proof}
  The proof follows by induction, unfolding the definition of
  \var{sort}. All decisions are made by calling \var{key}, so both
  sides are equal.
\end{proof}

Taken together, we can define prefix lexicographic orderings on lists,
where the lexicographic ordering is only allowed to compare up to the
first $n$ elements of two lists. We define the relation $\le_n$ on
$\var{list}~A$ such that for all $xs, ys : \var{list}~A$,
\begin{equation*}
  xs \le_n ys \coloneqq (\var{firstn}~n~xs) \lexle (\var{firstn}~n~ys)
\end{equation*}
For convenience, we define $\var{PrefixSorted}~n$ to hold when a list
of lists is sorted if you only look at the first $n$ elements of each:
\begin{lstlisting}
  Definition PrefixSorted (n : nat) : list (list A) -> Prop
  := Sorted (keyOrd (firstn n)).
\end{lstlisting}

We can describe the relationship between orders that can look at fewer
or more characters with the following two theorems. We will briefly
offer an explanation of why these theorems makes sense, but these
explanations do not mirror the Coq proofs we have for these theorems,
which are hard to follow.

\begin{theorem}[key\_lt\_firstn\_ge]
  For all $j$ and $k$ such that $j \le k$,
  \begin{equation*}
    x <_j y \implies x <_k y
  \end{equation*}
\end{theorem}
\begin{proof}
  If $x <_j y$, then there must be some index $i < j$ such that $x_i <
  y_i$. Then $i < k$ as well, so $x <_k y$.
\end{proof}

\begin{theorem}[key\_le\_firstn\_ge]
  For all $j$ and $k$ such that $k \ge j$,
  \begin{equation*}
    x \le_k y \implies x \le_j y
  \end{equation*}
\end{theorem}
\begin{proof}
  If $x \le_k y$, then either the first $k$ characters are all
  equivalent, in which case the first $j$ characters are all
  equivalent as well, or there was some $i < k$ such that $x_i < y_i$,
  but at every index before $i$ $x$ and $y$ are equivalent. If $j <
  i$, then the first $j$ characters are still equivalent, and we have
  $x \equiv y \implies x \le_j y$. If $j \ge i$, then the deciding character is
  among the first $j$ and $x \le_j k$.
\end{proof}

If we consider the implications this has for stability, a weaker order
can distinguish between fewer elements, which means that it considers
more elements equivalent. This means that stability with respect to a
weaker order is actually a stronger property, because the weaker
order, considering more elements equivalent, is more restrictive about
which swaps it allows.

\begin{theorem}[StablePerm\_weaken]
  Suppose we have a two preorders $O$ and $O'$ on $A$, and two lists
  $l_1$ and $l_2$, with elements of type $A$.

  We also know that on the elements of $l_1$, $O'$ is stronger than
  $O$. If we denote by $\le$ and $\le'$ the relations for $O$ and $O'$
  respectively, this means that for all $x$ and $y$ in $l_1$,
  \begin{equation*}
    x \le' y \implies x \le y.
  \end{equation*}
  Then, denoting by \var{StablePerm} stability relative to the
  equivalence relation of $O$, and by $\var{StablePerm}'$ stability
  relative to the equivalence relation of $O'$, we have
  \begin{equation*}
    \var{StablePerm}~l_1~l_2 \implies \var{StablePerm}'~l_1~l_2
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason inductively on the proof of $\var{StablePerm}~l_1~l_2$
  using \var{StablePermInd\_iff}. The important case is the swap case,
  where we have to show
  \begin{equation*}
    \var{StablePerm}'~(y :: x :: l)~(x :: y :: l)
  \end{equation*}
  given that $x \not\equiv y$. To show this, we need to know that $x \not\equiv'
  y$, which follows from the fact that $O'$ is stronger than $O$.
\end{proof}

In a functional setting, we can define an LSD radixsort elegantly by
repeatedly moving the last column to the front of the matrix and
sorting on the first column. For a matrix with $n$ columns,
\begin{equation*}
  \var{radixsort} = (\var{hdsort} \comp \var{map}~\var{rrot})^n
\end{equation*}
Or, in Coq:
\begin{lstlisting}
  Definition radixsort m (n : nat) : list (list A)
  := rep (hdsort ∘ map rrot) n m.
\end{lstlisting}
Here \var{hdsort} is just an insertion sort using $\le_1$ ordering.

First we want to prove that \var{radixsort} permutes the rows of the
matrix. We will start by proving the invariant that if we rotate the
last column to the front and sort $j$ times, the resulting matrix is a
permutation of simply rotating the last column to the front $j$
times.
\begin{lemma}[radixsort\_perm\_inv]
  For all matrices $m$ and $j : \mathbb{N}$,
  \begin{equation*}
    \var{Permutation}~((\var{map}~\var{rrot})^j~m)~
    ((\var{hdsort} \comp \var{map}~\var{rrot})^j~m)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $j$. In the inductive case,
  \begin{align*}
    {}& (\var{hdsort} \comp \var{map}~\var{rrot})^{j+1}~m \\
    ={}& \var{hdsort}~(\var{map}~\var{rrot}~(\var{hdsort} \comp
    \var{map}~\var{rrot})^j~m) \\
    \eqperm{}& \var{map}~\var{rrot}~((\var{hdsort} \comp
    \var{map}~\var{rrot})^j~m) \\
    \eqperm{}& \var{map}~\var{rrot}~((\var{map}~\var{rrot})^j~m) \\
    ={}& (\var{map}~\var{rrot})^{j+1}~m \qedhere
  \end{align*}
\end{proof}

\begin{theorem}[radixsort\_perm]
  For all matrices $m$ with rows of length $n$,
  \begin{equation*}
    \var{Permutation}~m~(\var{radixsort}~m~n)
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{align*}
    m ={}&
    \var{map}~\var{rrot}^n~m \\
    ={}& (\var{map}~\var{rrot})^n~m \\
    \eqperm{}& (\var{hdsort} \comp \var{map}~\var{rrot})^n~m \\
    ={}& \var{radixsort}~m~n \qedhere
  \end{align*}
\end{proof}

We will prove the stability of \var{radixsort} in a similar way, but
first we need a lemma.
\begin{lemma}[StablePerm\_map\_rrot]
  For all matrices $m$ and $m'$,
  \begin{equation*}
    \var{StablePerm}~m~m' \implies
    \var{StablePerm}~(\var{map}~\var{rrot}~m)~(\var{map}~\var{rrot}~m').
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on the evidence that $m \eqperm m'$
  using \var{StablePermInd\_iff}. The important case is the swap case,
  where we have to show
  \begin{equation*}
    \var{StablePerm}~(\var{map}~(y :: x :: l))~(\var{map}~(x :: y :: l))
  \end{equation*}
  given that $x \not\equivle y$. For this it suffices to show that
  $\var{rrot}~x \not\equivle \var{rrot}~y$. Since lexicographic
  equivalence means that all the pairs of elements at corresponding
  indices are equivalent, when \var{rrot} moves the first elements of
  both lists to the end of both lists, it preserves lexicographic
  equivalence. Therefore
  \begin{equation*}
    x \equivle y \iff \var{rrot}~x \equivle \var{rrot}~y \qedhere
  \end{equation*}
\end{proof}

\begin{lemma}[radixsort\_stable\_inv]
  For all matrices $m$ and $j : \mathbb{N}$,
  \begin{equation*}
    \var{StablePerm}~((\var{map}~\var{rrot})^j~m)~
    ((\var{hdsort} \comp \var{map}~\var{rrot})^j~m)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $j$. In the inductive case,
  \begin{align*}
    {}& (\var{hdsort} \comp \var{map}~\var{rrot})^{j+1}~m \\
    ={}&\var{hdsort}~(\var{map}~\var{rrot}~(\var{hdsort} \comp
    \var{map}~\var{rrot})^j~m) \\
    \eqstable{}& \var{map}~\var{rrot}~((\var{hdsort} \comp
    \var{map}~\var{rrot})^j~m) \\
    \eqstable{}& \var{map}~\var{rrot}~((\var{map}~\var{rrot})^j~m) \\
    ={}& (\var{map}~\var{rrot})^{j+1}~m
  \end{align*}
  Going from the first line to the second line requires using
  \var{StablePerm\_weaken} to show that the stability we are
  guaranteed by \var{hdsort}, namely stability with respect to an
  ordering that examines at most one element, implies stability with
  respect to a regular lexicographic ordering.
\end{proof}

\begin{theorem}[radixsort\_stable]
  For all matrices $m$ with rows of length $n$,
  \begin{equation*}
    \var{StablePerm}~m~(\var{radixsort}~m~n)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof similar to the proof of \var{radixsort\_perm}.
\end{proof}

We would also like to establish an invariant to prove that
\var{radixsort} produces a sorted list. To do so, we need to be able
to reason about the process of adding a column to the front of a
sorted matrix and sorting on that column. Intuitively, we need to know
that if we have a matrix that is sorted on the first $j$ columns, and
we add a new column to the front and sort on it, the result list will
be sorted on the first $j+1$ columns. For a proof, see
\cref{appendix:hdsort_sorted_S}.
\begin{restatable*}[hdsort\_sorted\_S]{theorem}{hdsortsortedS}
  For all matrices $m$ and naturals $j$,
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{map}~\var{tl}~m) \implies
    \var{PrefixSorted}~(j+1)~(\var{hdsort}~m)
  \end{equation*}
\end{restatable*}

We also need the following property about \var{prepend\_col}, that
states that prepending a column and then removing the first column is
a no-op.
\begin{theorem}[map\_tl\_prepend]
  For all matrices $m : \var{list}\ \allowbreak(\var{list}~A)$ and columns $c :
  \var{list}~A$, where $c$ has at least as many elements as $m$ has columns,
  \begin{equation*}
    \var{map}~\var{tl}~(\var{prepend\_col}~c~m) = m
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $m$. In the inductive case, we have to
  prove that
  \begin{align*}
    &\var{map}~\var{tl}~(\var{prepend\_col}~(h_c :: t_c)~(r :: m)) \\
    ={}&\var{map}~\var{tl}~((h_c :: r) :: \var{prepend\_col}~t_c~m) \\
    ={}&r :: \var{map}~\var{tl}~(\var{prepend\_col}~t_c~m) \\
    ={}&r :: m
  \end{align*}
  where the last line follows from the induction hypothesis.
\end{proof}

\begin{corollary}
  For all matrices $m$ and rows $r$,
  \begin{equation*}
    \var{map}~\var{tl}~(\var{map}~\var{rrot}~m) = \var{map}~\var{init}~m
  \end{equation*}
\end{corollary}
\begin{proof}
  \begin{align*}
    {}& \var{map}~\var{tl}~(\var{map}~\var{rrot}~m) \\
    ={}& \var{map}~\var{tl}~(\var{prepend\_col}~(\var{map}~\var{last}~m)~(\var{map}~\var{init}~m) \\
    ={}& \var{map}~\var{init}~m \qedhere
  \end{align*}
\end{proof}

Now we can establish our invariant:

\begin{lemma}[radixsort\_sorted\_inv]
  For all matrices $m$ with rows of length $n$, and $j \le n$,
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{radixsort}~m~j)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $j$. The base case is true because every
  list is sorted under ordering where you examine no elements of the
  list. In the inductive case, we reason backwards from our goal:
  \begin{align*}
    &\var{PrefixSorted}~(j+1)~(\var{radixsort}~m~(j+1)) \\
    \iff{}&\var{PrefixSorted}~(j+1)~(\var{hdsort}~(\var{map}~\var{rrot}~(\var{radixsort}~m~j))) \\
    \iff{}&\var{PrefixSorted}~j~(\var{map}~\var{tl}~(\var{map}~\var{rrot}~(\var{radixsort}~m~j))) \\
    \iff{}&\var{PrefixSorted}~j~(\var{map}~\var{init}~(\var{radixsort}~m~j)) \\
    \iff{}&\var{Sorted}~(\var{map}~(\var{firstn}~j)~(\var{map}~\var{init}~(\var{radixsort}~m~j))) \\
    \iff{}&\var{Sorted}~(\var{map}~(\var{firstn}~j)~(\var{radixsort}~m~j)) \\
    \iff{}&\var{PrefixSorted}~j~(\var{radixsort}~m~j)
  \end{align*}
  The last line follows from the induction hypothesis.
\end{proof}

\begin{theorem}[radixsort\_sorted]
  For all matrices $m$ with rows of length $n$,
  \begin{equation*}
    \var{Sorted}~(\var{radixsort}~m~n)
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{align*}
    &\var{Sorted}~(\var{radixsort}~m~n) \\
    \iff{}&\var{Sorted}~(\var{map}~(\var{firstn}~n)~(\var{radixsort}~m~n)) \\
    \iff{}&\var{PrefixSorted}~n~(\var{radixsort}~m~n))
  \end{align*}
  The last line follows from \var{radixsort\_sorted\_inv}.
\end{proof}

\begin{theorem}[radixsort\_correct]
  \label{thm:radixsort_correct}%
  For all matrices $m$ with $n$ columns,
  \begin{equation*}
    \var{Sorted}~(\var{radixsort}~m~n) \land \var{StablePerm}~(\var{radixsort}~m~n)~m
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows from \var{radixsort\_sorted} and \var{radixsort\_stable}.
\end{proof}

\section{Relationship to the standard BWT}
\label{sec:opt}

Standard implementations of the BWT do not actually reconstruct the
entire rotation matrix, as this would be inefficient. Instead, they
avoid this using the optimization described in this section. Although,
we did not prove or implement this optimization in Coq, it would be
straightforward to do so.

In \var{recreate}, we repeatedly prepend the last column to the front
of the matrix and sort on it. However, each time that column is the
same, so the permutation that takes us from the unsorted column to the
sorted column is the same. Therefore, we can calculate that
permutation function once and store it in a variable $p$. The $i$th
column of the recreated matrix is simply $(\var{apply}~p)^i~f$ where
$f$ is the first column of the sorted rotation matrix, so we can
define
\begin{lstlisting}
  Definition recreate' (p : list nat) (l : list A)
  : list (list A) :=
  transpose (iter (apply p) (length l) (apply p l))
\end{lstlisting}
In the definition above, we are repeatedly applying $p$ to the
first column of the rotation matrix, $\var{apply}~p~l$. Then we
take the transpose to create the matrix from the columns.

Since in $\var{unbwt}~i~l$, we want the $i$th row of the matrix, we
can eliminate the transpose by replacing the $\var{nth}~i$ with a
$\var{map}~\var{nth}~i$ Assuming we already have calculated $p$,
\begin{align*}
  \var{unbwt}~i~l &= (\var{recreate}~p~l)_i \\
  &= (\var{transpose}~(\var{iter}~(\var{apply}~p)~(\var{length}~l)~(\var{apply}~p~l)))_i \\
  &= \var{map}~(\lambda
  r.~r_i)~(\var{iter}~(\var{apply}~p)~(\var{length}~l)~(\var{apply}~p~l)) \\
  &= \var{map}~(\lambda r.~r_i)~(\var{tl}~(\var{iter}~(\var{apply}~p)~(1 +
  \var{length}~l)~l)) \\
  &= \var{tl}~(\var{map}~(\lambda r.~r_i)~(\var{iter}~(\var{apply}~p)~(1 + \var{length}~l)~l))
\end{align*}

Now, instead of indexing into each column at $i$, where the columns
are all the result of successively applying $p$ to $l$, we will
successively take the image of $i$ under $p$ to generate the indices
into $l$ where those characters appear. This is justified via the
following theorem:
\begin{theorem}
  For all $n$,
  \begin{equation*}
    \var{map}~(\lambda r.~r_i)~(\var{iter}~(\var{apply}~p)~n~l)
    =
    \var{map}~(\lambda j.~l_j)~(\var{iter}~(\var{image}~p)~n~i)
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{align*}
    & \var{map}~(\lambda r.~r_i)~(\var{iter}~(\var{apply}~p)~n~l) \\
    ={}& \var{map}~(\lambda r.~r_i)~[l,~(\var{apply}~p)~l,~(\var{apply}~p)^2~l,~\ldots,~(\var{apply}~p)^{n-1}~l] \\
    ={}& [l_i,~((\var{apply}~p)~l)_i,~((\var{apply}~p)^2~l)_i,~\ldots,~((\var{apply}~p)^{n-1}~l)_i] \\
    ={}& [l_i,~l_{p(i)},~l_{p^2(i)},~\ldots,~l_{p^{n-1}~i}] \\
    ={}& \var{map}~(\lambda j.~l_j)~[i,~p(i),~p^2(i),~\ldots,~p^{n-1}~i] \\
    ={}& \var{map}~(\lambda j.~l_j)~(\var{iter}~(\var{image}~p)~n~i) \qedhere
  \end{align*}
\end{proof}

So assuming we have some function $\var{calc\_sort\_perm} :
\var{list}~A \to \var{list}~\mathbb{N}$ to calculate the permutation
function that sorts $l$, we redefine \var{unbwt} as
\begin{lstlisting}
  Definition unbwt' (i : nat) (l : list A) : list A :=
  match l with
  | [] => []
  | d :: _ =>
  let p := calc_sort_perm l in
  let indices :=
  tl (iter (image p) (S (length l)) i)
  in map (fun j => nth l j d) indices
\end{lstlisting}
\var{calc\_sort\_perm} can be implemented in linear time using a
counting sort, so if \var{nth} took constant time, then \var{unbwt}
would take linear time as well. As a functional specification, this
closely matches the standard imperative algorithm for the inverse BWT.

\section{Conclusions}
We have implemented and proved that the the inverse BWT inverts the
forwards BWT, basing our work on Bird et~al.'s program calculation
derivation \cite{birdmu,pearls}. In the process, we defined the
properties of stable sorts, and implemented and proved correct an LSD
radixsort.

This paper shows that Bird's style of program calculation is very
useful for building machine-checked proofs. It leads to proofs that
are composed of many smaller lemmas, each justifying a single rewrite
step. Most of the time and effort went into proving the correctness of
radixsort. If we exclude the parts related to sorting or to other
general-purpose functions not specific to the BWT, the entire verified
implementation of the BWT consists of 42 lines of Gallina definitions,
7 lines of Gallina that are used only in proofs, and 823 lines of
theorem statements and Ltac code.\footnote{This includes
  \var{BurrowsWheeler.v}, \var{Columns.v}, \var{Rotations/Rotation.v},
  \var{Rotations/Rots.v}, \var{Lib/Iterate.v}, \var{Lib/Repeat.v} and
  \var{Lib/FindIndex.v}, which implement \var{bwl}, \var{bwn},
  \var{unbwt}, \var{cols}, \var{prepend\_col}, \var{lrot}, \var{rrot},
  \var{rots}, \var{iter}, and \var{rep}, \var{findIndex}.}

This implementation, including the optimizations mentioned in
\cref{sec:opt}, can be used as a functional specification to for a
verified implementation of bzip2. Of course, it would also be
necessary to have functional specifications for the other phases of
bzip2, such as Run-Length Encoding, Move-to-Front Encoding, and
arithmetic coding. As luck would have it, Bird has already analyzed
arithmetic coding \cite{pearls}, and the other two phases are
relatively simple.

\begin{acks}
  Thanks to Professor Andrew Appel for being extremely generous with his
  time and his guidance. He was always available for questions, and
  allowed me to focus my work on the areas I found interesting.

  I'd also like to thank Matthew Weaver, Lennart Berringer, and Qinshi
  Wang for answering my questions throughout the year. Zachary Kincaid
  provided useful comments on a draft of this paper.

  I'd like to thank Benjamin Huang, Yael Marans, and Eli Waksbaum for
  their help and support.
\end{acks}

\bibliography{draft_paper}

\appendix

\section{Insertion sort}
\label{appendix:insertion_sort}

Insertion sort is a simple and elegant stable sort that is easy to
reason about. The theorems proved in \cref{sec:unique} allow us to
prove things using insertion sort that apply to any stable sort. For
example, our proof of correctness for radixsort relies on the fact
that it uses insertion sort as the inner sort, but
\cref{thm:stablesort_unique} allows us to extend that proof to a
radixsort that uses any stable sort. We also use it as the
implementation of \var{lexsort} because it is convenient, but
similarly \cref{thm:sort_ord_unique} ensures that it can be replaced with
any other sort.

\begin{lstlisting}
  Fixpoint insert (x : A) (l : list A) :=
  match l with
  | [] => [x]
  | h :: t =>
  if le_dec x h then x :: h :: t else h :: insert x t
  end.

  Fixpoint sort (l : list A) : list A :=
  match l with
  | [] => []
  | h :: t => insert h (sort t)
  end.
\end{lstlisting}

\begin{lemma}[insert\_perm]
  For all lists $l$ and elements $x$,
  \begin{equation*}
    \var{Permutation}~(x::l)~(\var{insert}~x~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $l$. In the inductive case, we need to show
  \begin{equation*}
    \var{Permutation}~(x::h::t)~(\var{insert}~x~(h::t))
  \end{equation*}
  given the induction hypothesis that
  \begin{equation*}
    \var{Permutation}~(x::t)~(\var{insert}~x~t)
  \end{equation*}
  If $x \le h$, then $\var{insert}~x~(h::t) = x :: h :: t$ and we are
  done. Otherwise, $\var{insert}~x~(h::t) = h :: \var{insert}~x~t$ and
  we can apply the induction hypothesis.
\end{proof}

\begin{theorem}[sort\_perm]
  For all lists $l$,
  \begin{equation*}
    \var{Permutation~l}~(\var{sort}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof follows by induction on $l$. The inductive case is
  \begin{equation*}
    h :: t \eqperm h :: \var{sort}~t \eqperm
    \var{insert}~h~(\var{sort}~t) = \var{sort}~(h :: t)
  \end{equation*}
  where the first step follows from the induction hypothesis, and the
  second from \var{insert\_perm}.
\end{proof}

\begin{lemma}[insert\_sorted]
  For all lists $l$ and elements $x$,
  \begin{equation*}
    \var{Sorted}~l \implies \var{Sorted}~(\var{insert}~x~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on the proof that $\var{Sorted}~l$. In the
  inductive case we need prove that
  \begin{equation*}
    \var{Sorted}~(\var{insert}~x~(h::t))
  \end{equation*}
  If $x \le h$, then $\var{insert}~x~(h::t) = x :: h :: t$, and by
  \var{SortedLocal\_cons} we are done.

  If $x > h$, $\var{insert}~x~(h::t) = h :: \var{insert}~x~t$, and it
  suffices to show that $\forall y \in \var{insert}~x~t,~h \le y$. By
  \var{insert\_perm}, this is equivalent to $\forall y \in x::t,~h \le y$ From
  the the fact that $\var{Sorted}~(h::t)$, we know that $\forall y \in t,~h \le
  y$, and we have that $h < x$, so we are done.
\end{proof}

\begin{theorem}[sort\_sorted]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~(\var{sort}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof follows by induction on $l$ and \var{insert\_sorted}.
\end{proof}

\begin{lemma}[insert\_stable]
  For all lists $l$ and elements $x$,
  \begin{equation*}
    \var{StablePerm}~(x::l)~(\var{insert}~x~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $l$. In the inductive case, we need to show
  \begin{equation*}
    \var{StablePerm}~(x::h::t)~(\var{insert}~x~(h::t))
  \end{equation*}
  given the induction hypothesis that
  \begin{equation*}
    \var{StablePerm}~(x::t)~(\var{insert}~x~t)
  \end{equation*}
  If $x \le h$, then $\var{insert}~x~(h::t) = x :: h :: t$ and we are
  done. Otherwise, $x > h$ and $\var{insert}~x~(h::t) = h ::
  \var{insert}~x~t$.
  \begin{equation*}
    x :: h :: t \eqstable h :: x :: t \eqstable h :: \var{insert}~x~t
  \end{equation*}
  where the first step follows from the fact that $x \not\equiv h$ and the
  second from the induction hypothesis.
\end{proof}

\begin{theorem}[sort\_stable]
  For all lists $l$,
  \begin{equation*}
    \var{StablePerm}~l~(\var{sort}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof is almost identical to the proof for \var{sort\_perm}.
\end{proof}

\begin{theorem}[sort\_StableSort]
  \begin{equation*}
    \var{StableSort}~\var{sort}
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows from \var{sort\_sorted} and \var{sort\_stable}.
\end{proof}

\section{\var{hdsorted\_sorted\_S}}
\label{appendix:hdsort_sorted_S}

\begin{lemma}[insert\_Sorted\_S]
  For all matrices $m$, rows $r$, and naturals $j$,
  \begin{gather*}
    \var{PrefixSorted}~(j+1)~(\var{hdsort}~m) \implies \\
    \var{PrefixSorted}~j~(\var{tl}~r :: \var{map}~{tl}~m) \implies \\
    \var{PrefixSorted}~(j+1)~(\var{insert}_1~r~(\var{hdsort}~m))
  \end{gather*}
  where $\var{insert}_1$ is \var{insert} that uses the 1-prefix
  lexicographic ordering used by \var{hdsort}.
\end{lemma}
\begin{proof}
  We can destruct $\var{insert}_1~r~(\var{hdsort}~m)$ as $m_1 \dplus r
  :: m_2$, where $m_1 \dplus m_2 = \var{hdsort}~m$. Then by
  \var{Sorted\_app\_cons}, it suffices to show
  \begin{gather*}
    \var{PrefixSorted}~(j+1)~m_1 \land \\
    \var{PrefixSorted}~(j+1)~(r :: m_2) \land \\
    (\forall x \in m_1,~ r >_{j+1} x)
  \end{gather*}
  \begin{itemize}
  \item $\var{PrefixSorted}~(j+1)~m_1$ follows from the fact that
    $\var{hdsort}~m = m_1 \dplus m_2$ and
    $\var{PrefixSorted}~(j+1)~(\var{hdsort}~m)$.
  \item By similar reasoning, we know that
    $\var{PrefixSorted}~(j+1)~m_2$, so in order to show
    $\var{PrefixSorted}~(j+1)~(r :: m_2)$ all we need to prove is that
    for every row $x$ in $m_2$,
    \begin{equation*}
      r \le_{j+1} x
    \end{equation*}
    We know from the fact that $\var{insert}_1~r~(\var{hdsort}~m) =
    m_1 \dplus r :: m_2$ that $r \le_1 x$. Then by \var{key\_firstn\_S},
    we only need to show that
    \begin{equation*}
      \var{tl}~r \le_j \var{tl}~x
    \end{equation*}
    This follows from $\var{PrefixSorted}~j~(\var{tl}~r :: \var{map}~{tl}~m)$.
  \item We have that for any $x \in m_1$, $r >_1 x$ by the fact that
    $\var{insert}_1~r~(\var{hdsort}~m) = m_1 \dplus r :: m_2$. This
    gives us $r >_{j+1} x$ by \var{key\_lt\_firstn\_ge}. \qedhere
  \end{itemize}
\end{proof}

\hdsortsortedS
\begin{proof}
  This follows by induction from \var{insert\_Sorted\_S}
\end{proof}

\section{\var{Permutation} definitions equivalence}
\label{appendix:perm_def_eq}

We know we will eventually need to reason inductively about
permutation functions, so we would like to define the relationship
between applying a permutation function $i :: p$ and applying $p$.
Conceptually, the first element of $\var{apply}~(i::p)~l$ will be
$l_i$. The rest will be the result of applying some new permutation
$p'$, where everything greater than $i$ has been shifted down by one,
to some new list $l'$, which has its $i$th element removed. So, we
define \var{rem\_PermFun} and \var{rem\_nth} respectively, hoping to
prove that
\begin{equation*}
  \var{apply}~(i::p)~l =
  l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l)
\end{equation*}
\begin{lstlisting}
  Definition rem_PermFun i :=
  map (fun j => if lt_dec i j then pred j else j).

  Fixpoint rem_nth i l :=
  match l with
  | [] => []
  | h :: t =>
  match i with
  | 0 => t
  | S i' => h :: rem_nth i' t
  end
  end.
\end{lstlisting}

These two theorems define how \var{rem\_nth} affects indexing.
\begin{theorem}[nth\_lt\_rem\_nth, nth\_ge\_rem\_nth]
  For all lists $l$ and indices $i$ and $j$,
  \begin{equation*}
    j < i \implies (\var{rem\_nth}~i~l)_j = l_j
  \end{equation*}
  and
  \begin{equation*}
    j \ge i \implies (\var{rem\_nth}~i~l)_j = l_{j+1}
  \end{equation*}
\end{theorem}
\begin{proof}
  Both theorems follow by induction on $i$.
\end{proof}

As we expect, removing the $n$th element and sticking it on the front
preserves all elements.
\begin{theorem}[rem\_nth\_Perm]
  For all list $l$ and indices $i$
  \begin{equation*}
    \var{Permutation}~l~(l_i :: \var{rem\_nth}~i~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows from induction on $l$.
\end{proof}

\begin{theorem}[rem\_PermFun\_correct]
  For all permutation functions $p$, lists $l$, and indices $i$.
  \begin{equation*}
    \var{apply}~(i::p)~l =
    l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  Working backwards from our goal,
  \begin{align*}
    \var{apply}~(i::p)~l &=
    l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l) \\
    l_i :: \var{map}~(\lambda j.~l_j)~p &=
    l_i :: \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_j)~(\var{rem\_PermFun}~i~p) \\
    \var{map}~(\lambda j.~l_j)~p &=
    \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_j)\\
    &\phantom{=} \quad (\var{map}~(\lambda j. \bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j)~p) \\
    \var{map}~(\lambda j.~l_j)~p &=
    \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_{\bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j})~p \\
  \end{align*}
  From here, we need to prove that the mapping functions agree on all
  the elements of $p$, namely that for all $j \in p$
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_{\bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j}
  \end{equation*}
  We know that $i \neq j$, because $j \in p$ and $\var{NoDup}~(i :: p)$.
  Therefore there are two cases, $i < j$ and $i > j$. When $i < j$, we have
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_{j-1}
  \end{equation*}
  by \var{nth\_ge\_rem\_nth}, and when $i > j$, we have
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_j
  \end{equation*}
  by \var{nth\_lt\_rem\_nth}.
\end{proof}

\begin{theorem}[apply\_correct]
  For all permutation functions $p$ and lists $l$,
  \begin{equation*}
    \var{Permutation}~(\var{apply}~p~l)~l
  \end{equation*}
\end{theorem}
\begin{proof}
  First we let $n = \var{length}~l$, so that we can reason by
  induction on $n$. The base case is uninteresting. In the inductive
  case, we have the induction hypothesis that for $p :
  \var{list}~\mathbb{N}$ and $l : \var{list}~A$
  \begin{equation*}
    \var{lenth}~l = n \implies \var{PermFun}~n~p \implies
    \var{Permutation}~(\var{apply}~p~l)~l
  \end{equation*}
  Given $\var{PermFun}~(n+1)~(i::p')$, we must show that
  \begin{equation*}
    \var{Permutation}~(\var{apply}~(i :: p')~(a :: l))~(a :: l)
  \end{equation*}

  We reason as follows, using the notation $\eqperm$ to
  chain together \var{Permutation}s:
  \begin{align*}
    &\var{apply}~(i :: p')~(a :: l) \\
    \eqperm{}& \text{\{ by \var{rem\_PermFun\_correct} \}} \\
    &(a :: l)_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~(a::l))
    \\
    \eqperm{}& \text{\{ by IH \}} \\
    &(a :: l)_i :: \var{rem\_nth}~i~(a::l)
    \\
    \eqperm{}& \text{\{ \var{rem\_nth\_Perm} \}} \\
    & a :: l \qedhere
  \end{align*}
\end{proof}

\begin{theorem}[PermutationEx\_iff]
  \label{thm:permutationex_iff}
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{Permutation}~l~l' \iff \var{PermutationEx}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  The $\impliedby$ direction follows from \var{apply\_correct}.

  ($\implies$) We reason by induction on the evidence of
  $\var{Permutation}~l~l'$. The empty case is simple.
  \begin{itemize}
  \item We have a permutation function $p$ from the induction
    hypothesis such that $\var{apply}~p~l = l'$, and we need to
    construct a permutation function $p'$ such that
    $\var{apply}~p'~(x::l) = x :: l'$. We let $p' = 0 ::
    \var{map}~(+1)~p$, shifting every index up by one and mapping
    the first element of the list to itself.
  \item We have to construct a permutation function $p$ such that
    $\var{apply}~p~(y :: x :: l) = x :: y :: l$. We let $p = 1 :: 0 ::
    \var{map}~(+2)~(\var{seq}~0~(\var{length}~l))$, leaving every
    element in place except for the first 2.
  \item We have permutation functions $p_1$ and $p_2$ from our two
    induction hypotheses, such that
    \begin{align*}
      \var{apply}~p_1~l = l' && \var{apply}~p_2~l' = l''
    \end{align*}
    We need to construct a $p'$ such that $\var{apply}~p'~l=l''$, so
    we let $p' = p_2 \comp p_1$. For the definition of composition of
    permutations, see \cref{appendix:perm_comp}. Then,
    \begin{equation*}
      \var{apply}~(p_2 \comp p_1)~l =
      \var{apply}~p_2~(\var{apply}~p_1~l) = l'' \qedhere
    \end{equation*}
  \end{itemize}
\end{proof}

\section{\var{StablePerm} definitions equivalence}
\label{appendix:stableperm_def_eq}

\begin{lemma}[StablePerm\_app]
  For all lists $l$, $l'$, $m$ and $m'$,
  \begin{gather*}
    \var{StablePerm}~l~l' \implies \var{StablePerm}~m~m' \implies \\
    \var{StablePerm}~(l \dplus m)~(l' \dplus m')
  \end{gather*}
\end{lemma}
\begin{proof}
  This follows from the distributivity of \var{filter} over \var{app}.
\end{proof}

\begin{lemma}[StablePerm\_skip]
  For all lists $l$ and $l'$, and elements $a$,
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{StablePerm}~(a::l)~(a::l')
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show $\var{filter}~(\equiv x)~(a::l) =
  \var{filter}~(\equiv x)~(a::l')$. If $x \equiv a$, $a$ is added to both lists,
  and if $x \not\equiv a$ it is added to neither.
\end{proof}

\begin{lemma}[StablePerm\_swap]
  For all lists $l$ and $l'$, and elements $a$ and $b$ where $a \not\equiv b$,
  \begin{equation*}
    \var{StablePerm}~(a::b::l)~(b::a::l)
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show $\var{filter}~(\equiv x)~(a::b::l) =
  \var{filter}~(\equiv x)~(b::a::l)$. The proof follows from case analysis
  on $x \equiv a$ and $x \equiv b$.
\end{proof}

\begin{lemma}[StablePerm\_cons\_app]
  For all lists $l$, $l_1$ and $l_2$, and elements $a$ such that $\forall b
  \in l_1,~a \not\equiv b$,
  \begin{equation*}
    \var{StablePerm}~l~(l_1 \dplus l_2) \implies \var{StablePerm}~(a
    :: l)~(l_1 \dplus a :: l_2)
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show
  \begin{equation*}
    \var{filter}~(\equiv x)~(a::l) = \var{filter}~(\equiv x)~(l_1 \dplus a :: l_2)
  \end{equation*}
  If $x \not\equiv a$ the proof is easy, so assume $x \equiv a$. Then we have
  \begin{align*}
    \var{filter}~(\equiv x)~(l_1 \dplus a :: l_2)
    &= \var{filter}~(\equiv x)~l_1 \dplus \var{filter}~(a :: l_2) \\
    &= [] \dplus a::\var{filter}~l_2 \\
    &= a::\var{filter}~l
  \end{align*}
  where we know that $\var{filter}~(\equiv x)~l_1 = []$ because $\forall b \in
  l_1,~a \not\equiv b$, and the last step follows from
  $\var{StablePerm}~l~(l_1 \dplus l_2)$.
\end{proof}

\begin{theorem}[StablePerm\_destr]
  For all lists $l$ and $l'$, and elements $h$ and $h'$ such that $h
  \not\equiv h'$,
  \begin{gather*}
    \var{StablePerm}~(h :: l)~(h' :: l') \implies \\
    \exists~l_1~l_2, ~l = l_1 \dplus h' :: l_2 \land (\forall x \in l_1, h' \not\equiv x)
  \end{gather*}
\end{theorem}
\begin{proof}
  Let $l_1 = \var{take\_while}~(\not\equiv h')~l$ and $l_2 =
  \var{drop\_while}~(\not\equiv h')~l$. Clearly $l = l_1 \dplus l_2$, and
  because $\var{StablePerm}~(h :: l)~(h' :: l')$, we know the first
  element of $l_2$ must be $h'$. Let $l_2'$ be the rest of $l_2$, and
  we have two lists $l_1$ and $l_2'$ that satisfy the theorem.
\end{proof}

\begin{theorem}[StablePerm\_length]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{length}~l = \var{length}~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $n = \var{length}~l$. In the inductive
  case, we need to show that $\var{length}~(h::t) =
  \var{length}~(h'::t')$ given that $\var{StablePerm}~(h::t)~(h'::t')$
  and the induction hypothesis that for all lists of length $n$,
  \var{StablePerm} implies their lengths are equal.

  If $h \equiv h'$, then from $\var{StablePerm}~(h::t)~(h'::t')$ we know
  that $h = h'$. We can then remove $h$ from the front of $h::t$ and
  $h::t'$ and apply the induction hypothesis.

  If $h \not\equiv h'$, we can destructure $t$ and $t'$ as $l_1 \dplus h'
  :: l_2$ and $l_1' \dplus h :: l_2'$ respectively, using
  \var{StablePerm\_destr}. Then,
  \begin{equation*}
    \var{length}~t
    = \var{length}~(h' :: l_1  \dplus l_2)
    = \var{length}~(h  :: l_1' \dplus l_2')
    =\var{length}~t'
  \end{equation*}
  where the first and last steps follow from the induction hypothesis,
  using the fact that $\var{StablePerm}~t~(h'::l_1 \dplus l_2)$ and
  $\var{StablePerm}~t'~(h::l_1' \dplus l_2')$.
\end{proof}

\begin{theorem}[StablePerm\_Perm]
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{Permutation}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by strong induction on the length of both lists (we know
  the lengths are equal by \var{StablePerm\_length}). In the inductive
  case, we need to show $\var{Permutation}~(h :: t)~(h' :: t')$. If $h
  \equiv h$, we know that $h = h'$, and the proof follows by
  \var{perm\_skip}.

  If $h \not\equiv$, we again destructure $t$ and $t'$ as $l_1 \dplus h' ::
  l_2$ and $l_1' \dplus h :: l_2'$ respectively, using
  \var{StablePerm\_destr}.
  \begin{align*}
    h::t ={}& h :: l_1 \dplus h' :: l_2 \\
    \eqperm{}& h :: h' :: l_1 \dplus l_2 \\
    \eqperm{}& h' :: h :: l_1 \dplus l_2 \\
    \eqperm{}& h' :: h :: l_1' \dplus l_2' \\
    \eqperm{}& h' :: l_1' \dplus h :: l_2' \\
    ={}& h'::t
  \end{align*}
  The crucial step is $l_1 \dplus l_2 \eqperm l_1' \dplus l_2'$, which
  follows from the induction hypothesis.
\end{proof}

\begin{theorem}[StablePermInd\_iff]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \iff \var{StablePermInd}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  The ($\impliedby$) direction follows easily by induction on
  $\var{StablePermInd}~l~l'$, using \var{StablePerm\_skip},
  \var{StablePerm\_swap}, and \var{StablePerm\_trans}.

  The ($\implies$) direction is proved very similarly to
  \var{StablePerm\_Perm}, using \var{StablePerm\_destr} to split up
  the lists.
\end{proof}

Now we prove equivalence with \var{StablePermEx}.

\begin{theorem}[apply\_correct\_stable]
  \label{thm:apply_correct_stable}
  For all lists $l$ and permutation functions $p$,
  \begin{equation*}
    \var{StablePermFun}~l~p \implies \var{StablePerm}~(\var{apply}~p~l)~l
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the length of $l$. In the inductive case,
  we need to show
  \begin{equation*}
    \var{StablePerm}~(\var{apply}~(i :: p)~(a :: l))~(a :: l)
  \end{equation*}

  Using $\eqstable$ to chain \var{StablePerm}s transitively,
  \begin{align*}
    {}& \var{apply}~(i :: p)~(a :: l) \\
    ={}& (a :: l)_i  ::
    \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~(a :: l)) \\
    \eqstable{}& (a :: l)_i :: \var{rem\_nth}~i~(a :: l) \\
    \eqstable{}& (a :: l)_i
  \end{align*}
  where we rely on the induction hypothesis between the first and
  second lines.
\end{proof}

\begin{theorem}[StablePermEx\_iff]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \iff \var{StablePermEx}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We prove $\var{StablePermInd}~l~l' \iff \var{StablePermEx}~l~l'$.

  The $\implies$ direction is very similar to the $\implies$ direction
  of the \var{PermutationEx\_iff} proof: we reason by induction on
  $\var{StablePermInd}~l~l'$, constructing the same permutation
  functions in each case, but this time we have to prove that it is
  stable as well. The $\impliedby$ direction follows from
  \cref{thm:apply_correct_stable}.
\end{proof}


\section{Permutation composition}
\label{appendix:perm_comp}

In this section, we define \var{compose} to compose permutations,
which we will denote using $\comp$ in proofs. We prove the following
property:

\begin{theorem}[compose\_apply]
  For all lists $l$ and permutation functions $p_1$ and $p_2$
  \begin{equation*}
    \var{apply}~(p_2 \comp p_1)~l = \var{apply}~p_2~(\var{apply}~p_1~l)
  \end{equation*}
\end{theorem}

Because we represent the permutations as lists of outputs, we can
simply apply one permutation to the other permutation to compose them.
\begin{lstlisting}
  Definition compose p2 p1 := apply p2 p1.
\end{lstlisting}
This definition, while simple, does not make it obvious that
\var{compose\_apply} holds.
\begin{equation}
  \label{eq:compose_apply}
  \var{apply}~(\var{apply}~p_2~p_1)~l = \var{apply}~p_2~(\var{apply}~p_1~l)
\end{equation}

Conceptually, the theorem works because when we permute the list $l$,
we can think of it as permuting the original indices $\iota =
[0,~1,~\ldots,~n-1]$. If we rephrase \cref{eq:compose_apply} the effect on
on $I$, we get
\begin{equation*}
  \var{apply}~(\var{apply}~p_2~p_1)~\iota = \var{apply}~p_2~(\var{apply}~p_1~\iota)
\end{equation*}
This equation clearly holds if we can prove that $\iota$ is the identity
of \var{apply}, giving as an outline of a proof.

We will use the function \var{combine} defined in the Coq standard
library to annotate every element of $l$ with its index.
\begin{lstlisting}
  Fixpoint combine (l : list A) (l' : list B)
  : list (A*B) :=
  match l,l' with
  | x :: tl, y :: tl' => (x,y) :: (combine tl tl')
  | _, _ => nil
  end.
\end{lstlisting}

To formalize the idea that we can reason about the indices of the
original list $l$ in place of $l$, we need the following theorem.
Intuitively, it says that if we have two lists of pairs, if we know
that the $x$-coordinates are equal and we know that the list of pairs
are permutations of one another, then we know the $y$-coordinates must
be equal as well, because the permutation condition guarantees that
each $x$-coordinate is paired with the same $y$-coordinate in both
lists.
\begin{theorem}[Permutation\_combine\_eq]
  \label{thm:Permutation_combine_eq}
  For all lists $xs$, $ys_1$, and $ys_2$ with all the same length,
  if $xs$ has no duplicates then
  \begin{equation*}
    \var{Permutation}~(\var{combine}~xs~ys_1)~(\var{combine}~xs~ys_2) \implies ys_1 = ys_2
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the list $\var{combine}~xs~ys_1$. In the
  inductive case, we have to show that
  \begin{equation*}
    y_1 :: ys_1 = y_2 :: ys_2
  \end{equation*}
  given that
  \begin{equation*}
    (x,~y_1) :: \var{combine}~xs~ys_1 \eqperm (x, y_2) :: \var{combine}~xs~ys_2
  \end{equation*}
  This means that $(x, y_1) \in (x, y_2) :: \var{combine}~xs~ys_2$, so
  either $(x, y_1) = (x, y_2)$, in which case we are done, or $(x,
  y_1) \in \var{combine}~xs~ys_2$. In the latter case, we have a
  contradiction, because we know that $x :: xs$ contains no
  duplicates, so $x$ cannot be in $xs$.
\end{proof}

We also need to show that \var{apply} distributes over \var{combine}:
\begin{theorem}[apply\_combine]
  For all lists $xs$ and $ys$ with equal length,
  \begin{equation*}
    \var{apply}~p~(\var{combine}~xs~ys) =
    \var{combine}~(\var{apply}~p~xs)~(\var{apply}~p~ys)
  \end{equation*}
\end{theorem}
\begin{proof}
  Let $n = \var{length}~xs = \var{length}~ys$. We reason by induction
  on n. In the inductive case, we have to show
  \begin{align*}
    & \var{apply}~(i::p)~(\var{combine}~(x::xs)~(y::ys)) \\
    ={}& \var{combine}~(\var{apply}~(i::p)~(x::xs))~(\var{apply}~(i::p)~(y::ys))
  \end{align*}
  We can use \var{rem\_PermFun\_correct} to simplify both sides of the
  equation. We have that the heads of both lists must are equal:
  \begin{equation*}
    (\var{combine}~(x::xs)~(y::ys))_i = ((x::xs)_i, (y::ys)_i)
  \end{equation*}
  That the tails are equal follows from the induction hypothesis, so
  we are done.
\end{proof}

Now we show that $\iota$ is the identity of $\var{compose}$/$\var{apply}$.
\begin{theorem}[compose\_id\_l]
  For all permutation functions $p$ on lists of length $n$,
  \begin{equation*}
    \var{compose}~(\var{seq}~0~n)~p = p
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof follows from induction on $p$. In the inductive case,
  \begin{align*}
    {}& \var{compose}~(\var{seq}~0~(n+1))~(i::p) \\
    ={}& \var{map}~(\lambda j.~(i::p)_j)~(\var{seq}~0~(n+1)) \\
    ={}& \var{map}~(\lambda j.~(i::p)_j)~(0 :: \var{seq}~1~(n+1)) \\
    ={}& i :: \var{map}~(\lambda j.~(i::p)_j)~(\var{seq}~1~(n+1)) \\
    ={}& i :: \var{map}~(\lambda j.~(i::p)_j)~(\var{map}~(\lambda j.~j+1)~(\var{seq}~0~n)) \\
    ={}& i :: \var{map}~(\lambda j.~(i::p)_{j+1})~(\var{seq}~0~n) \\
    ={}& i :: \var{map}~(\lambda j.~p_j)~(\var{seq}~0~n) \\
    ={}& i :: \var{compose}~(\var{seq}~0~n)~p \\
    ={}& i :: p \qedhere
  \end{align*}
\end{proof}

\begin{theorem}[compose\_id\_r]
  For all permutation functions $p$ on lists of length $n$,
  \begin{equation*}
    \var{compose}~p~(\var{seq}~0~n) = p
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{equation*}
    \var{compose}~p~(\var{seq}~0~n)
    = \var{map}~(\lambda j.~(\var{seq}~0~n)_j)~p
    = \var{map}~(\lambda j.~j)~p
    = p \qedhere
  \end{equation*}
\end{proof}

Now we can prove the main theorem:
\begin{theorem}[compose\_apply]
  For all lists $l$ and permutation functions $p_1$ and $p_2$
  \begin{equation*}
    \var{apply}~(p_2 \comp p_1)~l = \var{apply}~p_2~(\var{apply}~p_1~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  Letting $\iota = \var{seq}~0~(\var{length}~l)$, we annotate $l$ with
  indices. By \var{apply\_correct}, we have
  \begin{equation*}
    \var{combine}~\iota~l \eqperm \var{apply}~(p_2 \comp p_1)~(\var{combine}~\iota~l)
  \end{equation*}
  and
  \begin{gather*}
    \var{combine}~\iota~l
    \eqperm
    \var{apply}~p_1~(\var{combine}~\iota~l) \\
    \eqperm
    \var{apply}~p_2~(\var{apply}~p_1~(\var{combine}~\iota~l))
  \end{gather*}
  We combine these by transitivity, and then simplify. For space, we
  denote \var{apply} by only juxtaposition, and \var{combine} by $\oplus$.
  \begin{align*}
    (p_2 \comp p_1)~(\iota \oplus l)
    &\eqperm
    p_2~(p_1~(\iota \oplus l)) \\
    ((p_2 \comp p_1)~\iota) \oplus ((p_2 \comp p_1)~l)
    &\eqperm
    p_2~((p_1~\iota) \oplus (p_1~l)) \\
    (p_2 \comp p_1) \oplus ((p_2 \comp p_1)~l)
    &\eqperm
    p_2~(p_1 \oplus (p_1~l)) \\
    (p_2 \comp p_1) \oplus ((p_2 \comp p_1)~l)
    &\eqperm
    (p_2~p_1)\oplus(p_2~(p_1~l))
  \end{align*}
  Because $p_2 \comp p_1 = p_2~p_1$, we can apply
  \cref{thm:Permutation_combine_eq} to show $(p_2 \comp p_1)~l \eqperm
  p_2~(p_1~l)$
\end{proof}

\end{document}
