\documentclass[sigplan,10pt,anonymous,review]{thesis}

\acmConference
    [CPP'20]
    {ACM SIGPLAN International Conference on Certified Programs and Proofs}
    {January 20--21, 2020}{New Orleans, LA, USA}
\acmYear{2020}
\acmISBN{} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM} \acmDOI{} %
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
\citestyle{acmauthoryear}  %% For author/year citations
%\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].

\begin{document}

\title[Coq Formalization of Stable Sorts, Radix Sort, and BWT]{A Coq
  Formalization of Stable Sorts, Radix Sort, and the Burrows-Wheeler
  Transform}

\author{Jake Waksbaum}
\affiliation{
  \department{Department of Computer Science}
  \institution{Princeton University}
  \city{Princeton}
  \state{New Jersey}
  \postcode{08540}
  \country{USA}
}
\email{waksbaum@princeton.edu}

\author{Andrew W. Appel}
\affiliation{
  \department{Department of Computer Science}
  \institution{Princeton University}
  \city{Princeton}
  \state{New Jersey}
  \postcode{08540}
  \country{USA}
}
\email{appel@princeton.edu}

\begin{abstract}
Stable sorts are an important and fundamental type of sorting
algorithm. Some algorithms, such as radix sorts, rely upon the
stability of a sorting subroutine for their correctness. In other
cases, stable sorts are useful for reasoning about algorithms, often
because any two stable sorts are interchangeable. One such algorithm
is the Burrows--Wheeler transform (BWT), which has applications in
compression, full-text indexing, and pattern searching.

There has not been much work on formally verifying stable sorts, or
rather, the stability of stable sorts. While Bird
et~al.\ \cite{birdmu,pearls} provide an elegant derivation and proof
for the BWT, formalizing it in Coq reveals that they rely on a few
lemmas that are not trivial to prove, including the correctness of
least-significant digit (LSD) radix sort. We develop the theory of
stable sorts, and use this theory to prove the correctness of LSD
radix sort and thereby the BWT.
\end{abstract}

%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML} <ccs2012>
<concept>
<concept_id>10003752.10003809.10010031.10002975</concept_id>
<concept_desc>Theory of computation~Data compression</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003809.10010031.10010033</concept_id>
<concept_desc>Theory of computation~Sorting and searching</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10010940.10010992.10010998</concept_id>
<concept_desc>Software and its engineering~Formal methods</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Data compression}
\ccsdesc[500]{Theory of computation~Sorting and searching}
\ccsdesc[500]{Software and its engineering~Formal methods}
%% End of generated code

\keywords{Formal methods, Sorting and searching, Data compression}

\maketitle

\section{Introduction}
\label{sec:intro}

Stable sorts are an important and fundamental type of sorting
algorithm. A sort is stable if it preserves the relative order of
equivalent elements. For example, if we are stably sorting the list
$[\var{mix},\ \allowbreak\var{fog},\ \allowbreak\var{age},\ \allowbreak\var{fit}]$
by the first character of each word, we have to produce
$[\var{age},\ \allowbreak\var{fog},\ \allowbreak\var{fit},\ \allowbreak\var{mix}]$
and not
$[\var{age},\ \allowbreak\var{fit},\ \allowbreak\var{fog},\ \allowbreak\var{mix}]$.
Even though both are sorted on the first characters, because \var{fog}
preceded \var{fit} in the original list, it must also precede
\var{fit} in the stably sorted list.

Stability can be directly useful in certain contexts, such as when
sorting a table by a single column. However, stability is also crucial
to the correctness of certain general-purpose algorithms. One such
algorithm, or set of algorithms, is radix sort. Radix sorts are used
to sort data lexicographically, and can be more efficient than
comparison-based sorting methods, both in theory and in practice
\cite{McIlroy93,Bentley:1997:FAS:314161.314321,10.1007/978-3-540-89097-3_3}.
Most radix sorts rely on a some stable sorting subroutine, as does
least-significant-digit (LSD) radix sort (\cref{sec:radix_sort}).

Stable sorts are also important for reasoning about sorting algorithms
more broadly. This is because any two stable sorting algorithms are
equivalent, in the sense that they will produce the same outputs for a
given input. Crucially, this allows us to reason using simple
algorithms and replace them with more efficient algorithms later on.
We make use of this technique in order to reason about LSD radix sort,
as well as the Burrows-Wheeler transform (BWT).

The Burrows--Wheeler transform is an invertible transformation that
makes a string more amenable to compression by other methods
\cite{bw}. Notably, the BWT is the core pass in the popular,
open-source bzip2 compression software \cite{tsai_2016}. In addition,
the BWT has been applied to full-text indexing and pattern searching,
for example in genomics \cite{ferragina_index, dna}.

Bird et~al.\ \cite{alg-of-prog,birdmu} provide an excellent derivation
of the BWT and its inverse based on the technique of program
calculation, in which programs are calculated from their
specifications using algebraic transformations. However, their
derivation leaves a few crucial lemmas unproven, including the
correctness of LSD radix sort.

Although there is a line of work on mergesort stability proofs
\cite{leroy_2018,Sternagel2013,Leino2015,deGouw2014}, these proofs are
not directly applicable to radix sort. In fact, the correctness of LSD
radix sort remains unproven and more generally, nobody has contributed
a general theory for reasoning about stable sorts.

In this paper, we develop such a theory: we give multiple definitions
for permutations and stable permutations and prove them equivalent.
This allows us to choose whichever definition makes the proof easiest.
We also prove that any two sorts are interchangeable.
\begin{restatable*}[StableSort\_unique]{corollary}{stablesortunique}
  \label{thm:stablesort_unique}
  For all stable sorting functions $f, g: \var{list}~A \to
  \var{list}~A$,
  \begin{equation*}
    (\forall l,f~l = g~l)
  \end{equation*}
\end{restatable*}

Throughout our proofs about LSD radix sort and the BWT, we reason
about sorting algorithms whose implementation is irrelevant as long as
they are correct. We could, in principle, rely only on the fact that
they fulfill the specification of a sorting algorithm
(\cref{def:sort}). However, in practice this is difficult, because the
inductive definition of permutations can produce unhelpful induction
hypotheses, especially in the transitive case. Instead, we reason
about specifically insertion sort. By \cref{thm:stablesort_unique} we
can always replace insertion sort (which is slow) with any other
(presumably faster) stable sort.

Finally, we use the correctness of radix sort to reason about the BWT.
Specifically, we implement the functions $\var{bwl} : \var{list}~A \to
\var{list}~A$ and $\var{bwn} : \var{list}~A \to \nat$ that produce
the string and index part of the BWT respectively, and the function
$\var{unbwt} : \nat \to \var{list}~A \to \var{list}~A$ that reverses
the transform. We then prove the following theorem:
\begin{restatable*}[unbwt\_correct]{theorem}{unbwtcorrect}
  \label{thm:unbwt_correct}
  For all lists $l$,
  \begin{equation*}
    \var{unbwt}~(\var{bwn}~l)~(\var{bwl}~l) = l
  \end{equation*}
\end{restatable*}

The implementation is useful in its own right as a functional
specification for a potential verified implementation of a program
that relies on Burrows--Wheeler, such as bzip2. But more generally,
this provides evidence that Bird's approach of program calculation is
useful for proving the correctness of algorithms in a machine-checked
setting. Bird has applied this approach to greedy algorithms, the
Boyer-Moore algorithm, the Knuth-Morris-Pratt algorithm, Sudoku
solvers, arithmetic coding, the Schorr-Waite algorithm, the
Johnson-Trotter algorithm, and many more problems \cite{pearls}.

Every definition and theorem in this paper corresponds directly to a
definition or theorem in our Coq sources, which we provide as
supplementary materials. Definitions are copied verbatim, modulo
whitespace. Theorems and definitions are rephrased for clarity, with
the name of the corresponding Coq theorem given in parentheses. The
only exceptions are \cref{sec:opt} and places where we give
hypothetical definitions to drive the explanation.

\section{Sorting}
\label{sec:sorting}

Whenever we sort a list, we do so with respect to some ordering on its
elements. For example, we could sort a list of students by first name,
last name, birthday, etc. More precisely, for sorting we need a total,
decidable preorder: a \textit{preorder} is a relation $\le$ that is
reflexive and transitive, it is \textit{total} if we can compare any
two elements, and it is \textit{decidable} if we have a procedure that
takes any two elements $x$ and $y$ and tells us if $x \le y$ or $x \ge y$.

Given a preorder, we can define when a list is in ascending order with
respect to that preorder. In this case, as will be the case with other
properties we will discuss, there are multiple reasonable definitions.
Our approach will be to state all the definitions we can think of and
to prove equivalence between them. This increases our confidence that
we have defined things correctly, and allows us to choose whichever
definition we find most convenient for a given proof.

We can define \var{Sorted} in two main ways: an index-based definition
(\var{SortedIx}), and an inductive definition (\var{Sorted}).

\begin{definition}[SortedIx]
  A list $l$ is sorted if for all indices $i$ and $j$,
  \begin{equation*}
    i \le j \implies l_i \le l_j
  \end{equation*}
\end{definition}

We use the notation $l_i$ for the $i$th element of a list, though in
Coq, that is given by $\var{nth}~i~l~d$, where $d:A$ is a default
value in case $i \ge \var{length}~l$.


\begin{definition}[Sorted]
  \label{def:sorted}
  \begin{gather*}
    \infer[\var{Sorted\_nil}]{\var{Sorted}~[]}{}
    \\
    \infer[\var{Sorted\_cons}]{\var{Sorted}~(a :: l)}{%
      (\forall~x \in l,~a < x) & \var{Sorted}~l
    }
  \end{gather*}
\end{definition}

\begin{restatable}[SortedIx\_iff]{theorem}{sortedixiff}
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~l \iff \var{SortedIx}~l
  \end{equation*}
\end{restatable}
\begin{proof}
  See \cref{appendix:sorted}.
\end{proof}

However, a sorting function cannot produce just any sorted list; the
output must also contain the same elements as the original list. To
capture this, we could use the \var{Permutation} relation defined in
the Coq standard library. This relation holds on two lists when they
contain all the same elements, and is defined inductively.
\begin{definition}[Permutation]
  \label{def:permutation}
  \begin{gather*}
    \infer[\var{perm\_nil}]{\var{Permutation}~[]~[]}{}
    \\
    \allowbreak
    \\
    \infer[\var{perm\_skip}]{\var{Permutation}~(x :: l)~(x :: l')}{%
      \var{Permutation}~l~l'
    }
    \\
    \allowbreak
    \\
    \infer[\var{perm\_swap}]{\var{Permutation}~(x :: y :: l)~(y :: x :: l)}{}
    \\
    \allowbreak
    \\
    \infer[\var{perm\_trans}]{\var{Permutation}~l~l''}{
      \var{Permutation}~l~l' & \var{Permutation}~l'~l''
    }
  \end{gather*}
\end{definition}
While this definition is useful, proofs that do induction directly on
the evidence of \var{Permutation} can sometimes get stuck on the
transitive case, hence the remark in the introduction about the
usefulness of \cref{thm:stablesort_unique}.

Before we go further, we should clarify what we mean by the word
\textit{permutation}, as it has two related meanings that are often
used interchangeably. We sometimes say that two lists are permutations
of one other if they contain all of the same elements, perhaps in a
different order, like the lists $l =
[\var{zebra},\ \allowbreak\var{appliance},\ \allowbreak\var{kite}]$
and $l' =
[\var{appliance},\ \allowbreak\var{kite},\ \allowbreak\var{zebra}]$
However, in mathematics we sometimes say that a permutation is
actually the bijection on the indices that defines the correspondence
between the two lists. In this case, we could map indices in $l$ to
indices in $l'$ by mapping $0 \mapsto 2$, mapping $1 \mapsto 0$, and mapping $2 \mapsto
0$. $p$ is the permutation that relates $l$ to $l'$ if for all indices
$i$ and $j$,
\begin{equation*}
  l_i = l'_{p(i)}
\end{equation*}
For consistency with Coq's built-in \var{Permutation} relation, in a
situation like this we will say that $l$ and $l'$ are permutations of
one other, and that $p$ is the \textit{permutation function} that
relates $l$ to $l'$. This suggests an alternative definition for
\var{Permutation}.

\begin{definition}[PermutationEx]
  \label{def:permutation_ex}
  Two lists $l$ and $l'$ are permutations of one another when there
  exists a permutation function $p$ on $n = \var{length}~l$ elements
  such that $p(l) = l'$.
\end{definition}
Here we are overloading the notation $p(l)$ to denote the result of
shuffling a list~$l$ according to a permutation function~$p$. To
define this precisely, we have to first define how we will represent
permutation functions in Coq.

One natural representation for a permutation function~$p$ is a list of
natural numbers, where each element~$i$ of $p$ stands in for the $i$th
element of the original list. For example, the representation of the
permutation mapping $l =
[\var{zebra},\ \allowbreak\var{appliance},\ \allowbreak\var{kite}]$ to
$l' =
[\var{appliance},\ \allowbreak\var{kite},\ \allowbreak\var{zebra}]$
would be $[1, 2, 0]$, because $\var{appliance} = l_1$, $\var{kite} =
l_2$ and $\var{zebra} = l_0$. So, the mathematical permutation
function~$p$ on $n$ elements is represented by the list
$[p^{-1}(0),\ \allowbreak p^{-1}(1),\ \allowbreak,\ldots,\ \allowbreak
  p^{-1}(n-1)]$. This gives us a very elegant definition for the
action of a permutation on lists.
\begin{equation*}
  p(l) \coloneq \var{map}~(\lambda i. l_i)~p
\end{equation*}
We implement this in Coq with \var{apply}. The definition is slightly
complicated by the fact that \var{nth} takes an extra parameter to
return in case $i$ is out of bounds.
\begin{lstlisting}
Definition apply p l : list A :=
  match l with
  | [] => []
  | d :: _ => map (fun i => nth i l d) p
  end.
\end{lstlisting}

Not any list of natural numbers represents a valid permutation
function, because a permutation must be a bijection. We can define the
property $\var{PermFun}~n~p$, which holds when $p : \var{list}~\nat$
represents a permutation function on $n$ elements.
\begin{definition}[PermFun]
  A list~$p$ of natural numbers is a permutation function on $n$
  elements if for all natural numbers $i$, $i \in p \iff i < n$, and if
  $p$ contains no duplicate elements.
\end{definition}

\begin{restatable}[PermutationEx\_iff]{theorem}{permutationexiff}
  \label{thm:permutationex_iff}
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{Permutation}~l~l' \iff \var{PermutationEx}~l~l'
  \end{equation*}
\end{restatable}
\begin{proof}
  See \cref{appendix:perm}.
\end{proof}

We should also mention a third definition for permutations that can be
used on types with decidable equality.
\begin{definition}[PermutationCount]
  \label{def:permutation_count}
  Two lists are permutations of one another when for any $x$,
  \begin{equation*}
    \var{count\_occ}~l~x = \var{count\_occ}~l'~x
  \end{equation*}
  where $\var{count\_occ}~l~x$ counts the number of elements in $l$
  that are equal to $x$.
\end{definition}

\begin{restatable}[PermutationCount\_iff]{theorem}{permutationcountiff}
  \label{thm:permutationcount_iff}
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{Permutation}~l~l' \iff \var{PermutationCount}~l~l'
  \end{equation*}
\end{restatable}
\begin{proof}
  See \cref{appendix:perm}.
\end{proof}

With this we can define the specification for sorting functions.
\begin{definition}[Sort]
  \label{def:sort}
  A function $f : \var{list}~A \to \var{list}~A$ is a sorting function
  when for all inputs $l$,
  \begin{equation*}
    \var{Sorted}~(f~l) \land \var{Permutation}~(f~l)~l
  \end{equation*}
\end{definition}

\section{Stable sorting}
\label{sec:stable_sorting}

Now that we've defined sorts, we can consider stable sorts. Stable
sorts must preserve the relative orders of equivalent elements. This
equivalence comes from the ordering we are using: any total decidable
preorder also gives us a decidable equivalence relation $\equiv$, where $x
\equiv y$ when $x \le y$ and $x \ge y$.

Stable sorts still must produce sorted outputs, but the output cannot
be just any permutation of the input. We need a stricter notion of
a permutation that respects equivalence, which we will call a stable
permutation.
\begin{definition}[StableSort]
  A function $f : \var{list}~A \to \var{list}~A$ is a stable sorting
  function when for all inputs $l$,
  \begin{equation*}
    \var{Sorted}~(f~l) \land \var{StablePerm}~(f~l)~l
  \end{equation*}
\end{definition}

To define stable permutations, we can tweak each of the definitions of
permutations mentioned above. For example, we can modify our
function-based definition.

\begin{definition}[StablePermEx]
  Two lists $l$ and $l'$ are stable permutations of one another when
  there exists a \textit{stable} permutation function~$p$ on $l$ such
  that $p(l) = l'$.
\end{definition}

\begin{definition}[StablePermFun]
  \label{def:stablepermfun}
  A permutation function $p$ is a \textit{stable} permutation function
  on some list $l$ when for all indices $i < j$ where $l_i \equiv l_j$, we
  also have $p(i) < p(j)$.
\end{definition}

We can also modify the inductive definition of permutations to respect
equivalence. The problematic rule is \var{perm\_swap} because we could
potentially swap two equivalent elements. We will restrict it, so that
you can only swap $x$ and $y$ when $x \nequiv y$.

\begin{definition}[StablePermInd]
  \begin{gather*}
    \infer[\var{StablePermInd\_nil}]{\var{StablePermInd}~[]~[]}{}
    \\ \\
    \infer[\var{StablePermInd\_skip}]{\var{StablePermInd}~(x :: l)~(x :: l')}{%
      \var{StablePermInd}~l~l'
    }
    \\ \\
    \infer[\var{StablePermInd\_swap}]
          {\var{StablePermInd}~(x :: y :: l)~(y :: x :: l)}
          {x \not\equiv y}
    \\ \\
    \infer[\var{StablePermInd\_trans}]{\var{StablePermInd}~l~l''}{
      \var{StablePermInd}~l~l' & \var{StablePermInd}~l'~l''
    }
  \end{gather*}
\end{definition}

Finally, we can say that if two lists are in a stable permutation, and
for some $x$ we examine only the the elements equivalent to $x$, then
those elements should appear in the same order in both lists.
\begin{definition}[StablePerm]
\item Two lists $l$ and $l'$ are stable permutations of one another if
  for every $x$,
  \begin{equation*}
    \var{filter}~(\lambda y. y \equiv x)~l = \var{filter}~(\lambda y. y \equiv x)~l'
  \end{equation*}
\end{definition}
In a sense, this is a stricter version of
\cref{def:permutation_count}, because that can be rephrased as
$\var{filter}~(\lambda y. y = x)~l = \var{filter}~(\lambda y. y = x)~l'$.

\begin{restatable}[StablePermEx\_iff]{theorem}{stablepermexiff}
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \iff \var{StablePermEx}~l~l'
  \end{equation*}
\end{restatable}
\begin{proof}
  See \cref{appendix:stability}.
\end{proof}

\begin{restatable}[StablePermInd\_iff]{theorem}{stablepermindiff}
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \iff \var{StablePermInd}~l~l'
  \end{equation*}
\end{restatable}
\begin{proof}
  See \cref{appendix:stability}.
\end{proof}

From these definitions, we can see that stable sorts are restricted in
which permutation they produce. In fact, there is only one stable
permutation that is also sorted.
\begin{theorem}[StablePerm\_Sorted\_eq]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~l \implies \var{Sorted}~l' \implies
    \var{StablePerm}~l~l' \implies l = l'.
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $l$. In the inductive case, we have to
  show that
  \begin{equation*}
    h :: t = h' :: t'
  \end{equation*}
  given $\var{Sorted}~(h :: t)$, $\var{Sorted}~(h' :: t')$, and
  $\var{StablePerm}~(h :: t)~(h' :: t')$, as well as $t = t'$ from the
  induction hypothesis. So it suffices to show $h = h'$.

  First, we can prove that $h \equiv h'$. We know that $h \in h' :: t'$ and
  $h' \in h :: t$ because they are permutations of one another. If $h =
  h'$, then we are done, so assuming $h \neq h'$ we know $h \in t'$ and $h'
  \in t$. Because both $h::t$ and $h'::t'$ are sorted, we know that $h \le
  h'$ and $h' \le h$, implying $h \equiv h'$.

  From this we can prove that $h = h'$. From $\var{StablePerm}~(h ::
  t)~(h' :: t')$ we have
  \begin{align*}
    \var{filter}~(\equiv h)~(h :: t) &= \var{filter}~(\equiv h)~(h' :: t') \\
    h :: \var{filter}~(\equiv h)~t &= h' :: \var{filter}~(\equiv h)~t'
  \end{align*}
  We also have $t = t'$ by the induction hypothesis, so we can
  conclude that $h = h'$.
\end{proof}

This means that any two stable sorts must produce the same results for
every input, and are thus interchangeable.
\stablesortunique

This can be extended even to unstable sorting algorithms when $x \equiv y
\implies x = y$. This isn't always true: for example, if we are
sorting strings by their first character, then $\text{\texttt{fit}} \equiv
\text{\texttt{fog}}$, but clearly $\text{\texttt{fit}} \neq
\text{\texttt{fog}}$. When we do have that the equivalence ($\equiv$)
implies Leibniz equality ($=$), we say that we have an \textit{order},
as we have when we are sorting strings using all of their characters.
Under an order, as opposed to a preorder, equivalent elements are
actually indistinguishable, so you can't tell if they have been
swapped.

\begin{theorem}[all\_perm\_stable]
  If we have an equivalence that implies Leibniz equality, then for
  all lists $l$ and $l'$
  \begin{equation*}
    \var{Permutation}~l~l' \implies \var{StablePerm}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the evidence of $\var{Permutation}~l~l'$.
  Every case other than the \var{perm\_swap} case follows easily.
  In the \var{perm\_swap} case, we must show that
  \begin{equation*}
    \var{StablePerm}~(y :: x :: l)~(x :: y :: l)
  \end{equation*}
  for all $x$ and $y$. If $x \not\equiv y$, we can just apply
  \var{StablePerm\_swap}. If $x \equiv y$, then we know that $x = y$, and
  we can show
  \begin{equation*}
    \var{StablePerm}~(x :: x :: l)~(x :: x :: l)
  \end{equation*}
  by applying \var{perm\_skip} twice.
\end{proof}

\begin{corollary}[Sort\_StableSort\_Ord]
  \label{thm:stablesort_ord}
  When sorting using an order,
  \begin{equation*}
    \var{Sort}~f \iff \var{StableSort}~f
  \end{equation*}
  for any $f$.
\end{corollary}

\begin{corollary}[Sort\_Ord\_unique]
  \label{thm:sort_ord_unique}
  When sorting using an order, for any two sorting function $f, g:
  \var{list}~A \to \var{list}~A$,
  \begin{equation*}
    (\forall l, f~l = g~l)
  \end{equation*}
\end{corollary}

\section{LSD radix sort}
\label{sec:radix_sort}

Least-significant digit (LSD) radix sort is a simple radix sort that
is used to lexicographically sort strings of equal length. We can
define what it means to sort lexicographically.

\begin{definition}[lex\_le]
  Given a preorder~$\le$ on a type $A$, we define the preorder $\lexle$
  on $\var{list}~A$.
  \begin{gather*}
    \infer[\var{lex\_le\_nil}]{[] \lexle ys}{}
    \\
    \\
    \infer[\var{lex\_le\_cons\_lt}]{(x :: xs) \lexle (y :: ys)}{%
      x < y
    }
    \\
    \\
    \infer[\var{lex\_le\_cons\_eq}]{(x :: xs) \lexle (y :: ys)}{%
      x \equiv y & xs \lexle ys
    }
  \end{gather*}
\end{definition}

LSD radix sort operates by conceptually placing all the strings in a
matrix, and then stably sorting on each column, from the end of the
string to the front (\cref{fig:lsd_radixsort}). This works because if
two strings match on a column, the stable sort ``defers'' to the
preexisting ordering, and that preexisting ordering was determined by
sorting by the tail of the strings. This mirrors the way a
lexicographic ordering ``defers'' to the rest of a string when two
strings match on earlier characters.

\begin{figure}
  \centering
  \begin{tt}
    \setlength{\tabcolsep}{0pt}
    \begin{tabular}{c}
    fit \\
    rib \\
    fly \\
    vat \\
    age \\
    mix \\
    row \\
    fog
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    ri&b& \\
    ag&e& \\
    fo&g& \\
    fi&t& \\
    va&t& \\
    mi&x& \\
    fl&y& \\
    ro&w&
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    v&a&t \\
    a&g&e \\
    r&i&b \\
    f&i&t \\
    m&i&x \\
    f&l&y \\
    f&o&g \\
    r&o&w
    \end{tabular}
    $\rightarrow$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &a&ge \\
    &f&it \\
    &f&ly \\
    &f&og \\
    &m&ix \\
    &r&ib \\
    &r&ow \\
    &v&at
    \end{tabular}
  \end{tt}
  \caption{LSD radix sort in-place}
  \label{fig:lsd_radixsort}
\end{figure}

In a functional setting, we can define an LSD radix sort elegantly by
repeatedly moving the last column to the front of the matrix and
sorting on the first column (\cref{fig:lsd_radixsort_func}). First, we
define the right rotation of a list using \var{rrot}, which moves the
last element of a list to the front. Our definition relies on
\var{init}, which drops the last element of a list. Note that in order
to ensure that $\var{last}$ is total, it takes an extra argument to
return in the case of an empty list.
\begin{lstlisting}
Definition rrot (l: list A) : list A :=
  match l with
  | [] => []
  | hd :: tl => last l hd :: init l
  end.

Fixpoint init l : list A :=
  match l with
  | [] | [_] => []
  | hd :: tl => hd :: init tl
  end.
\end{lstlisting}

Now we can define \var{radixsort}, taking the number of columns $n$ as
a parameter.
\begin{equation*}
  \var{radixsort}~m~n \coloneq (\var{hdsort} \comp \var{map}~\var{rrot})^n~m
\end{equation*}
We use the notation $f^i z$ for $\var{rep}~f~i~z$, which implements
repeated function application.

\begin{figure}
  \centering
  \begin{tt}
    \setlength{\tabcolsep}{0pt}
    \begin{tabular}{c}
    fit \\
    rib \\
    fly \\
    vat \\
    age \\
    mix \\
    row \\
    fog
    \end{tabular}
    $\overset{\var{rot}}{\rightarrow}$
    \begin{tabular}{c}
    tfi \\
    bri \\
    yfl \\
    tva \\
    eag \\
    xmi \\
    wro \\
    gfo
    \end{tabular}
    $\overset{\var{sort}}{\rightarrow}$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &b&ri \\
    &e&ag \\
    &g&fo \\
    &t&fi \\
    &t&va \\
    &x&mi \\
    &y&fl \\
    &w&ro
    \end{tabular}
    $\overset{\var{rot}}{\rightarrow}$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    ibr \\
    gea \\
    ogf \\
    itf \\
    atv \\
    ixm \\
    lyf \\
    owr
    \end{tabular}
    $\overset{\var{sort}}{\rightarrow}$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &a&tv \\
    &g&ea \\
    &i&br \\
    &i&tf \\
    &i&xm \\
    &l&yf \\
    &o&gf \\
    &o&wr
    \end{tabular}
    $\overset{\var{rot}}{\rightarrow}$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    vat \\
    age \\
    rib \\
    fit \\
    mix \\
    fly \\
    fog \\
    row
    \end{tabular}
    $\overset{\var{sort}}{\rightarrow}$
    \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &a&ge \\
    &f&it \\
    &f&ly \\
    &f&og \\
    &m&ix \\
    &r&ib \\
    &r&ow \\
    &v&at
    \end{tabular}
  \end{tt}
  \caption{LSD radix sort using rotations}
  \label{fig:lsd_radixsort_func}
\end{figure}

Here \var{hdsort} is an insertion sort
(\cref{appendix:insertion_sort}) that only examines the first
character of each string. We can implement this by defining orderings
that compare elements using some key function.

\begin{definition}[keyOrd]
  Given a preorder on a type~$K$ and a key function~$\var{key} : A \to
  K$, we define a preorder~$\le_{\var{key}}$ on $A$ that compares all elements using
  the key function.
  \begin{equation*}
    x \le_{\var{key}} y \coloneqq (\var{key}~x) \le (\var{key}~y)
  \end{equation*}
\end{definition}

Now we can define prefix lexicographic orderings on lists, where the
lexicographic ordering is only allowed to compare up to the first $n$
elements of two lists.

\begin{definition}[lexle\_firstn]
  We define the relation $\le_n$ on $\var{list}~A$
  such that for all $xs, ys : \var{list}~A$,
  \begin{align*}
    xs \le_n ys &\coloneqq xs \le_{(\var{firstn}~n)} ys \\
              &= (\var{firstn}~n~xs) \lexle (\var{firstn}~n~ys)
  \end{align*}
\end{definition}
We also define $\var{PrefixSorted}~n$ as \var{Sorted} specialized to
the $\le_n$ ordering. \var{hdsort} is insertion sort specialized to the
$\le_1$ ordering.

We would like to prove that radix sort is a stable sort.

\begin{corollary}[radixsort\_correct]
  \label{thm:radixsort_correct}%
  For all matrices $m$ with $n$ columns,
  \begin{equation*}
    \var{Sorted}~(\var{radixsort}~m~n) \land \var{StablePerm}~(\var{radixsort}~m~n)~m
  \end{equation*}
\end{corollary}

For both properties, our approach will be to prove some invariant that
holds after $j$ iterations. For \var{StablePerm}, we can say that
after $j$ iterations of rotating and sorting, the matrix should be a
stable permutation of the original matrix had we rotated $j$ times
without sorting.

\begin{restatable*}[radixsort\_stable\_inv]{theorem}{radixsortstableinv}
  \label{thm:radixsort_stable_inv}
  For all matrices $m$ and $j : \nat$,
  \begin{equation*}
    \var{StablePerm}~(\var{radixsort}~m~j)~((\var{map}~\var{rrot})^j~m)
  \end{equation*}
\end{restatable*}

For \var{Sorted}, the invariant will be that after $j$ iterations, the
matrix is sorted on the first $j$ columns.

\begin{restatable*}[radixsort\_sorted\_inv]{theorem}{radixsortsortedinv}
  \label{thm:radixsort_sorted_inv}
  For all matrices $m$ with rows of length $n$, and $j \le n$,
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{radixsort}~m~j)
  \end{equation*}
\end{restatable*}

To prove both of these invariants, we will have to be able to reason
about the relationship between the various orderings we are using: we
use $\le_1$ in \var{hdsort}, $\le_j$ in $\var{PrefixSorted}~j$, and
$\lexle \approx \le_n$ everywhere else.

In general, let's consider the relationship between two orders $\le_j$
and $\le_k$ such that $j \le k$. If when comparing two strings $x$ and
$y$, in the first $j$ elements we can conclude that $x <_j y$, then
examining more elements can't change that.

\begin{lemma}[key\_lt\_firstn\_ge]
  \label{thm:key_lt_firstn_ge}
  For all $j$ and $k$ such that $j \le k$,
  \begin{equation*}
    x <_j y \implies x <_k y
  \end{equation*}
\end{lemma}

We can rephrase this in terms of $\le$ and $\equiv$.

\begin{corollary}[key\_le\_firstn\_ge]
  For all $j$ and $k$ such that $j \le k$,
  \begin{equation*}
    x \le_k y \implies x \le_j y
  \end{equation*}
\end{corollary}
\begin{proof}
  This is the contrapositive of \cref{thm:key_lt_firstn_ge}.
\end{proof}

\begin{corollary}
  For all $j$ and $k$ such that $j \le k$,
  \begin{equation*}
    x \equiv_k y \implies x \equiv_j y
  \end{equation*}
\end{corollary}

These theorems show that $\equiv_j$ is weaker than $\equiv_k$ in the sense that
it can distinguish between fewer elements. If we consider the
implications this has for stability, stability with respect to a
weaker order is actually a stronger property, because the weaker
order, considering more elements equivalent, is more restrictive about
which swaps it allows.

\begin{lemma}[StablePerm\_weaken]
  \label{thm:stableperm_weaken}
  Suppose we have a two preorders $\le$ and $\le'$ on $A$, and two lists
  $l_1,l_2 : \var{list}~A$, such that for all $x$ and $y$ in $l_1$,
  \begin{equation*}
    x \le' y \implies x \le y.
  \end{equation*}
  That is, $\le'$ is stronger than $\le$.

  Then, denoting by \var{StablePerm} stability relative to $\le$ and
  by $\var{StablePerm}'$ stability relative to $\le'$,
  \begin{equation*}
    \var{StablePerm}~l_1~l_2 \implies \var{StablePerm}'~l_1~l_2
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason inductively on the proof of $\var{StablePerm}~l_1~l_2$.
  The important case is the swap case, where we have to show
  \begin{equation*}
    \var{StablePerm}'~(y :: x :: l)~(x :: y :: l)
  \end{equation*}
  given that $x \not\equiv y$.

  We have that $x \equiv' y \implies x \equiv y$, so $x \not\equiv y \implies x
  \not\equiv' y$.
\end{proof}

Now we can prove the stability invariant.
\radixsortstableinv
\begin{proof}
  We reason by induction on $j$. In the inductive case, we have to prove
  \begin{equation*}
    \var{StablePerm}~(\var{radixsort}~m~(j+1))~((\var{map}~\var{rrot})^{j+1}~m)
  \end{equation*}
  Expanding $\var{radixsort}~m~(j+1)$:
  \begin{align*}
     \var{radixsort}~m~(j+1)
    ={}& (\var{hdsort} \comp \var{map}~\var{rrot})^{j+1}~m \\
    ={}& \var{hdsort}~(\var{map}~\var{rrot}~(\var{hdsort} \comp \var{map}~\var{rrot})^j~m)
  \end{align*}
  By \cref{thm:stableperm_weaken}, the stability relative to $\le_1$
  that is guaranteed by \var{hdsort} implies stability relative to
  $\lexle$, so we have only to show.
  \begin{align*}
    \var{StablePerm}~&(\var{map}~\var{rrot}~(\var{hdsort} \comp \var{map}~\var{rrot})^j~m) \\
                     &((\var{map}~\var{rrot})^{j+1}~m)
  \end{align*}
  By \cref{thm:stableperm_map_rrot} (proved below), we can take off
  the outer $\var{map}~\var{rrot}$ from both sides.
  \begin{equation*}
    \var{StablePerm}~
    ((\var{hdsort} \comp \var{map}~\var{rrot})^j~m)~((\var{map}~\var{rrot})^j~m)
  \end{equation*}
  This is exactly the induction hypothesis.
\end{proof}

\begin{lemma}[StablePerm\_map\_rrot]
  \label{thm:stableperm_map_rrot}
  For all matrices $m$ and $m'$,
  \begin{equation*}
    \var{StablePerm}~m~m' \implies
    \var{StablePerm}~(\var{map}~\var{rrot}~m)~(\var{map}~\var{rrot}~m').
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $\var{StablePerm}~m~m'$. The important
  case is the swap case, where we have to show
  \begin{equation*}
    \var{StablePerm}~(\var{map}~\var{rrot}~(y :: x :: l))~(\var{map}~\var{rrot}~(x :: y :: l))
  \end{equation*}
  given that $x \not\equivle y$.

  Here we can apply the swap rule if we can show that $\var{rrot}~x
  \not\equivle \var{rrot}~y$. This follows from the fact that
  \begin{equation*}
    x \equivle y \iff \var{rrot}~x \equivle \var{rrot}~y \qedhere
  \end{equation*}
\end{proof}

We would also like to prove the sorted invariant
(\cref{thm:radixsort_sorted_inv}). To do so, we need to be able to
reason about the process of adding a column to the front of a sorted
matrix and sorting on that column. Intuitively, we need to know that
if we have a matrix that is sorted on the first $j$ columns, and we
add a new column to the front and sort on it, the result will be
sorted on the first $j+1$ columns.
\begin{restatable}[hdsort\_sorted\_S]{lemma}{hdsortsortedS}
  \label{thm:hdsortsortedS}
  For all matrices $m$ and naturals $j$,
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{map}~\var{tl}~m) \implies
    \var{PrefixSorted}~(j+1)~(\var{hdsort}~m)
  \end{equation*}
\end{restatable}
\begin{proof}
  See \cref{appendix:hdsort_sorted_S}.
\end{proof}

\radixsortsortedinv
\begin{proof}
  We reason by induction on $j$. In the inductive case, we must prove.
  \begin{equation*}
    \var{PrefixSorted}~(j+1)~(\var{radixsort}~m~(j+1))
  \end{equation*}
  We can reason backwards from our goal. Unfolding \var{radixsort},
  \begin{equation*}
    \var{PrefixSorted}~(j+1)~(\var{hdsort}~(\var{map}~\var{rrot}~(\var{radixsort}~m~j)))
  \end{equation*}
  By \cref{thm:hdsortsortedS}, this is true if the tail of the matrix
  was already sorted on the first $j$ columns.
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{map}~\var{tl}~(\var{map}~\var{rrot}~(\var{radixsort}~m~j)))
  \end{equation*}
  Because $\var{tl} \comp \var{rrot} = \var{init}$,
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{map}~\var{init}~(\var{radixsort}~m~j))
  \end{equation*}
  Unfolding $\var{PrefixSorted}~j$,
  \begin{equation*}
    \var{Sorted}~(\var{map}~(\var{firstn}~j)~(\var{map}~\var{init}~(\var{radixsort}~m~j)))
  \end{equation*}
  Here, because $j$ is less than the length of all the rows,
  $\var{firstn}~j \comp \var{init} = \var{firstn}~j$.
  \begin{equation*}
    \var{Sorted}~(\var{map}~(\var{firstn}~j)~(\var{radixsort}~m~j))
  \end{equation*}
  Folding back up $\var{PrefixSorted}~j$, we have the induction hypothesis
  \begin{equation*}
    \var{PrefixSorted}~j~(\var{radixsort}~m~j) \qedhere
  \end{equation*}
\end{proof}

\section{The Burrows-Wheeler transform}
\label{sec:bwt}

The Burrows--Wheeler transform was originally introduced as a pass in a
compression algorithm \cite{bw}, and continues to be used as a part of
the popular, open-source bzip2 compression software \cite{tsai_2016}.
Here we describe a standard compression algorithm built around BWT.
\begin{enumerate}
\item Apply the BWT, which tends to create runs of identical
  characters.
\item Apply Move-to-Front (MTF) encoding, in which a character is
  represented by the number of different characters encountered since
  the last occurrence of this character. This creates many runs of
  zeroes from the runs in the output of BWT. On English texts, often
  50-60\% of the values produced by the MTF pass are zeroes
  \cite{fenwick2007}.
\item Apply Run-Length Encoding (RLE) to encode only the runs of
  zeroes with their lengths, using a bijective base-2 encoding
  \cite{bw-analysis, tsai_2016}.
\item Compress with some zeroth-order coder like a Huffman coder.
\end{enumerate}

The BWT is also at the heart of algorithms that exploit the
relationship between the rotation matrix in the BWT and the suffix
array data structure in order to search for patterns in a compressed
text \cite{ferragina_index}. These techniques have been applied to
enable fast matching of DNA subsequences \cite{dna}.

\begin{figure}
  \centering
  \begin{tt}
  \begin{tabular}{rc}
    0  & abracadabra! \\
    1  & bracadabra!a \\
    2  & racadabra!ab \\
    3  & acadabra!abr \\
    4  & cadabra!abra \\
    5  & adabra!abrac \\
    6  & dabra!abraca \\
    7  & abra!abracad \\
    8  & bra!abracada \\
    9  & ra!abracadab \\
    10 & a!abracadabr \\
    11 & !abracadabra
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{rc}
    0  & !abracadabr\textbf{a} \\
    1  & a!abracadab\textbf{r} \\
    2  & abra!abraca\textbf{d} \\
    \textit{3} & \textit{abracadabra\textbf{!}} \\
    4  & acadabra!ab\textbf{r} \\
    5  & adabra!abra\textbf{c} \\
    6  & bra!abracad\textbf{a} \\
    7  & bracadabra!\textbf{a} \\
    8  & cadabra!abr\textbf{a} \\
    9  & dabra!abrac\textbf{a} \\
    10 & ra!abracada\textbf{b} \\
    11 & racadabra!a\textbf{b}
  \end{tabular}
  \end{tt}
  \caption{BWT acting on \var{abracadabra!}}
  \label{fig:bw_ex}
\end{figure}

To apply the BWT to a string of length $n$:
\begin{enumerate}
\item Form the \(n \times n\) matrix of all cyclic shifts of the
  original string (referred to as the rotation matrix). Starting
  with the original string, each row is formed from the previous one
  by removing the first character and sticking it on the end.
\item Sort the rows lexicographically.
\item Take the last column of the matrix. That string is the result of
  the transformation.
\item Note the index where the original string appears in the sorted
  rotation matrix. This piece of information is necessary to reverse
  the transformation.
\end{enumerate}
As shown in \cref{fig:bw_ex}, applying the algorithm to the string
\var{abracadabra!} produces the new string \var{ard!rcaaabb} and the
index \var{3}.

The BWT tends to produce runs like the three \var{a}s or the two
\var{b}s in \var{ard!rcaaabb}, and it is this feature that makes the
resulting string easier to compress. It creates those runs by
exploiting the correlation between adjacent characters. In many
real-world inputs, adjacent characters are correlated, which means
that by knowing one character, one can predict the character that
follows it with some success. Sorting the rotation matrix
lexicographically brings together rows that start with the same
characters. But, the beginning and end of each row were adjacent in
the original string, so this will also tend to bring together rows
that end with the same characters.

\section{Inverting the BWT}
\label{sec:invert_bwt}

Although it is not obvious, the transformed string and the index
provide enough information to recover the the original string. The
crucial insight is that from the transformed string, which is the last
column of the sorted rotation matrix, we can reconstruct the entire
sorted rotation matrix. From there, we can find the row that contains
the original string using the index.

\begin{figure}
  $
  {\var{abra\textbf{c}}}%
  {\underbrace{\var{adabra\textcolor{gray}{!abrac}}}}%
  {\var{\textcolor{gray}{adabra!}}}
  $
  \caption{The last column of the sorted rotation matrix contains each
    character of the original string, sorted by the $n$ characters
    that follow it in the original string. This shows the $n$
    characters that follow \var{c} are \var{adabra!abrac}, hence that
    is ``\var{c}'s rotation''.}
  \label{fig:last_col_key}
\end{figure}

How, though, does the last column encapsulate all the information in
the sorted rotation matrix? First, we can notice that each column is a
permutation of the original string. In particular, the last column
contains each character of the original string, sorted by ``its
rotation'': the $n$ characters that follow it in the original string
(\cref{fig:last_col_key}).

We can gain more insight if we examine how the last column is created
when sorting the rotation matrix with an LSD radix sort as in
\cref{fig:radixsort_rots} (this is justified by
\cref{thm:sort_ord_unique}). After $j$ iterations, the column directly
to the left of the shaded column (the ``next column'') contains all
the characters in the original string sorted by the $j$ characters
that follow it in the original string.

\begin{figure*}
  \centering
  \begin{tt}
  \setlength{\tabcolsep}{0pt}
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    abracadabra&!& \\
    bracadabra!&a& \\
    cadabra!abr&a& \\
    dabra!abrac&a& \\
    bra!abracad&a& \\
    !abracadabr&a& \\
    racadabra!a&b& \\
    ra!abracada&b& \\
    adabra!abra&c& \\
    abra!abraca&d& \\
    acadabra!ab&r& \\
    a!abracadab&r&
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    racadabra&!&ab \\
    bracadabr&a&!a \\
    acadabra!&a&br \\
    a!abracad&a&br \\
    dabra!abr&a&ca \\
    bra!abrac&a&da \\
    cadabra!a&b&ra \\
    !abracada&b&ra \\
    abra!abra&c&ad \\
    ra!abraca&d&ab \\
    abracadab&r&a! \\
    adabra!ab&r&ac
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    acadabra&!&abr \\
    racadabr&a&!ab \\
    cadabra!&a&bra \\
    !abracad&a&bra \\
    abra!abr&a&cad \\
    ra!abrac&a&dab \\
    abracada&b&ra! \\
    adabra!a&b&rac \\
    bra!abra&c&ada \\
    a!abraca&d&abr \\
    bracadab&r&a!a \\
    dabra!ab&r&aca
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    a&!&abracadabr \\
    r&a&!abracadab \\
    d&a&bra!abraca \\
    !&a&bracadabra \\
    r&a&cadabra!ab \\
    c&a&dabra!abra \\
    a&b&ra!abracad \\
    a&b&racadabra! \\
    a&c&adabra!abr \\
    a&d&abra!abrac \\
    b&r&a!abracada \\
    b&r&acadabra!a
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &!&abracadabra \\
    &a&!abracadabr \\
    &a&bra!abracad \\
    &a&bracadabra! \\
    &a&cadabra!abr \\
    &a&dabra!abrac \\
    &b&ra!abracada \\
    &b&racadabra!a \\
    &c&adabra!abra \\
    &d&abra!abraca \\
    &r&a!abracadab \\
    &r&acadabra!ab
  \end{tabular}
  \end{tt}
  \caption{Sorting the rotation matrix with LSD radix sort. The
    highlighted column is the last column that was sorted on.}
  \label{fig:radixsort_rots}
\end{figure*}

The ``next-column'' converges to the last column as the matrix is
sorted, because on each iteration it can look one character further in
the string. The last column has full knowledge because it can look at
all $n$ characters. For example, in \cref{fig:radixsort_rots} we can
see that the ``next column'' after three iterations is
\var{ar!drcaaaabb}, which is very similar to the last column at the
end of the process, \var{ard!rcaaaabb}. The only difference is the
\var{!} and \var{d} are swapped, because the three characters that
follow them are the same (\var{abr}), so we can't tell yet which
should go first. By the end, we can see that \var{d}, whose rotation
is \var{abra!abracad}, should go before \var{!}, whose rotation is
\var{abracadabra!}.

From this perspective, the last column is the best ``next column'' for
the LSD radix sort, and we can use it at every iteration
(\cref{fig:recreate}). This gives us a way to recover the entire
sorted rotation matrix from the last column~$s$: starting with the
matrix as just a single column $s$, sort the matrix so far by the
first column, then prepend $s$ and repeat. As the matrix grows from
right to left, we are mimicking the process of LSD radix sort in
\cref{fig:radixsort_rots} in that the sorted portions to the right of
the shaded column are identical.

\begin{figure*}
  \centering
  \begin{tt}
  \setlength{\tabcolsep}{0pt}
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{aaaaaaaaaaa}&!& \\
    \textcolor{lightgray}{rrrrrrrrrrr}&a& \\
    \textcolor{lightgray}{ddddddddddd}&a& \\
    \textcolor{lightgray}{!!!!!!!!!!!}&a& \\
    \textcolor{lightgray}{rrrrrrrrrrr}&a& \\
    \textcolor{lightgray}{ccccccccccc}&a& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&b& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&b& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&c& \\
    \textcolor{lightgray}{aaaaaaaaaaa}&d& \\
    \textcolor{lightgray}{bbbbbbbbbbb}&r& \\
    \textcolor{lightgray}{bbbbbbbbbbb}&r&
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{aaaaaaaaa}&!&ab \\
    \textcolor{lightgray}{rrrrrrrrr}&a&!a \\
    \textcolor{lightgray}{ddddddddd}&a&br \\
    \textcolor{lightgray}{!!!!!!!!!}&a&br \\
    \textcolor{lightgray}{rrrrrrrrr}&a&ca \\
    \textcolor{lightgray}{ccccccccc}&a&da \\
    \textcolor{lightgray}{aaaaaaaaa}&b&ra \\
    \textcolor{lightgray}{aaaaaaaaa}&b&ra \\
    \textcolor{lightgray}{aaaaaaaaa}&c&ad \\
    \textcolor{lightgray}{aaaaaaaaa}&d&ab \\
    \textcolor{lightgray}{bbbbbbbbb}&r&a! \\
    \textcolor{lightgray}{bbbbbbbbb}&r&ac
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{aaaaaaaa}&!&abr \\
    \textcolor{lightgray}{rrrrrrrr}&a&!ab \\
    \textcolor{lightgray}{dddddddd}&a&bra \\
    \textcolor{lightgray}{!!!!!!!!}&a&bra \\
    \textcolor{lightgray}{rrrrrrrr}&a&cad \\
    \textcolor{lightgray}{cccccccc}&a&dab \\
    \textcolor{lightgray}{aaaaaaaa}&b&ra! \\
    \textcolor{lightgray}{aaaaaaaa}&b&rac \\
    \textcolor{lightgray}{aaaaaaaa}&c&ada \\
    \textcolor{lightgray}{aaaaaaaa}&d&abr \\
    \textcolor{lightgray}{bbbbbbbb}&r&a!a \\
    \textcolor{lightgray}{bbbbbbbb}&r&aca
  \end{tabular}
  $\rightarrow \cdots \rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    \textcolor{lightgray}{a}&!&abracadabr \\
    \textcolor{lightgray}{r}&a&!abracadab \\
    \textcolor{lightgray}{d}&a&bra!abraca \\
    \textcolor{lightgray}{!}&a&bracadabra \\
    \textcolor{lightgray}{r}&a&cadabra!ab \\
    \textcolor{lightgray}{c}&a&dabra!abra \\
    \textcolor{lightgray}{a}&b&ra!abracad \\
    \textcolor{lightgray}{a}&b&racadabra! \\
    \textcolor{lightgray}{a}&c&adabra!abr \\
    \textcolor{lightgray}{a}&d&abra!abrac \\
    \textcolor{lightgray}{b}&r&a!abracada \\
    \textcolor{lightgray}{b}&r&acadabra!a
  \end{tabular}
  $\rightarrow$
  \begin{tabular}{c>{\columncolor[gray]{0.9}}cc}
    &!&abracadabra \\
    &a&!abracadabr \\
    &a&bra!abracad \\
    &a&bracadabra! \\
    &a&cadabra!abr \\
    &a&dabra!abrac \\
    &b&ra!abracada \\
    &b&racadabra!a \\
    &c&adabra!abra \\
    &d&abra!abraca \\
    &r&a!abracadab \\
    &r&acadabra!ab
  \end{tabular}
  \end{tt}
  \caption{Recreating the rotation matrix from the last column: at
    every iteration, we prepend the result of the BWT and sort on it.}
  \label{fig:recreate}
\end{figure*}

This intuition is related to the following lemma.
\begin{restatable*}[lexsort\_rots\_hdsort]{lemma}{lexsortrotshdsort}
  For all lists $l$,
  \begin{equation*}
    (\var{hdsort} \comp
    \var{map}~\var{rrot})~(\var{lexsort}~(\var{rots}~l)) =
    \var{lexsort}~(\var{rots}~l)
  \end{equation*}
\end{restatable*}
This lemma says that if we take the last column of the sorted rotation
matrix and move it to the front, and then sort on it, we produce the
same sorted rotation matrix that we started with. This lemma means
that when we repeatedly prepend $s$ and sort by it, we are preserving
the rest of the sorted rotation matrix, even the parts we have yet to
reconstruct.

\section{Implementing the forwards BWT}
\label{sec:forwards_BWT}

First, we define the left rotation of a list. \var{lrot} moves the
first element of a list to the end.
\begin{lstlisting}
Definition lrot (l: list A) : list A :=
  match l with
  | [] => []
  | hd :: tl => tl ++ [hd]
  end.
\end{lstlisting}
Now we produce the rotation matrix by repeatedly applying \var{lrot}.
We will use \var{iter}, which takes a function $f : A \to A$, a number
$n$, and an initial value $z$, and returns a list of length $n$ formed
by repeatedly applying $f$ to $z$:
\begin{equation*}
  \var{iter}~f~n~z = [z,~f~z,~f~(f~z),~\ldots,~f^{n-1} z]
  \label{eq:iter}
\end{equation*}
\begin{lstlisting}
Fixpoint iter (f : A -> A) (n : nat) (z : A)
  : list A :=
  match n with
  | O   => []
  | S m => z :: iter f m (f z)
  end.
\end{lstlisting}
Now we can define \var{rots} to produce the rotation matrix.
\begin{lstlisting}
Definition rots (l : list A) : list (list A) :=
  iter lrot (length l) l.
\end{lstlisting}

\var{bwl} produces the transformed string by taking the last column of
the sorted rotation matrix. \var{lexsort} lexicographically sorts the
rotation matrix using insertion sort (\cref{appendix:insertion_sort}).
\begin{lstlisting}
Definition bwl (l : list A) : list A :=
  match l with
  | [] => []
  | hd :: _ =>
    List.map (fun x => last x hd) (lexsort (rots l))
  end.
\end{lstlisting}

\var{bwn} returns the index in the sorted rotation matrix where the
original string appears. It relies on \var{findIndex}, which searches
a list for an element $y$ that is equal to the given element $x$, and
returns the index of $y$.
\begin{lstlisting}
Fixpoint findIndex (x : A) (ls : list A) : nat :=
  match ls with
  | [] => 0
  | hd :: tl =>
    if x == hd then 0
    else S (findIndex x tl)
  end.

Definition bwn (l : list A) : nat :=
  findIndex l (lexsort (rots l)).
\end{lstlisting}

Given these definitions, we can quickly prove that \var{bwn} satisfies
its specification.

\begin{lemma}[findIndex\_correct]
  For all elements $x$ and lists $l$,
  \begin{equation*}
    (\exists y \in l,~x \equiv y) \implies l_{(\var{findIndex}~x~xs)} \equiv x
  \end{equation*}
\end{lemma}
\begin{proof}
  This follows by induction on $l$: At every step, either the
  current element is equivalent to $x$ and we return $0$, or there
  must be such a $y$ in the rest of the list.
\end{proof}

\begin{lemma}[orig\_in\_sorted\_rots]
  For every non-empty list $l$,
  \begin{equation*}
    \exists y \in \var{lexsort}~(\var{rots}~l),~x = y
  \end{equation*}
\end{lemma}
\begin{proof}
  Clearly $l$ is in $\var{rots}~l$ at index 0, and because
  \var{lexsort} produces a permutation of its input, $l$ must be
  in $\var{lexsort}\ \allowbreak(\var{rots}~l)$ as well.
\end{proof}

\begin{restatable}[bwn\_correct]{theorem}{bwncorrect}
  \label{thm:bwn_correct}
  For every non-empty list $xs$,
  \begin{equation*}
    (\var{lexsort}~(\var{rots}~xs))_{(\var{bwn}~xs)} = xs
  \end{equation*}
\end{restatable}
\begin{proof}
  This follows from \var{findIndex\_correct} if we can show that
  $\exists y \in xs,~x = y$, which we have by \var{orig\_in\_sorted\_rots}.
\end{proof}

\section{Implementing the inverse BWT}
\label{sec:inverse_BWT}

As outlined in \cref{sec:invert_bwt}, we recreate the sorted rotation
matrix from the last column~$l$ using \var{recreate}. It takes a
parameter $j : \nat$ so that we can recursively recreate the first $j$
columns of the matrix, one column at a time.
\begin{lstlisting}
Fixpoint recreate (j : nat) (l : list A)
  : list (list A) :=
  match j with
  | O    => map (const []) l
  | S j' => hdsort (prepend_col l (recreate j' l))
  end.

Definition prepend_col
  : list A -> list (list A) -> list (list A)
  := zipWith cons.

Fixpoint zipWith (a : list A) (b : list B)
  : list C :=
  match (a, b) with
  | (ahd :: atl, bhd :: btl) =>
    f ahd bhd :: zipWith atl btl
  | _ => []
  end.
\end{lstlisting}

The specification for \var{recreate} is that it recreates the first
$j$ columns of the sorted rotation matrix. For this we need to define
\var{cols}.
\begin{lstlisting}
Definition cols (j : nat)
  : list (list A) -> list (list A)
  := map (firstn j).
\end{lstlisting}

\begin{restatable*}[recreate\_correct\_inv]{theorem}{recreatecorrectinv}
  For all lists $l$ and $j \le \var{length}~l$,
  \begin{equation*}
  \var{recreate}~j~(\var{bwl}~l) = \var{cols}~j~(\var{lexsort}~(\var{rots}~l))
  \end{equation*}
\end{restatable*}

First, we note that moving the last column of the rotation matrix to
the front is the same as moving the last row to the top, because the
rotation matrix is symmetric.
\begin{lemma}[map\_rrot\_rots]
  For all lists $l$,
  \begin{equation*}
    \var{map}~\var{rrot}~(\var{rots}~l) = \var{rrot}~(\var{rots}~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  Letting $n = \var{length}~l$
  \begin{align*}
       &\var{map}~\var{rrot}~(\var{rots}~l) \\
    ={}&\var{map}~\var{rrot}~[l,~\var{lrot}~l,~\var{lrot}^2~l,~\ldots,~\var{lrot}^{n-1}~l] \\
    ={}&[\var{rrot}~l,~l,~\var{lrot}~l,~\ldots,~\var{lrot}^{n-2}~l] \\
    ={}&[\var{lrot}^{n-1}~l,~l,~\var{lrot}~l,~\ldots,~\var{lrot}^{n-2}~l] \\
    ={}&\var{rrot}~[l,~\var{lrot}~l,~\ldots,~\var{lrot}^{n-1}~l] \\
    ={}&\var{rrot}~(\var{rots}~l) \qedhere
  \end{align*}
\end{proof}

\lexsortrotshdsort
\begin{proof}
  By \cref{thm:sort_ord_unique}, any two sorts on an order are
  equivalent, and we can replace \var{lexsort} with \var{radixsort}.
  \begin{align*}
       &(\var{hdsort} \comp \var{map}~\var{rrot})~(\var{lexsort}~(\var{rots}~l)) \\
    ={}&(\var{hdsort} \comp \var{map}~\var{rrot})~((\var{hdsort} \comp \var{map}~\var{rrot})^n~(\var{rots}~l)) \\
    ={}&(\var{hdsort} \comp \var{map}~\var{rrot})^{n+1}~(\var{rots}~l) \\
    ={}&(\var{hdsort} \comp \var{map}~\var{rrot})^n~((\var{hdsort} \comp \var{map}~\var{rrot})~(\var{rots}~l)) \\
    ={}&\var{lexsort}~((\var{hdsort} \comp \var{map}~\var{rrot})~(\var{rots}~l)) \\
    ={}&\var{lexsort}~((\var{hdsort} \comp \var{rrot})~(\var{rots}~l)) \\
    ={}&\var{lexsort}~(\var{rots}~l)
  \end{align*}
  The last step follows because the function $\var{hdsort} \comp
  \var{rrot}$ permutes the rotation matrix right before it is sorted
  by \var{lexsort}, so it doesn't change the final result.
\end{proof}

\begin{lemma}[cols\_hdsort\_comm]
  For all matrices $m$ and naturals $j$,
  \begin{equation*}
    \var{cols}~(j+1)~(\var{hdsort}~m) = \var{hdsort}~(\var{cols}~(j+1)~m)
  \end{equation*}
\end{lemma}
\begin{proof}
  \var{hdsort} only examines the first elements of the rows of $m$,
  and $\var{cols}~(j+1)$ doesn't alter the first elements of the rows
  of $m$.
\end{proof}

\begin{lemma}[cols\_map\_rrot]
  For all matrices $m$ and $j$ such that for all rows $r$,
  $\var{length}~r > j$,
  \begin{gather*}
    \var{cols}~(j+1)~(\var{map}~\var{rrot}~m) = \\
    \var{prepend\_col}~(\var{map}~\var{last}~m)~(\var{cols}~j~m)
  \end{gather*}
\end{lemma}
\begin{proof}
  We reason by induction on $m$. In the inductive case we need to show that
  \begin{gather*}
    \var{cols}~(j+1)~(\var{map}~\var{rrot}~(r::m)) = \\
    \var{prepend\_col}~(\var{map}~\var{last}~(r::m))~(\var{cols}~j~(r::m))
  \end{gather*}
  Simplifying, we have that the tails are equal by the induction
  hypothesis, and we must show that the heads are equal.
  \begin{align*}
    \var{firstn}~(j+1)~(\var{rrot}~r) &= \var{last}~r :: \var{firstn}~j~r \\
    \var{firstn}~(j+1)~(\var{last}~r :: \var{init}~r) &= \var{last}~r
    :: \var{firstn}~j~r \\
    \var{last}~r :: \var{firstn}~j~(\var{init}~r) &= \var{last}~r :: \var{firstn}~j~r \\
    \var{firstn}~j~(\var{init}~r) &= \var{firstn}~j~r
  \end{align*}
  The bottom equation is true because $j > \var{length}~r$, so if $j=
  0$, $r = []$.
\end{proof}

Now we are ready to prove the invariant.
\recreatecorrectinv
\begin{proof}
  We reason by induction on $j$. Letting $m =
  \var{lexsort}~(\var{rots}~l)$,
  \begin{align*}
    &\var{cols}~(j+1)~m \\
    ={}& \var{cols}~(j+1)~(\var{hdsort}~(\var{map}~\var{rrot}~m)) \\
    ={}&
    \var{hdsort}~(\var{cols}~(j+1)~(\var{map}~\var{rrot}~m)) \\
    ={}&
    \var{hdsort}~(\var{prepend\_col}~(\var{map}~\var{last}~m)~(\var{cols}~j~m)) \\
    ={}&
    \var{hdsort}~(\var{prepend\_col}~(\var{bwl}~l)~(\var{recreate}~j~(\var{bwl}~l))) \\
    ={}& \var{recreate}~(j + 1)~(\var{bwl}~l) \qedhere
  \end{align*}
\end{proof}

Once we have the sorted rotation matrix, it is easy to recover the
original string by indexing into the matrix at the correct row.
\begin{lstlisting}
Definition unbwt (i : nat) (l : list A) : list A :=
  nth i (recreate (length l) l) l.
\end{lstlisting}

\begin{theorem}[cols\_id]
  For all matrices $m$ such that all the rows of $m$ have length less
  than or equal to $n$,
  \begin{equation*}
    \var{cols}~n~m = m
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows by induction on $m$.
\end{proof}

\unbwtcorrect
\begin{proof}
\begin{align*}
     \var{unbwt}~(\var{bwn}~l)~(\var{bwl}~l)
  &= (\var{recreate}~(\var{length}~l))(\var{bwl}~l))_{(\var{bwn})~l}\\
  &= (\var{cols}~(\var{length}~l)~(\var{lexsort}~(\var{rots}~l)))_{(\var{bwn})~l} \\
  &= (\var{lexsort}~(\var{rots}~l))_{(\var{bwn})~l} \\
  &= l \qedhere
\end{align*}
\end{proof}

\section{Relationship to the standard BWT}
\label{sec:opt}

Standard implementations of the BWT do not actually reconstruct the
entire rotation matrix, as this would be inefficient. They avoid this
using the optimization described in this section. Although, we did not
prove or implement this optimization in Coq, it would be
straightforward to do so.

In \var{recreate}, we repeatedly prepend the last column to the front
of the matrix and sort on it. However, each time that column is the
same, so the permutation that takes us from the unsorted column to the
sorted column is the same. Therefore, we can calculate that
permutation function once and store it in a variable $p$. The $i$th
column of the recreated matrix is simply $(\var{apply}~p)^i~f$ where
$f$ is the first column of the sorted rotation matrix, so we can
define
\begin{lstlisting}
Definition recreate' (p : list nat) (l : list A)
  : list (list A) :=
  let f := apply p l in
  transpose (iter (apply p) (length l) f)
\end{lstlisting}

Since in $\var{unbwt}~i~l$, we want the $i$th row of the matrix, we
can eliminate the transpose by replacing the $\var{nth}~i$ with
$\var{map}~\var{nth}~i$ Assuming we already have calculated $p$,
\begin{align*}
  \var{unbwt}~i~l &= (\var{recreate}~p~l)_i \\
  &= (\var{transpose}~(\var{iter}~(\var{apply}~p)~(\var{length}~l)~(\var{apply}~p~l)))_i \\
  &= \var{map}~(\lambda
  r.~r_i)~(\var{iter}~(\var{apply}~p)~(\var{length}~l)~(\var{apply}~p~l)) \\
  &= \var{map}~(\lambda r.~r_i)~(\var{tl}~(\var{iter}~(\var{apply}~p)~(1 +
  \var{length}~l)~l)) \\
  &= \var{tl}~(\var{map}~(\lambda r.~r_i)~(\var{iter}~(\var{apply}~p)~(1 + \var{length}~l)~l))
\end{align*}

Now, instead of indexing into each column at $i$, where the columns
are all the result of successively applying $p$ to $l$, we will
successively take the image of $i$ under $p$ to generate the indices
into $l$ where those characters appear.
\begin{lstlisting}
Definition image (p : list nat) i := findIndex i p.
\end{lstlisting}
This is justified via the following theorem.
\begin{theorem}
  For all $n$,
  \begin{equation*}
    \var{map}~(\lambda r.~r_i)~(\var{iter}~(\var{apply}~p)~n~l)
    =
    \var{map}~(\lambda j.~l_j)~(\var{iter}~(\var{image}~p)~n~i)
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{align*}
    & \var{map}~(\lambda r.~r_i)~(\var{iter}~(\var{apply}~p)~n~l) \\
    ={}& \var{map}~(\lambda r.~r_i)~[l,~(\var{apply}~p)~l,~(\var{apply}~p)^2~l,~\ldots,~(\var{apply}~p)^{n-1}~l] \\
    ={}& [l_i,~((\var{apply}~p)~l)_i,~((\var{apply}~p)^2~l)_i,~\ldots,~((\var{apply}~p)^{n-1}~l)_i] \\
    ={}& [l_i,~l_{p(i)},~l_{p^2(i)},~\ldots,~l_{p^{n-1}~i}] \\
    ={}& \var{map}~(\lambda j.~l_j)~[i,~p(i),~p^2(i),~\ldots,~p^{n-1}~i] \\
    ={}& \var{map}~(\lambda j.~l_j)~(\var{iter}~(\var{image}~p)~n~i) \qedhere
  \end{align*}
\end{proof}

So assuming we have some function $\var{calc\_sort\_perm} :
\var{list}~A \to \var{list}~\nat$ to calculate the permutation
function that sorts $l$, we redefine \var{unbwt} as
\begin{lstlisting}
Definition unbwt' (i : nat) (l : list A) : list A :=
  match l with
  | [] => []
  | d :: _ =>
    let p := calc_sort_perm l in
    let indices :=
      tl (iter (image p) (S (length l)) i)
    in map (fun j => nth l j d) indices
\end{lstlisting}
\var{calc\_sort\_perm} can be implemented in linear time using a
counting sort, so if \var{nth} took constant time, then \var{unbwt}
would take linear time as well. As a functional specification, this
closely matches the standard imperative algorithm for the inverse BWT.

\section{Conclusions}
We have implemented and proved that the the inverse BWT inverts the
forwards BWT, basing our work on Bird et~al.'s program calculation
derivation \cite{birdmu,pearls}. In the process, we defined stable
sorts, proved that all stable sorts are equivalent, and implemented
and proved correct an LSD radix sort.

This paper shows that Bird's style of program calculation is very
useful for building machine-checked proofs. It leads to proofs that
are composed of many smaller lemmas, each justifying a single rewrite
step. Most of the time and effort went into proving the correctness of
LSD radix sort. If we exclude the parts related to sorting or to other
general-purpose functions not specific to the BWT, the entire verified
implementation of the BWT consists of 42 lines of Gallina definitions,
7 lines of Gallina that are used only in proofs, and 823 lines of
theorem statements and Ltac code.\footnote{This includes
  \var{BurrowsWheeler.v}, \var{Columns.v}, \var{Rotations/Rotation.v},
  \var{Rotations/Rots.v}, \var{Lib/Iterate.v}, \var{Lib/Repeat.v} and
  \var{Lib/FindIndex.v}, which implement \var{bwl}, \var{bwn},
  \var{unbwt}, \var{cols}, \var{prepend\_col}, \var{lrot}, \var{rrot},
  \var{rots}, \var{iter}, and \var{rep}, \var{findIndex}.}

This implementation, including the optimizations mentioned in
\cref{sec:opt}, can be used as a functional specification to for a
verified implementation of bzip2 \cite{tsai_2016}. Of course, it would
also be necessary to have functional specifications for the other
phases of bzip2, such as Run-Length Encoding, Move-to-Front Encoding,
and arithmetic coding. As luck would have it, Bird has already
analyzed arithmetic coding \cite{pearls}, and the other two phases are
relatively simple.

\begin{acks}
  Thanks to Professor Andrew Appel for being extremely generous with his
  time and his guidance. He was always available for questions, and
  allowed me to focus my work on the areas I found interesting.

  I'd also like to thank Matthew Weaver, Lennart Berringer, and Qinshi
  Wang for answering my questions throughout the year. Zachary Kincaid
  provided useful comments on a draft of this paper.

  I'd like to thank Benjamin Huang, Yael Marans, and Eli Waksbaum for
  their help and support.
\end{acks}

\nocite{*}
\bibliography{draft_paper}

\clearpage

\appendix

\section{Sorted}
\label{appendix:sorted}

There is a third definition for \var{Sorted} that is very similar to
\cref{def:sorted}, but uses transitivity to avoid parametrizing over
every element in the list.
\begin{definition}[SortedLocal]
  \begin{gather*}
    \infer[\var{SortedLocal\_nil}]{\var{SortedLocal}~[]}{}
    \\
    \\
    \infer[\var{SortedLocal\_1}]{\var{SortedLocal}~[a]}{}
    \\
    \\
    \infer[\var{SortedLocal\_cons}]{\var{SortedLocal}~(a :: b :: l)}{%
      a < b & \var{SortedLocal}~(b :: l)
    }
  \end{gather*}
\end{definition}

\begin{theorem}[SortedLocal\_iff]
  \label{thm:sortedlocal_iff}
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~l \iff \var{SortedLocal}~l
  \end{equation*}
\end{theorem}
\begin{proof}
  Both directions follow by induction on $l$, making use of the
  transitivity of $\le$.
\end{proof}

\sortedixiff*
\begin{proof}
  We reason by induction on $l$. The base cases are easy, so we
  address the inductive case, $l = a :: l'$

  ($\implies$) We have to prove that $(a :: l')_i \le (a :: l')_j$ given
  $i \le j$ and $\var{Sorted}~(a::l)$. We reason by cases on $i$ and $j$:
  \begin{itemize}
  \item $i = j = 0$. By reflexivity, $a \le a$.
  \item $i = 0,~j=j' + 1$. We need to show $a \le l'_{j'}$. We know that
    $a \le x$ for all $x \in l'$ by $\var{Sorted}~(a::l')$, so we are
    done.
  \item $i = i' +1,~j = j' + 1$. We need to show $l'_{i'} \le l'_{j'}$,
    which follows from the induction hypothesis.
  \end{itemize}

  ($\impliedby$) We have $\var{SortedIx}\ \allowbreak(a :: l')$, and
  we need to show $\var{Sorted}\ \allowbreak(a :: l')$. By
  \cref{thm:sortedlocal_iff}, we can just prove
  $\var{SortedLocal}\ \allowbreak(a :: l')$. If $l' = []$, we can
  apply $\var{SortedLocal\_1}$, so assume $l' = b :: l''$. To apply
  \var{SortedLocal\_cons} we need to show that $a \le b$. That follows
  because $a = (a :: b :: l'')_0$ and $b = (a :: b :: l'')_1$, so by
  $\var{SortedIx}\ \allowbreak(a :: b :: l'')$, we are done.
\end{proof}

\section{Permutation}
\label{appendix:perm}

In this section we sometimes use the notation $\eqperm$ to chain
\var{Permutation} relationships transitively.

\permutationcountiff*
\begin{proof}
  The ($\implies$) direction follows from induction in the evidence of
  $\var{Permutation}~l~l'$.

  In the ($\impliedby$) direction, we reason by induction on $l$. In
  the inductive case, we have to show $\var{Permutation}~(a::l)~l'$.
  We know that $a \in l'$ because $\var{count\_occ}~a~l' =
  \var{count\_occ}~a~(a::l) > 0$. That means we can deconstruct $l'$
  as $L \dplus a :: R$, and
  \begin{equation*}
    L \dplus a :: R \eqperm a :: L \dplus R \eqperm a :: l
  \end{equation*}
  where the last step follows from the induction hypothesis.
\end{proof}

We can define the image and preimage of an index under a permutation
function.
\begin{lstlisting}
Definition image (p : list nat) i := findIndex i p.
Definition preimage (p : list nat) i := nth i p 0.
\end{lstlisting}
We will use the notation $p(i)$ for $\var{image}~i$ and $p^{-1}(i)$
for $\var{preimage}~i$. It should be clear from context whether $p(x)$
denotes $\var{image}~x$ or $\var{apply}~x$, and when it is not we will
use \var{image} and \var{apply} explicitly.

We will not prove it here, but $\var{preimage}~p$ and $\var{image}~p$
are both of bijections of type $\nat_{<n} \to \nat_{<n}$ and are
inverses of each other. We also have the following two theorems:
\begin{theorem}[nth\_preimage\_apply, nth\_image\_apply]
  For all lists $l$, permutations $p$, and indices $i$,
  \begin{align*}
    (\var{apply}~p~l)_{p(i)} = l_i && l_{p^{-1}(i)} = (\var{apply}~p~l)_i
  \end{align*}
\end{theorem}
\begin{proof}
  The first equation (\var{nth\_preimage\_apply}) holds because
  \begin{align*}
       (\var{apply}~p~l)_{p(i)}
    &= (\var{map}~(\lambda j.~l_j)~p)_{\var{findIndex}~i~p} \\
    &= l_{p_{(\var{findIndex}~i~p)}} \\
    &= l_i
  \end{align*}
  and the second follows from the first.
  \begin{equation*}
      (\var{apply}~p~l)_i
    = (\var{apply}~p~l)_{p(p^{-1}(i))}
    = l_{p^{-1}(i)}
    \qedhere
  \end{equation*}
\end{proof}

We know we will eventually need to reason inductively about
permutation functions, so we would like to define the relationship
between applying a permutation function $i :: p$ and applying $p$.
Conceptually, the first element of $\var{apply}~(i::p)~l$ will be
$l_i$. The rest will be the result of applying some new permutation
$p'$, where everything greater than $i$ has been shifted down by one,
to some new list $l'$, which has its $i$th element removed. So, we
define \var{rem\_PermFun} and \var{rem\_nth} respectively, hoping to
prove that
\begin{equation*}
  \var{apply}~(i::p)~l =
  l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l)
\end{equation*}
\begin{lstlisting}
Definition rem_PermFun i :=
  map (fun j => if lt_dec i j then pred j else j).

Fixpoint rem_nth i l :=
  match l with
  | [] => []
  | h :: t =>
    match i with
    | 0 => t
    | S i' => h :: rem_nth i' t
    end
  end.
\end{lstlisting}

These two theorems define how \var{rem\_nth} affects indexing.
\begin{theorem}[nth\_lt\_rem\_nth, nth\_ge\_rem\_nth]
  For all lists $l$ and indices $i$ and $j$,
  \begin{equation*}
    j < i \implies (\var{rem\_nth}~i~l)_j = l_j
  \end{equation*}
  and
  \begin{equation*}
    j \ge i \implies (\var{rem\_nth}~i~l)_j = l_{j+1}
  \end{equation*}
\end{theorem}
\begin{proof}
  Both theorems follow by induction on $i$.
\end{proof}

As we expect, removing the $n$th element and sticking it on the front
preserves all elements.
\begin{theorem}[rem\_nth\_Perm]
  For all list $l$ and indices $i$
  \begin{equation*}
    \var{Permutation}~l~(l_i :: \var{rem\_nth}~i~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows from induction on $l$.
\end{proof}

\begin{theorem}[rem\_PermFun\_correct]
  For all permutation functions $p$, lists $l$, and indices $i$.
  \begin{equation*}
    \var{apply}~(i::p)~l =
    l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  Working backwards from our goal,
  \begin{align*}
    \var{apply}~(i::p)~l &=
    l_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~l) \\
    l_i :: \var{map}~(\lambda j.~l_j)~p &=
    l_i :: \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_j)~(\var{rem\_PermFun}~i~p) \\
    \var{map}~(\lambda j.~l_j)~p &=
    \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_j)\\
    &\phantom{=} \quad (\var{map}~(\lambda j. \bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j)~p) \\
    \var{map}~(\lambda j.~l_j)~p &=
    \var{map}~(\lambda j.~(\var{rem\_nth}~i~l)_{\bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j})~p \\
  \end{align*}
  From here, we need to prove that the mapping functions agree on all
  the elements of $p$, namely that for all $j \in p$
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_{\bvar{if}~i < j~\bvar{then}~j-1~\bvar{else}~j}
  \end{equation*}
  We know that $i \neq j$, because $j \in p$ and $\var{NoDup}~(i :: p)$.
  Therefore there are two cases, $i < j$ and $i > j$. When $i < j$, we have
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_{j-1}
  \end{equation*}
  by \var{nth\_ge\_rem\_nth}, and when $i > j$, we have
  \begin{equation*}
    l_j = (\var{rem\_nth}~i~l)_j
  \end{equation*}
  by \var{nth\_lt\_rem\_nth}.
\end{proof}

\begin{theorem}[apply\_correct]
  For all permutation functions $p$ and lists $l$,
  \begin{equation*}
    \var{Permutation}~(\var{apply}~p~l)~l
  \end{equation*}
\end{theorem}
\begin{proof}
  First we let $n = \var{length}~l$, so that we can reason by
  induction on $n$. The base case is uninteresting. In the inductive
  case, we have the induction hypothesis that for $p :
  \var{list}~\nat$ and $l : \var{list}~A$
  \begin{equation*}
    \var{lenth}~l = n \implies \var{PermFun}~n~p \implies
    \var{Permutation}~(\var{apply}~p~l)~l
  \end{equation*}
  Given $\var{PermFun}~(n+1)~(i::p')$, we must show that
  \begin{equation*}
    \var{Permutation}~(\var{apply}~(i :: p')~(a :: l))~(a :: l)
  \end{equation*}

  \begin{align*}
    &\var{apply}~(i :: p')~(a :: l) \\
    \eqperm{}& \text{\{ by \var{rem\_PermFun\_correct} \}} \\
    &(a :: l)_i :: \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~(a::l))
    \\
    \eqperm{}& \text{\{ by IH \}} \\
    &(a :: l)_i :: \var{rem\_nth}~i~(a::l)
    \\
    \eqperm{}& \text{\{ \var{rem\_nth\_Perm} \}} \\
    & a :: l \qedhere
  \end{align*}
\end{proof}

\permutationexiff*
\begin{proof}
  The $\impliedby$ direction follows from \var{apply\_correct}.

  ($\implies$) We reason by induction on the evidence of
  $\var{Permutation}~l~l'$. The empty case is simple.
  \begin{itemize}
  \item We have a permutation function $p$ from the induction
    hypothesis such that $\var{apply}~p~l = l'$, and we need to
    construct a permutation function $p'$ such that
    $\var{apply}~p'~(x::l) = x :: l'$. We let $p' = 0 ::
    \var{map}~(+1)~p$, shifting every index up by one and mapping
    the first element of the list to itself.
  \item We have to construct a permutation function $p$ such that
    $\var{apply}~p~(y :: x :: l) = x :: y :: l$. We let $p = 1 :: 0 ::
    \var{map}~(+2)~(\var{seq}~0~(\var{length}~l))$, leaving every
    element in place except for the first 2.
  \item We have permutation functions $p_1$ and $p_2$ from our two
    induction hypotheses, such that
    \begin{align*}
      \var{apply}~p_1~l = l' && \var{apply}~p_2~l' = l''
    \end{align*}
    We need to construct a $p'$ such that $\var{apply}~p'~l=l''$, so
    we let $p' = p_2 \comp p_1$. For the definition of composition of
    permutations, see \cref{appendix:perm_comp}. Then,
    \begin{equation*}
      \var{apply}~(p_2 \comp p_1)~l =
      \var{apply}~p_2~(\var{apply}~p_1~l) = l'' \qedhere
    \end{equation*}
  \end{itemize}
\end{proof}

\section{Stability}
\label{appendix:stability}

We defined \var{StablePermFun} in terms of the indices of the original
list~$l$ (\cref{def:stablepermfun}), but we just as easily could have
defined it in terms of the indices of the permuted list~$l'$.

\begin{definition}[StablePermFun\_preimage]
  A permutation function $p$ is a \textit{stable} permutation function
  on some list $l$ when for all indices $i < j$ where $p(l)_i \equiv p(l)_j$, we
  also have $p^{-1}(i) < p^{-1}(j)$.
\end{definition}

\begin{theorem}[StablePermFun\_iff]
  For all lists~$l$ and $p : \var{list}~\nat$,
  \begin{equation*}
    \var{StablePermFun}~l~p \iff \var{StablePermFun\_preimage}~l~p
  \end{equation*}
\end{theorem}
\begin{proof}
  Both directions are very similar, we will prove just the $\implies$
  direction here.

  We have that $\var{StablePermFun}~l~p$, and we have an $i$ and $j$
  such that $i < j$ and $p(l)_i \equiv p(l)_j$. We must show that
  $p^{-1}(i) < p^{-1}(j)$. For the sake of contradiction, suppose
  $p^{-1}(i) \ge p^{-1}(j)$. If $p^{-1}(i) = p^{-1}(j)$, then $i = j$,
  contradicting $i < j$, so suppose $p^{-1}(i) > p^{-1}(j)$. Then,
  since $p(l)_i \equiv p(l)_j$ implies $l_{p^{-1}(i)} \equiv l_{p^{-1}(j)}$,
  from $\var{StablePermFun}~l~p$ we know that $p(p^{-1}(i)) >
  p(p^{-1}(i))$, and therefore that $i > j$. This is also a
  contradiction, so it must be that $p^{-1}(i) < p^{-1}(j)$.
\end{proof}


\begin{lemma}[StablePerm\_app]
  For all lists $l$, $l'$, $m$ and $m'$,
  \begin{gather*}
    \var{StablePerm}~l~l' \implies \var{StablePerm}~m~m' \implies \\
    \var{StablePerm}~(l \dplus m)~(l' \dplus m')
  \end{gather*}
\end{lemma}
\begin{proof}
  This follows from the distributivity of \var{filter} over \var{app}.
\end{proof}

\begin{lemma}[StablePerm\_skip]
  For all lists $l$ and $l'$, and elements $a$,
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{StablePerm}~(a::l)~(a::l')
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show $\var{filter}~(\equiv x)~(a::l) =
  \var{filter}~(\equiv x)~(a::l')$. If $x \equiv a$, $a$ is added to both lists,
  and if $x \not\equiv a$ it is added to neither.
\end{proof}

\begin{lemma}[StablePerm\_swap]
  For all lists $l$ and $l'$, and elements $a$ and $b$ where $a \not\equiv b$,
  \begin{equation*}
    \var{StablePerm}~(a::b::l)~(b::a::l)
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show $\var{filter}~(\equiv x)~(a::b::l) =
  \var{filter}~(\equiv x)~(b::a::l)$. The proof follows from case analysis
  on $x \equiv a$ and $x \equiv b$.
\end{proof}

\begin{lemma}[StablePerm\_cons\_app]
  For all lists $l$, $l_1$ and $l_2$, and elements $a$ such that $\forall b
  \in l_1,~a \not\equiv b$,
  \begin{equation*}
    \var{StablePerm}~l~(l_1 \dplus l_2) \implies \var{StablePerm}~(a
    :: l)~(l_1 \dplus a :: l_2)
  \end{equation*}
\end{lemma}
\begin{proof}
  For any $x$, we have to show
  \begin{equation*}
    \var{filter}~(\equiv x)~(a::l) = \var{filter}~(\equiv x)~(l_1 \dplus a :: l_2)
  \end{equation*}
  If $x \not\equiv a$ the proof is easy, so assume $x \equiv a$. Then we have
  \begin{align*}
    \var{filter}~(\equiv x)~(l_1 \dplus a :: l_2)
    &= \var{filter}~(\equiv x)~l_1 \dplus \var{filter}~(a :: l_2) \\
    &= [] \dplus a::\var{filter}~l_2 \\
    &= a::\var{filter}~l
  \end{align*}
  where we know that $\var{filter}~(\equiv x)~l_1 = []$ because $\forall b \in
  l_1,~a \not\equiv b$, and the last step follows from
  $\var{StablePerm}~l~(l_1 \dplus l_2)$.
\end{proof}

\begin{theorem}[StablePerm\_destr]
  For all lists $l$ and $l'$, and elements $h$ and $h'$ such that $h
  \not\equiv h'$,
  \begin{gather*}
    \var{StablePerm}~(h :: l)~(h' :: l') \implies \\
    \exists~l_1~l_2, ~l = l_1 \dplus h' :: l_2 \land (\forall x \in l_1, h' \not\equiv x)
  \end{gather*}
\end{theorem}
\begin{proof}
  Let $l_1 = \var{take\_while}~(\not\equiv h')~l$ and $l_2 =
  \var{drop\_while}~(\not\equiv h')~l$. Clearly $l = l_1 \dplus l_2$, and
  because $\var{StablePerm}~(h :: l)~(h' :: l')$, we know the first
  element of $l_2$ must be $h'$. Let $l_2'$ be the rest of $l_2$, and
  we have two lists $l_1$ and $l_2'$ that satisfy the theorem.
\end{proof}

\begin{theorem}[StablePerm\_length]
  For all lists $l$ and $l'$
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{length}~l = \var{length}~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on $n = \var{length}~l$. In the inductive
  case, we need to show that $\var{length}~(h::t) =
  \var{length}~(h'::t')$ given that $\var{StablePerm}~(h::t)~(h'::t')$
  and the induction hypothesis that for all lists of length $n$,
  \var{StablePerm} implies their lengths are equal.

  If $h \equiv h'$, then from $\var{StablePerm}~(h::t)~(h'::t')$ we know
  that $h = h'$. We can then remove $h$ from the front of $h::t$ and
  $h::t'$ and apply the induction hypothesis.

  If $h \not\equiv h'$, we can destructure $t$ and $t'$ as $l_1 \dplus h'
  :: l_2$ and $l_1' \dplus h :: l_2'$ respectively, using
  \var{StablePerm\_destr}. Then,
  \begin{equation*}
    \var{length}~t
    = \var{length}~(h' :: l_1  \dplus l_2)
    = \var{length}~(h  :: l_1' \dplus l_2')
    =\var{length}~t'
  \end{equation*}
  where the first and last steps follow from the induction hypothesis,
  using the fact that $\var{StablePerm}~t~(h'::l_1 \dplus l_2)$ and
  $\var{StablePerm}~t'~(h::l_1' \dplus l_2')$.
\end{proof}

\begin{theorem}[StablePerm\_Perm]
  For all lists $l$ and $l'$,
  \begin{equation*}
    \var{StablePerm}~l~l' \implies \var{Permutation}~l~l'
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by strong induction on the length of both lists (we know
  the lengths are equal by \var{StablePerm\_length}). In the inductive
  case, we need to show $\var{Permutation}~(h :: t)~(h' :: t')$. If $h
  \equiv h$, we know that $h = h'$, and the proof follows by
  \var{perm\_skip}.

  If $h \not\equiv$, we again destructure $t$ and $t'$ as $l_1 \dplus h' ::
  l_2$ and $l_1' \dplus h :: l_2'$ respectively, using
  \var{StablePerm\_destr}.
  \begin{align*}
    h::t ={}& h :: l_1 \dplus h' :: l_2 \\
    \eqperm{}& h :: h' :: l_1 \dplus l_2 \\
    \eqperm{}& h' :: h :: l_1 \dplus l_2 \\
    \eqperm{}& h' :: h :: l_1' \dplus l_2' \\
    \eqperm{}& h' :: l_1' \dplus h :: l_2' \\
    ={}& h'::t
  \end{align*}
  The crucial step is $l_1 \dplus l_2 \eqperm l_1' \dplus l_2'$, which
  follows from the induction hypothesis.
\end{proof}

\stablepermindiff*
\begin{proof}
  The ($\impliedby$) direction follows easily by induction on
  $\var{StablePermInd}~l~l'$, using \var{StablePerm\_skip},
  \var{StablePerm\_swap}, and \var{StablePerm\_trans}.

  The ($\implies$) direction is proved very similarly to
  \var{StablePerm\_Perm}, using \var{StablePerm\_destr} to split up
  the lists.
\end{proof}

Now we prove equivalence with \var{StablePermEx}.

\begin{theorem}[apply\_correct\_stable]
  \label{thm:apply_correct_stable}
  For all lists $l$ and permutation functions $p$,
  \begin{equation*}
    \var{StablePermFun}~l~p \implies \var{StablePerm}~(\var{apply}~p~l)~l
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the length of $l$. In the inductive case,
  we need to show
  \begin{equation*}
    \var{StablePerm}~(\var{apply}~(i :: p)~(a :: l))~(a :: l)
  \end{equation*}

  Using $\eqstable$ to chain \var{StablePerm}s transitively,
  \begin{align*}
    {}& \var{apply}~(i :: p)~(a :: l) \\
    ={}& (a :: l)_i  ::
    \var{apply}~(\var{rem\_PermFun}~i~p)~(\var{rem\_nth}~i~(a :: l)) \\
    \eqstable{}& (a :: l)_i :: \var{rem\_nth}~i~(a :: l) \\
    \eqstable{}& (a :: l)_i
  \end{align*}
  where we rely on the induction hypothesis between the first and
  second lines.
\end{proof}

\stablepermexiff*
\begin{proof}
  We prove $\var{StablePermInd}~l~l' \iff \var{StablePermEx}~l~l'$.

  The $\implies$ direction is very similar to the $\implies$ direction
  of the \var{PermutationEx\_iff} proof: we reason by induction on
  $\var{StablePermInd}~l~l'$, constructing the same permutation
  functions in each case, but this time we have to prove that it is
  stable as well. The $\impliedby$ direction follows from
  \cref{thm:apply_correct_stable}.
\end{proof}

\section{Insertion sort}
\label{appendix:insertion_sort}

\begin{lstlisting}
Fixpoint insert (x : A) (l : list A) :=
  match l with
  | [] => [x]
  | h :: t =>
    if le_dec x h
    then x :: h :: t
    else h :: insert x t
  end.

Fixpoint sort (l : list A) : list A :=
  match l with
  | [] => []
  | h :: t => insert h (sort t)
  end.
\end{lstlisting}

\begin{lemma}[insert\_perm]
  For all lists $l$ and elements $x$,
  \begin{equation*}
    \var{Permutation}~(x::l)~(\var{insert}~x~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $l$. In the inductive case, we need to show
  \begin{equation*}
    \var{Permutation}~(x::h::t)~(\var{insert}~x~(h::t))
  \end{equation*}
  given the induction hypothesis that
  \begin{equation*}
    \var{Permutation}~(x::t)~(\var{insert}~x~t)
  \end{equation*}
  If $x \le h$, then $\var{insert}~x~(h::t) = x :: h :: t$ and we are
  done. Otherwise, $\var{insert}~x~(h::t) = h :: \var{insert}~x~t$ and
  we can apply the induction hypothesis.
\end{proof}

\begin{theorem}[sort\_perm]
  For all lists $l$,
  \begin{equation*}
    \var{Permutation~l}~(\var{sort}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof follows by induction on $l$. The inductive case is
  \begin{equation*}
    h :: t \eqperm h :: \var{sort}~t \eqperm
    \var{insert}~h~(\var{sort}~t) = \var{sort}~(h :: t)
  \end{equation*}
  where the first step follows from the induction hypothesis, and the
  second from \var{insert\_perm}.
\end{proof}

\begin{lemma}[insert\_sorted]
  For all lists $l$ and elements $x$,
  \begin{equation*}
    \var{Sorted}~l \implies \var{Sorted}~(\var{insert}~x~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on the proof that $\var{Sorted}~l$. In the
  inductive case we need prove that
  \begin{equation*}
    \var{Sorted}~(\var{insert}~x~(h::t))
  \end{equation*}
  If $x \le h$, then $\var{insert}~x~(h::t) = x :: h :: t$, and by
  \var{SortedLocal\_cons} we are done.

  If $x > h$, $\var{insert}~x~(h::t) = h :: \var{insert}~x~t$, and it
  suffices to show that $\forall y \in \var{insert}~x~t,~h \le y$. By
  \var{insert\_perm}, this is equivalent to $\forall y \in x::t,~h \le y$ From
  the the fact that $\var{Sorted}~(h::t)$, we know that $\forall y \in t,~h \le
  y$, and we have that $h < x$, so we are done.
\end{proof}

\begin{theorem}[sort\_sorted]
  For all lists $l$,
  \begin{equation*}
    \var{Sorted}~(\var{sort}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof follows by induction on $l$ and \var{insert\_sorted}.
\end{proof}

\begin{lemma}[insert\_stable]
  For all lists $l$ and elements $x$,
  \begin{equation*}
    \var{StablePerm}~(x::l)~(\var{insert}~x~l)
  \end{equation*}
\end{lemma}
\begin{proof}
  We reason by induction on $l$. In the inductive case, we need to show
  \begin{equation*}
    \var{StablePerm}~(x::h::t)~(\var{insert}~x~(h::t))
  \end{equation*}
  given the induction hypothesis that
  \begin{equation*}
    \var{StablePerm}~(x::t)~(\var{insert}~x~t)
  \end{equation*}
  If $x \le h$, then $\var{insert}~x~(h::t) = x :: h :: t$ and we are
  done. Otherwise, $x > h$ and $\var{insert}~x~(h::t) = h ::
  \var{insert}~x~t$.
  \begin{equation*}
    x :: h :: t \eqstable h :: x :: t \eqstable h :: \var{insert}~x~t
  \end{equation*}
  where the first step follows from the fact that $x \not\equiv h$ and the
  second from the induction hypothesis.
\end{proof}

\begin{theorem}[sort\_stable]
  For all lists $l$,
  \begin{equation*}
    \var{StablePerm}~l~(\var{sort}~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof is almost identical to the proof for \var{sort\_perm}.
\end{proof}

\begin{theorem}[sort\_StableSort]
  \begin{equation*}
    \var{StableSort}~\var{sort}
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows from \var{sort\_sorted} and \var{sort\_stable}.
\end{proof}

\section{\var{hdsorted\_sorted\_S}}
\label{appendix:hdsort_sorted_S}

\begin{lemma}[insert\_Sorted\_S]
  For all matrices $m$, rows $r$, and naturals $j$,
  \begin{gather*}
    \var{PrefixSorted}~(j+1)~(\var{hdsort}~m) \implies \\
    \var{PrefixSorted}~j~(\var{tl}~r :: \var{map}~{tl}~m) \implies \\
    \var{PrefixSorted}~(j+1)~(\var{insert}_1~r~(\var{hdsort}~m))
  \end{gather*}
  where $\var{insert}_1$ is \var{insert} that uses the 1-prefix
  lexicographic ordering used by \var{hdsort}.
\end{lemma}
\begin{proof}
  We can destruct $\var{insert}_1~r~(\var{hdsort}~m)$ as $m_1 \dplus r
  :: m_2$, where $m_1 \dplus m_2 = \var{hdsort}~m$. Then by
  \var{Sorted\_app\_cons}, it suffices to show
  \begin{gather*}
    \var{PrefixSorted}~(j+1)~m_1 \land \\
    \var{PrefixSorted}~(j+1)~(r :: m_2) \land \\
    (\forall x \in m_1,~ r >_{j+1} x)
  \end{gather*}
  \begin{itemize}
  \item $\var{PrefixSorted}~(j+1)~m_1$ follows from the fact that
    $\var{hdsort}~m = m_1 \dplus m_2$ and
    $\var{PrefixSorted}~(j+1)~(\var{hdsort}~m)$.
  \item By similar reasoning, we know that
    $\var{PrefixSorted}~(j+1)~m_2$, so in order to show
    $\var{PrefixSorted}~(j+1)~(r :: m_2)$ all we need to prove is that
    for every row $x$ in $m_2$,
    \begin{equation*}
      r \le_{j+1} x
    \end{equation*}
    We know from the fact that $\var{insert}_1~r~(\var{hdsort}~m) =
    m_1 \dplus r :: m_2$ that $r \le_1 x$. Then by \var{key\_firstn\_S},
    we only need to show that
    \begin{equation*}
      \var{tl}~r \le_j \var{tl}~x
    \end{equation*}
    This follows from $\var{PrefixSorted}~j~(\var{tl}~r :: \var{map}~{tl}~m)$.
  \item We have that for any $x \in m_1$, $r >_1 x$ by the fact that
    $\var{insert}_1~r~(\var{hdsort}~m) = m_1 \dplus r :: m_2$. This
    gives us $r >_{j+1} x$ by \var{key\_lt\_firstn\_ge}. \qedhere
  \end{itemize}
\end{proof}

\hdsortsortedS*
\begin{proof}
  This follows by induction from \var{insert\_Sorted\_S}
\end{proof}

\section{Permutation composition}
\label{appendix:perm_comp}

In this section, we define \var{compose} to compose permutations,
which we will denote using $\comp$ in proofs. We prove the following
property:

\begin{theorem}[compose\_apply]
  For all lists $l$ and permutation functions $p_1$ and $p_2$
  \begin{equation*}
    \var{apply}~(p_2 \comp p_1)~l = \var{apply}~p_2~(\var{apply}~p_1~l)
  \end{equation*}
\end{theorem}

Because we represent the permutations as lists of outputs, we can
simply apply one permutation to the other permutation to compose them.
\begin{lstlisting}
Definition compose p2 p1 := apply p2 p1.
\end{lstlisting}
This definition, while simple, does not make it obvious that
\var{compose\_apply} holds.
\begin{equation}
  \label{eq:compose_apply}
  \var{apply}~(\var{apply}~p_2~p_1)~l = \var{apply}~p_2~(\var{apply}~p_1~l)
\end{equation}

Conceptually, the theorem works because when we permute the list $l$,
we can think of it as permuting the original indices $\iota =
[0,~1,~\ldots,~n-1]$. We can specialize \cref{eq:compose_apply} in terms of
$\iota$.
\begin{equation*}
  \var{apply}~(\var{apply}~p_2~p_1)~\iota = \var{apply}~p_2~(\var{apply}~p_1~\iota)
\end{equation*}
This equation clearly holds if we can prove that $\iota$ is the identity
of \var{apply}, giving as an outline of a proof.

We will use the function \var{combine} defined in the Coq standard
library to annotate every element of $l$ with its index.
\begin{lstlisting}
Fixpoint combine (l : list A) (l' : list B)
  : list (A*B) :=
  match l,l' with
  | x :: tl, y :: tl' => (x,y) :: (combine tl tl')
  | _, _ => nil
  end.
\end{lstlisting}

To formalize the idea that we can reason about the indices of the
original list $l$ in place of $l$, we need the following theorem.
Intuitively, it says that if we have two lists of pairs, if we know
that the $x$-coordinates are equal and we know that the list of pairs
are permutations of one another, then we know the $y$-coordinates must
be equal as well, because the permutation condition guarantees that
each $x$-coordinate is paired with the same $y$-coordinate in both
lists.
\begin{theorem}[Permutation\_combine\_eq]
  \label{thm:Permutation_combine_eq}
  For all lists $xs$, $ys_1$, and $ys_2$ with all the same length,
  if $xs$ has no duplicates then
  \begin{equation*}
    \var{Permutation}~(\var{combine}~xs~ys_1)~(\var{combine}~xs~ys_2) \implies ys_1 = ys_2
  \end{equation*}
\end{theorem}
\begin{proof}
  We reason by induction on the list $\var{combine}~xs~ys_1$. In the
  inductive case, we have to show that
  \begin{equation*}
    y_1 :: ys_1 = y_2 :: ys_2
  \end{equation*}
  given that
  \begin{equation*}
    (x,~y_1) :: \var{combine}~xs~ys_1 \eqperm (x, y_2) :: \var{combine}~xs~ys_2
  \end{equation*}
  This means that $(x, y_1) \in (x, y_2) :: \var{combine}~xs~ys_2$, so
  either $(x, y_1) = (x, y_2)$, in which case we are done, or $(x,
  y_1) \in \var{combine}~xs~ys_2$. In the latter case, we have a
  contradiction, because we know that $x :: xs$ contains no
  duplicates, so $x$ cannot be in $xs$.
\end{proof}

We also need to show that \var{apply} distributes over \var{combine}.
\begin{theorem}[apply\_combine]
  For all lists $xs$ and $ys$ with equal length,
  \begin{equation*}
    \var{apply}~p~(\var{combine}~xs~ys) =
    \var{combine}~(\var{apply}~p~xs)~(\var{apply}~p~ys)
  \end{equation*}
\end{theorem}
\begin{proof}
  Let $n = \var{length}~xs = \var{length}~ys$. We reason by induction
  on n. In the inductive case, we have to show
  \begin{align*}
    & \var{apply}~(i::p)~(\var{combine}~(x::xs)~(y::ys)) \\
    ={}& \var{combine}~(\var{apply}~(i::p)~(x::xs))~(\var{apply}~(i::p)~(y::ys))
  \end{align*}
  We can use \var{rem\_PermFun\_correct} to simplify both sides of the
  equation. We have that the heads of both lists must are equal.
  \begin{equation*}
    (\var{combine}~(x::xs)~(y::ys))_i = ((x::xs)_i, (y::ys)_i)
  \end{equation*}
  That the tails are equal follows from the induction hypothesis, so
  we are done.
\end{proof}

Now we show that $\iota$ is the identity of $\var{compose}$/$\var{apply}$.
\begin{theorem}[compose\_id\_l]
  For all permutation functions $p$ on lists of length $n$,
  \begin{equation*}
    \var{compose}~(\var{seq}~0~n)~p = p
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof follows from induction on $p$. In the inductive case,
  \begin{align*}
    {}& \var{compose}~(\var{seq}~0~(n+1))~(i::p) \\
    ={}& \var{map}~(\lambda j.~(i::p)_j)~(\var{seq}~0~(n+1)) \\
    ={}& \var{map}~(\lambda j.~(i::p)_j)~(0 :: \var{seq}~1~(n+1)) \\
    ={}& i :: \var{map}~(\lambda j.~(i::p)_j)~(\var{seq}~1~(n+1)) \\
    ={}& i :: \var{map}~(\lambda j.~(i::p)_j)~(\var{map}~(\lambda j.~j+1)~(\var{seq}~0~n)) \\
    ={}& i :: \var{map}~(\lambda j.~(i::p)_{j+1})~(\var{seq}~0~n) \\
    ={}& i :: \var{map}~(\lambda j.~p_j)~(\var{seq}~0~n) \\
    ={}& i :: \var{compose}~(\var{seq}~0~n)~p \\
    ={}& i :: p \qedhere
  \end{align*}
\end{proof}

\begin{theorem}[compose\_id\_r]
  For all permutation functions $p$ on lists of length $n$,
  \begin{equation*}
    \var{compose}~p~(\var{seq}~0~n) = p
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{equation*}
    \var{compose}~p~(\var{seq}~0~n)
    = \var{map}~(\lambda j.~(\var{seq}~0~n)_j)~p
    = \var{map}~(\lambda j.~j)~p
    = p \qedhere
  \end{equation*}
\end{proof}

Now we can prove the main theorem.
\begin{theorem}[compose\_apply]
  For all lists $l$ and permutation functions $p_1$ and $p_2$
  \begin{equation*}
    \var{apply}~(p_2 \comp p_1)~l = \var{apply}~p_2~(\var{apply}~p_1~l)
  \end{equation*}
\end{theorem}
\begin{proof}
  Letting $\iota = \var{seq}~0~(\var{length}~l)$, we annotate $l$ with
  indices. By \var{apply\_correct}, we have
  \begin{equation*}
    \var{combine}~\iota~l \eqperm \var{apply}~(p_2 \comp p_1)~(\var{combine}~\iota~l)
  \end{equation*}
  and
  \begin{gather*}
    \var{combine}~\iota~l
    \eqperm
    \var{apply}~p_1~(\var{combine}~\iota~l) \\
    \eqperm
    \var{apply}~p_2~(\var{apply}~p_1~(\var{combine}~\iota~l))
  \end{gather*}
  We combine these by transitivity, and then simplify. For space, we
  denote \var{apply} by only juxtaposition, and \var{combine} by $\oplus$.
  \begin{align*}
    (p_2 \comp p_1)~(\iota \oplus l)
    &\eqperm
    p_2~(p_1~(\iota \oplus l)) \\
    ((p_2 \comp p_1)~\iota) \oplus ((p_2 \comp p_1)~l)
    &\eqperm
    p_2~((p_1~\iota) \oplus (p_1~l)) \\
    (p_2 \comp p_1) \oplus ((p_2 \comp p_1)~l)
    &\eqperm
    p_2~(p_1 \oplus (p_1~l)) \\
    (p_2 \comp p_1) \oplus ((p_2 \comp p_1)~l)
    &\eqperm
    (p_2~p_1)\oplus(p_2~(p_1~l))
  \end{align*}
  Because $p_2 \comp p_1 = p_2~p_1$, we can apply
  \cref{thm:Permutation_combine_eq} to show $(p_2 \comp p_1)~l \eqperm
  p_2~(p_1~l)$
\end{proof}

\end{document}
